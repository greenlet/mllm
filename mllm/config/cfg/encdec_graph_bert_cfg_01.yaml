dec_pyr:
  d_inner: 3072
  d_k: 64
  d_model: 768
  d_v: 64
  dropout_rate: 0.0
  enhance_type: matmul
  inp_len: 128
  n_heads: 12
  n_layers: 7
  n_similar_layers: 1
  n_vocab: 30522
  step: 2
  temperature: 0.0
emb_attn:
  d_inner: 3072
  d_k: 64
  d_model: 768
  d_v: 64
  dropout_rate: 0.0
  n_heads: 12
  n_layers: 2
emb_graph:
  d_model: 768
  gnn_conv:
    cls_name: GCNConv
    module_path: torch_geometric.nn.conv
    params:
      add_self_loops: null
      bias: true
      cached: false
      improved: false
      normalize: true
  hidden_dim: 768
  n_layers: 2
emb_mlp:
  act_fn: gelu
  d_model: 768
  d_out: 768
  window_size: 3
enc_bert:
  d_model: 768
  emb2_tok_name: ''
  emb_type: cls
  inp_len: 128
  pad_token_id: 0
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
middle_type: graph
share_enc_dec_proj_weights: false
train_cfg:
  batch_size: 10
  cite_embs_target_type: r2
  cite_embs_target_weight: 1.0
  cite_toks_target_type: all
  cite_toks_target_weight: 1.0
  input_toks_target_weight: 1.0
  learning_rate: 0.0001
  learning_rate_scheduler:
    cls_name: ReduceLROnPlateau
    module_path: torch.optim.lr_scheduler
    params: {}
  mask_cfg: null
  optimizer:
    cls_name: AdamW
    module_path: torch.optim
    params: {}
  pretrained_model_path: null
