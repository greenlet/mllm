{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertGenerationEncoder, BertGenerationDecoder, EncoderDecoderModel, BertTokenizer, AutoTokenizer\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput, BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n",
    "\n",
    "from mllm.model.embgen_bert import EncoderEmbDecoderModel\n",
    "from mllm.data.qna import get_hotpotqa\n",
    "from mllm.train.embgen_bert import get_sq_batch, get_sq_df, QuesInp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Generator model inference\n",
    "## Configs and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(os.path.expandvars('$HOME')) / 'data'\n",
    "\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "random_seed = 111\n",
    "inp_len = 128\n",
    "train_eed_bert_path = DATA_PATH / 'train_mllm_eed_bert_qna'\n",
    "eed_subdir = 'eedbert-20250315_140952-bert_base_uncased-d768-emp_f-qi_enc-chkpt_encdecbert_20250131_223521'\n",
    "# eed_subdir = 'eedbert-20250315_150043-bert_base_uncased-d768-emp_f-qi_dec'\n",
    "\n",
    "eed_train_path = train_eed_bert_path / eed_subdir\n",
    "eed_snapshot_fpath = eed_train_path / 'best.pth'\n",
    "\n",
    "device_name = 'cpu'\n",
    "# device_name = 'cuda'\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<QuesInp.Enc: 'enc'>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_ques_inp(subdir: str) -> QuesInp:\n",
    "    parts = subdir.split('-')\n",
    "    for part in parts:\n",
    "        if part.startswith('qi_'):\n",
    "            return QuesInp(part[3:])\n",
    "    raise Exception(f'Cannot find part `qi_QUESINP` where QUESINP is one of: {[qi.value for qi in QuesInp]}')\n",
    "\n",
    "ques_inp = get_ques_inp(eed_subdir)\n",
    "ques_inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models and dataset\n",
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type bert-generation. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type bert to instantiate a model of type bert-generation. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertGenerationDecoder were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tkz = BertTokenizer.from_pretrained(bert_model_name)\n",
    "print(tkz)\n",
    "enc_model: BertGenerationEncoder = BertGenerationEncoder.from_pretrained(bert_model_name, bos_token_id=101, eos_token_id=102)\n",
    "# add cross attention layers and use BERT's cls token as BOS token and sep token as EOS token\n",
    "dec_model: BertGenerationDecoder = BertGenerationDecoder.from_pretrained(\n",
    "    bert_model_name, add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102\n",
    ")\n",
    "model = EncoderEmbDecoderModel(encoder=enc_model, decoder=dec_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-uncased'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertGenerationConfig\n",
    "cfg: BertGenerationConfig = enc_model.config\n",
    "cfg.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load /home/misha/data/train_mllm_eed_bert_qna/eedbert-20250315_140952-bert_base_uncased-d768-emp_f-qi_enc-chkpt_encdecbert_20250131_223521/best.pth\n"
     ]
    }
   ],
   "source": [
    "print(f'Load {eed_snapshot_fpath}')\n",
    "checkpoint = torch.load(eed_snapshot_fpath, map_location=device)\n",
    "model.load_state_dict(checkpoint['model'], strict=True)\n",
    "model.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squad v2 Qna dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad_v2 (/home/misha/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32943c8ce42b4005bd2114bab1b9d927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove empty answers from dataset squad_v2. Size: 142192 --> 92749\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random_seed)\n",
    "# exclude_empty_answers = False\n",
    "exclude_empty_answers = True\n",
    "df_sq = get_sq_df(exclude_empty_answers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexts: [1 1 1 1 1]. (5, 128)\n",
      "QAs: [30 23 24 20 22]. 119. 2889\n",
      "Qs: [128 128 128 128 128]. 640. 81920\n",
      "As: [9 4 2 2 2]. 19. 109\n",
      "Context1. Armenia presently maintains good relations with almost every country in the world, with two major exceptions being its immediate neighbours, Turkey and Azerbaijan. Tensions were running high between Armenians and Azerbaijanis during the final years of the Soviet Union. The Nagorno-Karabakh War dominated the region's politics throughout the 1990s. The border between the two rival countrie\n",
      "Context2. Every dollar ($1) that is spent on pesticides for crops yields four dollars ($4) in crops saved. This means based that, on the amount of money spent per year on pesticides, $10 billion, there is an additional $40 billion savings in crop that would be lost due to damage by insects and weeds. In general, farmers benefit from having an increase in crop yield and from being able to grow a va\n",
      "Context3. Participation in the Premier League by some Scottish or Irish clubs has sometimes been discussed, but without result. The idea came closest to reality in 1998, when Wimbledon received Premier League approval to relocate to Dublin, Ireland, but the move was blocked by the Football Association of Ireland. Additionally, the media occasionally discusses the idea that Scotland's two biggest t\n",
      "Context4. The term can be found used in an October 1845 Massachusetts Circuit Court ruling in the patent case Davoll et al. v. Brown., in which Justice Charles L. Woodbury wrote that \"only in this way can we protect intellectual property, the labors of the mind, productions and interests are as much a man's own...as the wheat he cultivates, or the flocks he rears.\" The statement that \"discoveries \n",
      "Context5. In 2003 a congressional committee called the FBI's organized crime informant program \"one of the greatest failures in the history of federal law enforcement.\" The FBI allowed four innocent men to be convicted of the March 1965 gangland murder of Edward \"Teddy\" Deegan in order to protect Vincent Flemmi, an FBI informant. Three of the men were sentenced to death (which was later reduced to\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "inds = np.arange(batch_size)\n",
    "# inds += batch_size * 2\n",
    "batch = get_sq_batch(tkz=tkz, df_sq=df_sq, inds=inds, inp_len=inp_len, device=device, ques_inp=ques_inp)\n",
    "for ctx in batch.contexts:\n",
    "    print(ctx[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Context2. Question: How is the health of the general publis affected by pesticides?. A: control of insect-borne diseases and illnesses\n",
      "Q: Context5. Question: How much was the US government ordered to pay in damages?. A: $100 million\n",
      "Q: Context4. Question: What year did the \"discoveries are property\" concept appear in French law?. A: 1791\n",
      "Q: Context1. Question: Is the border between Armenia and Azerbaijan open or closed?. A: closed\n",
      "Q: Context3. Question: In which year did a Premier League team consider relocating to Ireland?. A: 1998\n"
     ]
    }
   ],
   "source": [
    "for q, a in batch.qas:\n",
    "    print(f'Q: {q}. A: {a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctxs_toks, other_toks = batch.gen_tensors()\n",
    "ctxs_mask = (ctxs_toks > 0).to(batch.device)\n",
    "ctx_enc_out: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=ctxs_toks, attention_mask=ctxs_mask)\n",
    "ctx_emb = ctx_enc_out.last_hidden_state[:, 0].unsqueeze(0)\n",
    "ctx_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctx_emb: torch.Size([1, 5, 768]). q_emb: torch.Size([1, 1, 768]). ctxq_emb: torch.Size([1, 6, 768])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "qa_ind = 1\n",
    "\n",
    "if batch.ques_inp == QuesInp.Enc:\n",
    "    q_toks_l, a_toks_l, a_att_masks_l, a_tgt_masks_l = other_toks\n",
    "    n_ans = len(a_toks_l)\n",
    "    q_toks, a_toks, a_att_mask, a_tgt_mask = q_toks_l[qa_ind], a_toks_l[qa_ind], a_att_masks_l[qa_ind], a_tgt_masks_l[qa_ind]\n",
    "    q_toks = q_toks.unsqueeze(0)\n",
    "    q_mask = (q_toks > 0).to(batch.device)\n",
    "    q_enc_out: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=q_toks, attention_mask=q_mask)\n",
    "    q_emb = q_enc_out.last_hidden_state[:, 0].unsqueeze(0)\n",
    "    ctxq_emb = torch.concatenate([ctx_emb, q_emb], dim=1)\n",
    "    print(f'ctx_emb: {ctx_emb.shape}. q_emb: {q_emb.shape}. ctxq_emb: {ctxq_emb.shape}')\n",
    "    a_toks = a_toks.repeat(len(a_att_mask), 1)\n",
    "    # a_toks_inp = a_toks * a_att_mask\n",
    "    a_toks_inp = a_toks\n",
    "    a_dec_out: CausalLMOutputWithCrossAttentions = model.decoder(\n",
    "        input_ids=a_toks_inp, attention_mask=a_att_mask, encoder_hidden_states=ctxq_emb,\n",
    "    )\n",
    "\n",
    "elif batch.ques_inp == QuesInp.Dec:\n",
    "    qa_toks_l, qa_att_masks_l, qa_tgt_masks_l = other_toks\n",
    "    n_qas = len(qa_toks_l)\n",
    "    qa_toks, qa_att_mask, qa_tgt_mask = qa_toks_l[qa_ind].unsqueeze(0), qa_att_masks_l[qa_ind], qa_tgt_masks_l[qa_ind]\n",
    "    qa_toks = qa_toks.repeat(len(qa_att_mask), 1)\n",
    "    qa_toks_inp = qa_toks * qa_att_mask\n",
    "    dec_out: CausalLMOutputWithCrossAttentions = model.decoder(\n",
    "        input_ids=qa_toks_inp, attention_mask=qa_att_mask, encoder_hidden_states=ctx_emb\n",
    "    )\n",
    "    n = 0\n",
    "    for i in range(qa_toks.shape[1]):\n",
    "        if qa_att_mask[0, i] == 0:\n",
    "            n = i\n",
    "            break\n",
    "    q_toks = qa_toks[0, :n + 1].clone()\n",
    "    q_toks[-1] = 0\n",
    "\n",
    "else:\n",
    "    raise Exception(f'Unsupported Question input type: {ques_inp}')\n",
    "\n",
    "print(q_toks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 30522])\n",
      "torch.Size([30522])\n",
      "23666\n",
      "torch.Size([1, 6, 30522])\n",
      "torch.Size([30522])\n",
      "23666\n",
      "torch.Size([1, 7, 30522])\n",
      "torch.Size([30522])\n",
      "1580\n",
      "torch.Size([1, 8, 30522])\n",
      "torch.Size([30522])\n",
      "29531\n",
      "torch.Size([1, 9, 30522])\n",
      "torch.Size([30522])\n",
      "29531\n",
      "torch.Size([1, 10, 30522])\n",
      "torch.Size([30522])\n",
      "29531\n",
      "torch.Size([1, 11, 30522])\n",
      "torch.Size([30522])\n",
      "29531\n",
      "torch.Size([1, 12, 30522])\n",
      "torch.Size([30522])\n",
      "29531\n",
      "torch.Size([1, 13, 30522])\n",
      "torch.Size([30522])\n",
      "29531\n",
      "torch.Size([1, 14, 30522])\n",
      "torch.Size([30522])\n",
      "29531\n",
      "bouts bouts â„¢pkinspkinspkinspkinspkinspkinspkins\n"
     ]
    }
   ],
   "source": [
    "def predict(model: EncoderEmbDecoderModel, enc_emb: torch.Tensor, toks: torch.Tensor, max_len: int = 10) -> list[int]:\n",
    "    i, toks_cur, toks_out = 0, toks.tolist(), []\n",
    "    inp_ids = toks.unsqueeze(0)\n",
    "    while i < max_len:\n",
    "        # att_mask = (inp_ids > 0).to(torch.int32)\n",
    "        att_mask = torch.ones_like(inp_ids)\n",
    "        dec_out: CausalLMOutputWithCrossAttentions = model.decoder(\n",
    "            input_ids=inp_ids, attention_mask=att_mask, encoder_hidden_states=enc_emb, use_cache=False,\n",
    "        )\n",
    "        print(dec_out.logits.shape)\n",
    "        probs_pred = torch.softmax(dec_out.logits[0, -1], dim=-1)\n",
    "        print(probs_pred.shape)\n",
    "        tok_out = torch.argmax(probs_pred, dim=-1)\n",
    "        print(tok_out.item())\n",
    "        tok = tok_out.item()\n",
    "        if tok == 102:\n",
    "            break\n",
    "        toks_cur[-1] = tok\n",
    "        toks_cur.append(0)\n",
    "        toks_out.append(tok)\n",
    "        inp_ids = torch.tensor(toks_cur, dtype=toks.dtype, device=toks.device).unsqueeze(0)\n",
    "        i += 1\n",
    "    return toks_out\n",
    "\n",
    "# print(tkz.decode(q_toks.flatten().flatten().cpu().tolist()))\n",
    "if ques_inp == QuesInp.Enc:\n",
    "    q_toks = tkz('Answer: ').input_ids\n",
    "    q_toks = torch.tensor([*q_toks, 0], dtype=torch.int64, device=device)\n",
    "    toks_out = predict(model, ctxq_emb, q_toks)\n",
    "else:\n",
    "    toks_out = predict(model, ctx_emb, q_toks)\n",
    "print(tkz.decode(toks_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<QuesInp.Enc: 'enc'>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9, 30]), torch.int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_att_mask.shape, qa_att_mask.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 2491,  1997, 14211,  1011, 15356,  7870,  1998, 24757,   102]),\n",
       " tensor([1002, 2531, 2454,  102]),\n",
       " tensor([14362,   102]),\n",
       " tensor([2701,  102]),\n",
       " tensor([2687,  102])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_toks_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False],\n",
       "       [ True,  True, False, False],\n",
       "       [ True,  True,  True, False],\n",
       "       [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 4\n",
    "t = np.tril(np.ones((n, n), dtype=bool), k=0)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28, 90,  9, 20])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randint(100, size=n)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28, 90,  9, 20],\n",
       "       [28, 90,  9, 20],\n",
       "       [28, 90,  9, 20],\n",
       "       [28, 90,  9, 20]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.repeat(a[None], n, axis=0)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[123,   0,   0,   0],\n",
       "       [ 28, 123,   0,   0],\n",
       "       [ 28,  90, 123,   0],\n",
       "       [ 28,  90,   9, 123]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_id = 123\n",
    "mask = np.eye(n, dtype=bool)\n",
    "bb = np.tril(b, k=-1)\n",
    "bb[mask] = mask_token_id\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28, 90,  9, 20])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at = torch.tensor(a)\n",
    "at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28,  0,  0,  0],\n",
       "        [28, 90,  0,  0],\n",
       "        [28, 90,  9,  0],\n",
       "        [28, 90,  9, 20]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atn = at.repeat(n, 1)\n",
    "atn = torch.tril(atn)\n",
    "atn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False],\n",
       "        [False,  True, False, False],\n",
       "        [False, False,  True, False],\n",
       "        [False, False, False,  True]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskt = torch.tensor(mask)\n",
    "maskt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[103,   0,   0,   0],\n",
       "        [ 28, 103,   0,   0],\n",
       "        [ 28,  90, 103,   0],\n",
       "        [ 28,  90,   9, 103]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atn[maskt] = tkz.mask_token_id\n",
    "atn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
