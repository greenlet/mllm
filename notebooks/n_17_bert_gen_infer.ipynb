{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertGenerationEncoder, BertGenerationDecoder, EncoderDecoderModel, BertTokenizer, AutoTokenizer\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput, BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n",
    "\n",
    "from mllm.model.embgen_bert import EncoderEmbDecoderModel\n",
    "from mllm.data.qna import get_hotpotqa\n",
    "from mllm.train.embgen_bert import get_sq_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Generator model inference\n",
    "## Configs and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(os.path.expandvars('$HOME')) / 'data'\n",
    "\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "random_seed = 111\n",
    "inp_len = 128\n",
    "train_eed_bert_path = DATA_PATH / 'train_mllm_eed_bert_qna'\n",
    "# eed_subdir = 'eedbert-20250303_220645-bert_base_uncased-d768'\n",
    "eed_subdir = 'eedbert-20250306_220643-bert_base_uncased-d768'\n",
    "\n",
    "eed_train_path = train_eed_bert_path / eed_subdir\n",
    "eed_snapshot_fpath = eed_train_path / 'best.pth'\n",
    "\n",
    "device_name = 'cpu'\n",
    "# device_name = 'cuda'\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models and dataset\n",
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type bert-generation. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type bert to instantiate a model of type bert-generation. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertGenerationDecoder were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tkz = BertTokenizer.from_pretrained(bert_model_name)\n",
    "print(tkz)\n",
    "enc_model: BertGenerationEncoder = BertGenerationEncoder.from_pretrained(bert_model_name, bos_token_id=101, eos_token_id=102)\n",
    "# add cross attention layers and use BERT's cls token as BOS token and sep token as EOS token\n",
    "dec_model: BertGenerationDecoder = BertGenerationDecoder.from_pretrained(\n",
    "    bert_model_name, add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102\n",
    ")\n",
    "model = EncoderEmbDecoderModel(encoder=enc_model, decoder=dec_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load /home/misha/data/train_mllm_eed_bert_qna/eedbert-20250306_220643-bert_base_uncased-d768/best.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Load {eed_snapshot_fpath}')\n",
    "checkpoint = torch.load(eed_snapshot_fpath)\n",
    "model.load_state_dict(checkpoint['model'], strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squad v2 Qna dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad_v2 (/home/misha/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d5333adb3545858be40788a27828b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(random_seed)\n",
    "ds_sq = load_dataset('squad_v2')\n",
    "df_sq = pd.concat([ds_sq['train'].to_pandas(), ds_sq['validation'].to_pandas()], axis=0)\n",
    "n_total = len(df_sq)\n",
    "df_sq = df_sq.sample(n_total)\n",
    "val_ratio = 0.05\n",
    "n_val = int(n_total * val_ratio)\n",
    "n_train = n_total - n_val\n",
    "df_sq_t, df_sq_v = df_sq.iloc[:n_train], df_sq.iloc[n_train:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context1. Traditionally a carnival feast was the last opportunity to eat well before the time of food shortage at the end of the winter during which one was limited to the minimum necessary. On what n\n",
      "Context2. DNA replication is for the most part extremely accurate, however errors (mutations) do occur.:7.6 The error rate in eukaryotic cells can be as low as 10âˆ’8 per nucleotide per replication, whe\n",
      "Context3. Like other American research universities, Northwestern was transformed by World War II. Franklyn B. Snyder led the university from 1939 to 1949, when nearly 50,000 military officers and per\n",
      "Context4. There were two main techniques in Greco-Roman mosaic: opus vermiculatum used tiny tesserae, typically cubes of 4 millimeters or less, and was produced in workshops in relatively small panels\n",
      "Context5. As a side effect of the electrochemical processes used by neurons for signaling, brain tissue generates electric fields when it is active. When large numbers of neurons show synchronized act\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "inds1 = np.arange(batch_size)\n",
    "inds1 += batch_size * 1\n",
    "batch1 = get_sq_batch(tkz=tkz, df_sq=df_sq, inds=inds1, inp_len=inp_len, device=device)\n",
    "for ctx in batch1.contexts:\n",
    "    print(ctx[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Context1. What was one limited to during the winter?. A: the minimum necessary\n",
      "Q: Context4. What were small panel mosaics known as?. A: emblemata\n",
      "Q: Context5. MEG of the brain is an abbreviation of what?. A: magnetoencephalography\n",
      "Q: Context2. What can small mutations be caused by?. A: DNA replication\n",
      "Q: Context3. Between 1939 and 1949, how many military officers and personnel were trained on the Evanston and Chicago campuses?. A: nearly 50,000\n"
     ]
    }
   ],
   "source": [
    "for q, a in batch1.qas:\n",
    "    print(f'Q: {q}. A: {a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qas_toks, qa_att_masks, qa_tgt_masks, ctxs_toks = batch1.gen_tensors()\n",
    "ctxs_mask = (ctxs_toks > 0).to(device)\n",
    "enc_out: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=ctxs_toks, attention_mask=ctxs_mask)\n",
    "enc_emb = enc_out.last_hidden_state[:, 0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_ind = 0\n",
    "qa_toks, qa_att_mask, qa_tgt_mask = qas_toks[qa_ind].unsqueeze(0), qa_att_masks[qa_ind], qa_tgt_masks[qa_ind]\n",
    "qa_toks = qa_toks.repeat(len(qa_att_mask), 1)\n",
    "qa_toks_inp = qa_toks * qa_att_mask\n",
    "dec_out: CausalLMOutputWithCrossAttentions = model.decoder(\n",
    "    input_ids=qa_toks_inp, attention_mask=qa_att_mask, encoder_hidden_states=enc_emb\n",
    ")\n",
    "\n",
    "n = 0\n",
    "for i in range(qa_toks.shape[1]):\n",
    "    if qa_att_mask[0, i] == 0:\n",
    "        n = i\n",
    "        break\n",
    "q_toks = qa_toks[0, :n + 1].clone()\n",
    "q_toks[-1] = 0\n",
    "\n",
    "# q_toks = qa_toks[0, :n].clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] context1. what was one limited to during the winter? [SEP] [PAD]\n",
      "torch.Size([1, 15, 30522])\n",
      "torch.Size([30522])\n",
      "1011\n",
      "torch.Size([1, 16, 30522])\n",
      "torch.Size([30522])\n",
      "102\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "def predict(model: EncoderEmbDecoderModel, enc_emb: torch.Tensor, toks: torch.Tensor, max_len: int = 10) -> list[int]:\n",
    "    i, toks_cur, toks_out = 0, toks.tolist(), []\n",
    "    inp_ids = toks.unsqueeze(0)\n",
    "    while i < max_len:\n",
    "        att_mask = inp_ids > 0\n",
    "        dec_out: CausalLMOutputWithCrossAttentions = model.decoder(\n",
    "            input_ids=inp_ids, attention_mask=att_mask, encoder_hidden_states=enc_emb,\n",
    "        )\n",
    "        print(dec_out.logits.shape)\n",
    "        probs_pred = torch.softmax(dec_out.logits[0, -1], dim=-1)\n",
    "        print(probs_pred.shape)\n",
    "        tok_out = torch.argmax(probs_pred, dim=-1)\n",
    "        print(tok_out.item())\n",
    "        tok = tok_out.item()\n",
    "        if tok == 102:\n",
    "            break\n",
    "        toks_cur[-1] = tok\n",
    "        toks_cur.append(0)\n",
    "        toks_out.append(tok)\n",
    "        inp_ids = torch.tensor(toks_cur, dtype=toks.dtype, device=toks.device).unsqueeze(0)\n",
    "        i =+ 1\n",
    "    return toks_out\n",
    "\n",
    "print(tkz.decode(q_toks.tolist()))\n",
    "toks_out = predict(model, enc_emb, q_toks)\n",
    "print(tkz.decode(toks_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(q_toks.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
