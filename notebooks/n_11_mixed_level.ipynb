{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "from typing import Optional, Union\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from transformers import PreTrainedTokenizer, GPT2Tokenizer\n",
    "\n",
    "\n",
    "from mllm.config.model import VocabEncoderCfg, EmbDecoderCfg\n",
    "from mllm.model.modules import VocabEncoder, VocabDecoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    temperature: float\n",
    "    inp_len: int\n",
    "    dropout_rate: float\n",
    "    # dropout: nn.Module\n",
    "\n",
    "    def __init__(self, temperature: float, inp_len: int = 0,\n",
    "                 dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.inp_len = inp_len\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = q / self.temperature\n",
    "        attn = torch.matmul(attn, k.transpose(2, 3))\n",
    "\n",
    "        if mask is not None:\n",
    "            # print_dtype_shape(attn)\n",
    "            # print_dtype_shape(mask)\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    n_heads: int\n",
    "    d_model: int\n",
    "    d_k: int\n",
    "    d_v: int\n",
    "    dropout_rate: float\n",
    "\n",
    "    def __init__(self, n_heads: int, d_model: int, d_k: int, d_v: int, dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_heads * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_heads * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_heads * d_v, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "\n",
    "        temp = d_k ** 0.5\n",
    "        temp = 1\n",
    "        self.attention = ScaledDotProductAttention(\n",
    "            temperature=temp, inp_len=10000,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        d_k, d_v, n_heads = self.d_k, self.d_v, self.n_heads\n",
    "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        # Pass through the pre-attention projection: b x lq x (n*dv)\n",
    "        # Separate different heads: b x lq x n x dv\n",
    "        q = self.w_qs(q)\n",
    "        k = self.w_ks(k)\n",
    "        v = self.w_vs(v)\n",
    "\n",
    "        q = q.view(sz_b, len_q, n_heads, d_k)\n",
    "        k = k.view(sz_b, len_k, n_heads, d_k)\n",
    "        v = v.view(sz_b, len_v, n_heads, d_v)\n",
    "\n",
    "        # Transpose for attention dot product: b x n x lq x dv\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "\n",
    "        q, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # Transpose to move the head dimension back: b x lq x n x dv\n",
    "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "        q = self.dropout(self.fc(q))\n",
    "        q += residual\n",
    "\n",
    "        q = self.layer_norm(q)\n",
    "\n",
    "        return q, attn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_hid, n_position=200):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Not a parameter\n",
    "        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, d_hid))\n",
    "\n",
    "    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n",
    "        ''' Sinusoid position encoding table '''\n",
    "        # TODO: make it with torch instead of numpy\n",
    "\n",
    "        def get_position_angle_vec(position):\n",
    "            return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
    "\n",
    "        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "        return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_table[:, :x.size(1)].clone().detach()\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        bias = True\n",
    "        self.w_1 = nn.Linear(d_in, d_hid, bias=bias) # position-wise\n",
    "        self.w_2 = nn.Linear(d_hid, d_in, bias=bias) # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        # x = self.w_2(F.leaky_relu(self.w_1(x)))\n",
    "        # x = self.w_2(F.sigmoid(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    ''' Compose with two layers '''\n",
    "\n",
    "    def __init__(self, n_heads, d_model, d_inner, d_k, d_v, dropout_rate: float = 0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_heads, d_model, d_k, d_v, dropout_rate=dropout_rate)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout_rate=dropout_rate)\n",
    "\n",
    "    def forward(self, enc_input: Optional[Tensor], enc_input_kv: Optional[Tensor] = None, slf_attn_mask: Optional[Tensor] = None):\n",
    "        if enc_input_kv is None:\n",
    "            enc_input_kv = enc_input\n",
    "        enc_output, enc_slf_attn = self.slf_attn(\n",
    "            enc_input, enc_input_kv, enc_input_kv, mask=slf_attn_mask)\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        return enc_output, enc_slf_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 256\n",
    "d_word_vec = d_model\n",
    "n_heads = 8\n",
    "d_v = d_k = d_model // n_heads\n",
    "dropout_rate = 0.0\n",
    "d_inner = 1024\n",
    "inp_len = 20_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "tkz = GPT2Tokenizer.from_pretrained('gpt2', model_max_length=1000)\n",
    "tkz.add_special_tokens({'pad_token': '<|pad_token|>'})\n",
    "n_vocab = len(tkz)\n",
    "pad_idx = tkz.pad_token_id\n",
    "print(pad_idx)\n",
    "\n",
    "cfg_vocab_enc = VocabEncoderCfg(\n",
    "    n_vocab=n_vocab, d_word_vec=d_word_vec, d_model=d_model, pad_idx=pad_idx, inp_len=inp_len, dropout_rate=dropout_rate,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VocabEncoder(\n",
       "  (src_word_emb): Embedding(50258, 256, padding_idx=50257)\n",
       "  (position_enc): PositionalEncoding()\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_encoder = VocabEncoder(**cfg_vocab_enc.dict())\n",
    "vocab_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_layer_1 = EncoderLayer(\n",
    "    n_heads=n_heads, d_model=d_model, d_inner=d_inner, d_k=d_k, d_v=d_v,\n",
    "    dropout_rate=dropout_rate,\n",
    ")\n",
    "enc_layer_2 = EncoderLayer(\n",
    "    n_heads=n_heads, d_model=d_model, d_inner=d_inner, d_k=d_k, d_v=d_v,\n",
    "    dropout_rate=dropout_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "113\n",
      "110\n",
      "64\n",
      "113\n",
      "108\n",
      "113\n"
     ]
    }
   ],
   "source": [
    "doc = \"\"\"\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n",
    "    all you need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
    "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "\n",
    "    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n",
    "    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n",
    "    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n",
    "    \"\"\"\n",
    "txts = [s.strip() for s in doc.split('\\n') if s.strip()]\n",
    "for txt in txts:\n",
    "    print(len(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([7, 50]) torch.int32\n",
      " torch.Size([7, 50, 50]) torch.int32\n"
     ]
    }
   ],
   "source": [
    "seq_len = 50\n",
    "\n",
    "def print_dtype_shape(x: Union[np.ndarray, Tensor], name: str = ''):\n",
    "    print(name, x.shape, x.dtype)\n",
    "\n",
    "def to_tensor(*xs: np.ndarray, device='cpu') -> tuple[Tensor, ...]:\n",
    "    return tuple(torch.from_numpy(x).to(device) for x in xs)\n",
    "\n",
    "def to_tokens(tkz: PreTrainedTokenizer, txts: list[str], seq_len: int, pad_tok: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    n = len(txts)\n",
    "    tokens = np.full((n, seq_len), pad_tok, dtype=np.int32)\n",
    "    masks = np.full((n, seq_len), 0, dtype=np.int32)\n",
    "    for i in range(n):\n",
    "        txt = txts[i]\n",
    "        toks = tkz(txt)['input_ids'][:seq_len]\n",
    "        tokens[i, :len(toks)] = toks\n",
    "        masks[i, :len(toks)] = 1\n",
    "    masks1 = masks[:, :, np.newaxis]\n",
    "    masks2 = masks[:, np.newaxis, :]\n",
    "    masks = np.matmul(masks1, masks2)\n",
    "    return tokens, masks\n",
    "\n",
    "tokens, masks = to_tokens(tkz, txts, seq_len, pad_idx)\n",
    "tokens, masks = to_tensor(tokens, masks)\n",
    "print_dtype_shape(tokens)\n",
    "print_dtype_shape(masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 50, 256]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "out_enc = vocab_encoder(tokens)\n",
    "print_dtype_shape(out_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 50, 256]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "out_enc_1, _ = enc_layer_1(out_enc, slf_attn_mask=masks)\n",
    "print_dtype_shape(out_enc_1)\n",
    "# out_enc_2, _ = enc_layer_2(out_enc_1, slf_attn_mask=masks)\n",
    "# print_dtype_shape(out_enc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([7, 17, 256]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "class ReduceLayer(nn.Module):\n",
    "    d_model: int\n",
    "    step: int\n",
    "    reducer: nn.Linear\n",
    "\n",
    "    def __init__(self, d_model: int, step: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.step = step\n",
    "        self.reducer = nn.Linear(in_features=d_model * step, out_features=d_model, bias=False)\n",
    "    \n",
    "    def forward(self, inp: Tensor) -> Tensor:\n",
    "        batch_size, seq_len, d_model = inp.shape\n",
    "        assert d_model == self.d_model, f'self.d_model = {self.d_model}. inp d_model = {d_model}'\n",
    "        len_mod = seq_len % self.step\n",
    "        # print_dtype_shape(inp, 'rdc_inp')\n",
    "        if len_mod > 0:\n",
    "            n_seq_add = self.step - len_mod\n",
    "            inp = F.pad(inp, (0, 0, n_seq_add, 0), value=0)\n",
    "            seq_len += n_seq_add\n",
    "            # print_dtype_shape(inp, 'rdc_inp_pad')\n",
    "        inp = inp.reshape(batch_size, seq_len // self.step, self.d_model * self.step)\n",
    "        # print_dtype_shape(inp, 'rds_reshape')\n",
    "        out = self.reducer(inp)\n",
    "        # print_dtype_shape(out, 'rdc_reduce')\n",
    "        return out\n",
    "\n",
    "step = 3\n",
    "enc = out_enc_1\n",
    "reduce_layer = ReduceLayer(d_model=d_model, step=step)\n",
    "\n",
    "# reduce_layer = torch.nn.Linear(in_features=d_model * 2, out_features=d_model, bias=False)\n",
    "# print_dtype_shape(enc)\n",
    "# if enc.shape[-1] % 2 == 1:\n",
    "#     enc = F.pad(enc, (0, 0, 1, 0), value=0)\n",
    "# enc = enc.reshape((enc.shape[0], enc.shape[1] // 2, enc.shape[2] * 2))\n",
    "# print_dtype_shape(enc)\n",
    "enc = reduce_layer(enc)\n",
    "print_dtype_shape(enc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dec_pyr': {'d_inner': 1024,\n",
      "             'd_k': 32,\n",
      "             'd_model': 256,\n",
      "             'd_v': 32,\n",
      "             'dropout_rate': 0.0,\n",
      "             'inp_len': 256,\n",
      "             'n_heads': 8,\n",
      "             'n_layers': 8,\n",
      "             'n_vocab': 50258,\n",
      "             'step': 2,\n",
      "             'with_vocab_decoder': True},\n",
      " 'enc_pyr': {'d_inner': 1024,\n",
      "             'd_k': 32,\n",
      "             'd_model': 256,\n",
      "             'd_v': 32,\n",
      "             'dropout_rate': 0.0,\n",
      "             'inp_len': 256,\n",
      "             'n_heads': 8,\n",
      "             'n_layers': 8,\n",
      "             'pad_idx': 50257,\n",
      "             'step': 2,\n",
      "             'vocab_encoder': {'d_model': 256,\n",
      "                               'd_word_vec': 256,\n",
      "                               'dropout_rate': 0.0,\n",
      "                               'inp_len': 256,\n",
      "                               'n_vocab': 50258,\n",
      "                               'pad_idx': 50257}}}\n"
     ]
    }
   ],
   "source": [
    "class EncPyrCfg(BaseModel):\n",
    "    vocab_encoder: VocabEncoderCfg\n",
    "    pad_idx: int\n",
    "    d_model: int\n",
    "    n_heads: int\n",
    "    d_k: int\n",
    "    d_v: int\n",
    "    d_inner: int\n",
    "    inp_len: int\n",
    "    step: int\n",
    "    n_layers: int\n",
    "    dropout_rate: float\n",
    "\n",
    "\n",
    "class DecPyrCfg(BaseModel):\n",
    "    d_model: int\n",
    "    n_heads: int\n",
    "    d_k: int\n",
    "    d_v: int\n",
    "    d_inner: int\n",
    "    inp_len: int\n",
    "    step: int\n",
    "    n_layers: int\n",
    "    dropout_rate: float\n",
    "    with_vocab_decoder: bool\n",
    "    n_vocab: int\n",
    "\n",
    "\n",
    "class EncdecHgCfg(BaseModel):\n",
    "    enc_pyr: EncPyrCfg\n",
    "    dec_pyr: DecPyrCfg\n",
    "\n",
    "\n",
    "def create_encdec_hg_cfg(\n",
    "        n_vocab: int, pad_idx: int, d_model: int = 256, n_heads: int = 8, d_inner: int = 1024, inp_len: int = 256, step: int = 2, dropout_rate: float = 0.0, with_vacab_decoder: bool = True) -> EncdecHgCfg:\n",
    "    d_word_vec = d_model\n",
    "    d_k = d_v = d_model // n_heads\n",
    "    n_layers = math.ceil(math.log(inp_len, step))\n",
    "    cfg_vocab_enc = VocabEncoderCfg(\n",
    "        n_vocab=n_vocab, d_word_vec=d_word_vec, d_model=d_model, pad_idx=pad_idx, inp_len=inp_len, dropout_rate=dropout_rate,\n",
    "    )\n",
    "    cfg_enc_pyr = EncPyrCfg(\n",
    "        vocab_encoder=cfg_vocab_enc, pad_idx=pad_idx, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_inner=d_inner, inp_len=inp_len, step=step, n_layers=n_layers, dropout_rate=dropout_rate,\n",
    "    )\n",
    "    cfg_dec_pyr = DecPyrCfg(\n",
    "        d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_inner=d_inner, inp_len=inp_len, step=step, n_layers=n_layers, dropout_rate=dropout_rate, with_vocab_decoder=with_vacab_decoder, n_vocab=n_vocab,\n",
    "    )\n",
    "    cfg_encdec_hg = EncdecHgCfg(enc_pyr=cfg_enc_pyr, dec_pyr=cfg_dec_pyr)\n",
    "    return cfg_encdec_hg\n",
    "\n",
    "cfg_encdec_hg = create_encdec_hg_cfg(n_vocab=len(tkz), pad_idx=tkz.pad_token_id)\n",
    "pprint(cfg_encdec_hg.dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens torch.Size([7, 50]) torch.int32\n",
      "inp_toks torch.Size([7, 256]) torch.int32\n",
      "out torch.Size([7, 1, 256]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "class EncoderPyramid(nn.Module):\n",
    "    cfg: EncPyrCfg\n",
    "    vocab_encoder: VocabEncoder\n",
    "    enc_layers: nn.ModuleList\n",
    "    rdc_layers: nn.ModuleList\n",
    "    inp_chunk_len: int\n",
    "\n",
    "    def __init__(self, cfg: EncPyrCfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.vocab_encoder = VocabEncoder(**cfg.vocab_encoder.dict())\n",
    "        self.enc_layers = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                n_heads=cfg.n_heads, d_model=cfg.d_model, d_inner=cfg.d_inner, d_k=cfg.d_k, d_v=cfg.d_v,\n",
    "                dropout_rate=cfg.dropout_rate,\n",
    "            ) for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "        self.rdc_layers = nn.ModuleList([\n",
    "            ReduceLayer(d_model=cfg.d_model, step=cfg.step) for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "\n",
    "    # Tensor of integer tokens: [batch_size, seq_len]\n",
    "    def forward(self, inp: Tensor) -> Tensor:\n",
    "        batch_size, seq_len = inp.shape\n",
    "        mask = (inp == self.cfg.pad_idx).to(torch.bool)\n",
    "        mask = np.matmul(mask.unsqueeze(-1), mask.unsqueeze(-2))\n",
    "        assert seq_len == self.cfg.inp_len, f'seq_len = {seq_len}. inp_len = {inp_len}'\n",
    "        # [batch_size, seq_len, d_model]\n",
    "        out = self.vocab_encoder(inp)\n",
    "        # print_dtype_shape(out, 'vocab_enc')\n",
    "        for enc_layer, rdc_layer in zip(self.enc_layers, self.rdc_layers):\n",
    "            out, _ = enc_layer(out, slf_attn_mask=mask)\n",
    "            inds = slice(0, out.shape[1], 2)\n",
    "            # print_dtype_shape(mask, 'mask 1')\n",
    "            mask = mask[:, inds, inds]\n",
    "            # print_dtype_shape(mask, 'mask 2')\n",
    "            out = rdc_layer(out)\n",
    "        return out\n",
    "\n",
    "enc_pyr = EncoderPyramid(cfg_encdec_hg.enc_pyr)\n",
    "pad_len = cfg_encdec_hg.enc_pyr.inp_len - tokens.shape[1]\n",
    "print_dtype_shape(tokens, 'tokens')\n",
    "inp_toks = F.pad(tokens, (0, pad_len), value=pad_idx)\n",
    "print_dtype_shape(inp_toks, 'inp_toks')\n",
    "out = enc_pyr(inp_toks)\n",
    "print_dtype_shape(out, 'out')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enh_inp torch.Size([7, 1, 256]) torch.float32\n",
      "enh_out torch.Size([7, 1, 512]) torch.float32\n",
      "enh_out_reshape torch.Size([7, 2, 256]) torch.float32\n",
      "enh_inp torch.Size([7, 2, 256]) torch.float32\n",
      "enh_out torch.Size([7, 2, 512]) torch.float32\n",
      "enh_out_reshape torch.Size([7, 4, 256]) torch.float32\n",
      "enh_inp torch.Size([7, 4, 256]) torch.float32\n",
      "enh_out torch.Size([7, 4, 512]) torch.float32\n",
      "enh_out_reshape torch.Size([7, 8, 256]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "class EnhanceLayer(nn.Module):\n",
    "    d_model: int\n",
    "    step: int\n",
    "    enhancer: nn.Linear\n",
    "\n",
    "    def __init__(self, d_model: int, step: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.step = step\n",
    "        self.enhancer = nn.Linear(in_features=d_model, out_features=d_model * step, bias=False)\n",
    "    \n",
    "    def forward(self, inp: Tensor) -> Tensor:\n",
    "        batch_size, seq_len, d_model = inp.shape\n",
    "        assert d_model == self.d_model, f'self.d_model = {self.d_model}. inp d_model = {d_model}'\n",
    "        print_dtype_shape(inp, 'enh_inp')\n",
    "        out = self.enhancer(inp)\n",
    "        print_dtype_shape(out, 'enh_out')\n",
    "        out = out.reshape(batch_size, seq_len * self.step, self.d_model)\n",
    "        print_dtype_shape(out, 'enh_out_reshape')\n",
    "        return out\n",
    "\n",
    "step = 2\n",
    "enh_layer = EnhanceLayer(cfg_encdec_hg.dec_pyr.d_model, step)\n",
    "enh_out = out\n",
    "enh_out = enh_layer(enh_out)\n",
    "enh_out = enh_layer(enh_out)\n",
    "enh_out = enh_layer(enh_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 50, 256]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "class DecoderPyramid(nn.Module):\n",
    "    cfg: DecPyrCfg\n",
    "    enc_layers: nn.ModuleList\n",
    "    enh_layers: nn.ModuleList\n",
    "    inp_chunk_len: int\n",
    "    # vocab_decoder: Optional[VocabDecoder]\n",
    "\n",
    "    def __init__(self, cfg: DecPyrCfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.enc_layers = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                n_heads=cfg.n_heads, d_model=cfg.d_model, d_inner=cfg.d_inner, d_k=cfg.d_k, d_v=cfg.d_v,\n",
    "                dropout_rate=cfg.dropout_rate,\n",
    "            ) for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "        self.enh_layers = nn.ModuleList([\n",
    "            EnhanceLayer(d_model=cfg.d_model, step=cfg.step) for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "        self.vocab_decoder = None\n",
    "        if self.cfg.with_vocab_decoder:\n",
    "            self.vocab_decoder = VocabDecoder(d_model=self.cfg.d_model, n_vocab=self.cfg.n_vocab)\n",
    "\n",
    "    # Tensor with embeddings: [batch_size, 1, d_model]\n",
    "    def forward(self, inp: Tensor) -> Tensor:\n",
    "        out = inp\n",
    "        for enc_layer, enh_layer in zip(self.enc_layers, self.enh_layers):\n",
    "            out = enh_layer(out)\n",
    "            out, _ = enc_layer(out)\n",
    "\n",
    "        if self.vocab_decoder is not None:\n",
    "            # [batch_size, seq_len, d_model]\n",
    "            out = self.vocab_decoder(out)\n",
    "            # [batch_size, seq_len, n_vocab]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
