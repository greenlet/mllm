{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Optional\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pydantic_yaml import parse_yaml_file_as\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\", torch_dtype=torch.float32, attn_implementation=\"sdpa\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 417.649MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(os.path.expandvars('$HOME')) / 'data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.4493, grad_fn=<SqrtBackward0>)\n",
      "tensor(4.7394, grad_fn=<SqrtBackward0>)\n",
      "tensor(14.1465, grad_fn=<SqrtBackward0>)\n"
     ]
    }
   ],
   "source": [
    "txt1 = 'Moscow calling'\n",
    "txt2 = \"pekin calling\"\n",
    "txt2 = \"Let's go\"\n",
    "txt3 = 'calling moscow'\n",
    "def get_emb(txt: str):\n",
    "    toks = tokenizer(txt)\n",
    "    inp = torch.tensor(toks['input_ids'])\n",
    "    out = model(inp.unsqueeze(0))\n",
    "    # key = 'last_hidden_state'\n",
    "    # return out[key].squeeze()[0]\n",
    "    key = 'pooler_output'\n",
    "    return out[key].squeeze()\n",
    "\n",
    "emb1 = get_emb(txt1)\n",
    "emb2 = get_emb(txt2)\n",
    "emb3 = get_emb(txt3)\n",
    "\n",
    "def dist(x, y):\n",
    "    # print(x.shape)\n",
    "    # return torch.sum(x * y) / torch.norm(x) / torch.norm(y)\n",
    "    d = x - y\n",
    "    return torch.sqrt(torch.sum(d * d))\n",
    "\n",
    "print(dist(emb1, emb2))\n",
    "print(dist(emb1, emb3))\n",
    "print(dist(emb2, emb3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 768 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43memb1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 768 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 2.9144e-03,  3.9209e-01,  4.2664e-02,  ..., -1.9043e-01,\n",
      "           1.9202e-01,  6.1035e-01],\n",
      "         [ 6.1328e-01,  1.9446e-01,  1.4746e-01,  ...,  1.5125e-01,\n",
      "           1.1514e+00,  8.3618e-03],\n",
      "         [-2.0581e-01,  4.9561e-01,  1.1243e-01,  ..., -2.5879e-01,\n",
      "           4.8877e-01,  4.0283e-01],\n",
      "         ...,\n",
      "         [-3.5425e-01,  2.0157e-02,  1.2979e+00,  ...,  6.1719e-01,\n",
      "           5.4834e-01,  5.4688e-01],\n",
      "         [-3.6888e-03,  7.3389e-01,  2.8931e-01,  ...,  7.4414e-01,\n",
      "           1.6663e-01,  1.0357e-03],\n",
      "         [ 3.7109e-01,  5.2393e-01, -4.7607e-01,  ...,  4.4525e-02,\n",
      "          -4.9097e-01, -3.6255e-01]]], dtype=torch.float16,\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.7397, -0.4351, -0.8726,  0.6460,  0.6240, -0.1565,  0.7266,  0.2549,\n",
      "         -0.5791, -1.0000, -0.3198,  0.9307,  0.9688,  0.4231,  0.8350, -0.7075,\n",
      "         -0.2744, -0.5811,  0.3804,  0.1144,  0.5566,  1.0000,  0.1964,  0.2974,\n",
      "          0.3547,  0.9688, -0.7827,  0.8584,  0.9312,  0.6895, -0.5327,  0.1989,\n",
      "         -0.9883, -0.1854, -0.9268, -0.9917,  0.4153, -0.6436,  0.0061,  0.1406,\n",
      "         -0.8423,  0.3442,  1.0000, -0.3081,  0.4121, -0.2659, -1.0000,  0.1801,\n",
      "         -0.8228,  0.8096,  0.8696,  0.6870,  0.1714,  0.3745,  0.5049, -0.0759,\n",
      "         -0.0836,  0.2174, -0.3169, -0.4766, -0.6846,  0.5435, -0.8496, -0.8467,\n",
      "          0.8281,  0.8193, -0.1499, -0.2869, -0.1247, -0.1516,  0.8105,  0.1548,\n",
      "          0.0398, -0.8579,  0.4556,  0.2456, -0.6548,  1.0000, -0.3594, -0.9663,\n",
      "          0.8633,  0.6899,  0.5317, -0.0529,  0.4827, -1.0000,  0.5952, -0.1593,\n",
      "         -0.9829,  0.2190,  0.6050, -0.1953,  0.7515,  0.5654, -0.5806, -0.3865,\n",
      "         -0.2932, -0.8330, -0.2698, -0.2424,  0.0191, -0.0955, -0.3999, -0.3909,\n",
      "          0.1985, -0.3955, -0.3809,  0.4800,  0.1655,  0.6157,  0.4561, -0.3848,\n",
      "          0.3586, -0.9355,  0.5483, -0.3997, -0.9800, -0.5449, -0.9834,  0.6509,\n",
      "         -0.3308, -0.2698,  0.8955, -0.0493,  0.4185, -0.1489, -0.8457, -1.0000,\n",
      "         -0.5391, -0.7437, -0.0152, -0.1984, -0.9678, -0.9590,  0.6250,  0.9243,\n",
      "          0.2512,  1.0000, -0.1113,  0.9272, -0.0635, -0.6567,  0.4656, -0.4058,\n",
      "          0.7563,  0.1436, -0.5298,  0.1866, -0.3435,  0.2227, -0.6997, -0.1742,\n",
      "         -0.6792, -0.9150, -0.3506,  0.8911, -0.6157, -0.8633, -0.0215, -0.0930,\n",
      "         -0.4622,  0.8057,  0.6860,  0.3062, -0.3933,  0.4016,  0.3625,  0.4495,\n",
      "         -0.7822,  0.1760,  0.3877, -0.3015, -0.8442, -0.9678, -0.3750,  0.3862,\n",
      "          0.9824,  0.6655,  0.2666,  0.6528, -0.2585,  0.6416, -0.9370,  0.9736,\n",
      "         -0.1963,  0.2720, -0.7012,  0.3298, -0.7866,  0.1940,  0.6938, -0.5591,\n",
      "         -0.7578, -0.1160, -0.5469, -0.2529, -0.8052,  0.2908, -0.2219, -0.2871,\n",
      "         -0.0774,  0.8945,  0.8716,  0.5615,  0.1221,  0.5972, -0.8228, -0.3601,\n",
      "          0.0204,  0.1847,  0.0610,  0.9849, -0.8184, -0.0074, -0.8716, -0.9746,\n",
      "         -0.0937, -0.7920, -0.1249, -0.7480,  0.6201, -0.4092,  0.1809,  0.3699,\n",
      "         -0.8638, -0.6577,  0.4187, -0.4290,  0.4990, -0.2908,  0.9492,  0.8794,\n",
      "         -0.6123,  0.3071,  0.9360, -0.9263, -0.7588,  0.5293, -0.1565,  0.8242,\n",
      "         -0.6050,  0.9722,  0.9165,  0.6816, -0.8428, -0.8013, -0.6890, -0.5811,\n",
      "         -0.0389,  0.2201,  0.8306,  0.5977,  0.2915,  0.2372, -0.4250,  0.9565,\n",
      "         -0.9780, -0.9429, -0.8887,  0.0947, -0.9863,  0.8311,  0.3372,  0.7227,\n",
      "         -0.4875, -0.6528, -0.9478,  0.6074,  0.1094,  0.9409, -0.3940, -0.8115,\n",
      "         -0.4983, -0.9287, -0.1578, -0.2371, -0.2036, -0.1164, -0.9126,  0.5381,\n",
      "          0.5293,  0.5400, -0.7842,  0.9893,  1.0000,  0.9600,  0.8315,  0.6919,\n",
      "         -1.0000, -0.8228,  1.0000, -0.9849, -1.0000, -0.8755, -0.6543,  0.3184,\n",
      "         -1.0000, -0.1713,  0.0999, -0.8882,  0.5391,  0.9502,  0.9180, -1.0000,\n",
      "          0.7207,  0.8447, -0.6333,  0.9360, -0.4136,  0.9492,  0.6460,  0.4485,\n",
      "         -0.2405,  0.4036, -0.9512, -0.7773, -0.4373, -0.7856,  0.9985,  0.0811,\n",
      "         -0.7017, -0.7734,  0.4978, -0.0999, -0.3081, -0.9502, -0.3777,  0.3242,\n",
      "          0.7734,  0.0227,  0.3411, -0.5332,  0.2549,  0.0111,  0.0819,  0.6465,\n",
      "         -0.8965, -0.0850, -0.0763, -0.4558, -0.5435, -0.9590,  0.9380, -0.3096,\n",
      "          0.7759,  1.0000,  0.3657, -0.7705,  0.6772,  0.1708, -0.7944,  1.0000,\n",
      "          0.7832, -0.9727, -0.5615,  0.5532, -0.5054, -0.5532,  0.9990, -0.0980,\n",
      "         -0.5986, -0.2803,  0.9834, -0.9839,  0.9966, -0.8218, -0.9463,  0.9380,\n",
      "          0.9131, -0.7466, -0.6479,  0.0620, -0.6714,  0.3318, -0.8018,  0.6333,\n",
      "          0.4771, -0.0750,  0.8364, -0.5278, -0.5796,  0.2207, -0.5122, -0.0437,\n",
      "          0.9409,  0.4780, -0.2783, -0.1290, -0.2952, -0.7915, -0.9458,  0.5278,\n",
      "          1.0000, -0.2556,  0.8149, -0.2947, -0.0881, -0.1656,  0.5376,  0.6030,\n",
      "         -0.2336, -0.7734,  0.6836, -0.8643, -0.9868,  0.4617,  0.1086, -0.3313,\n",
      "          1.0000,  0.4885,  0.2295,  0.2172,  0.9502,  0.0287,  0.3733,  0.7915,\n",
      "          0.9731, -0.2454,  0.5981,  0.5898, -0.8105, -0.3984, -0.6123,  0.0315,\n",
      "         -0.9214,  0.0493, -0.9175,  0.9453,  0.8940,  0.4082,  0.2612,  0.7070,\n",
      "          1.0000, -0.8965,  0.5518,  0.4456,  0.4841, -1.0000, -0.6587, -0.3928,\n",
      "         -0.0853, -0.7524, -0.4465,  0.2120, -0.9487,  0.7739,  0.5229, -0.9531,\n",
      "         -0.9800, -0.2367,  0.6011,  0.0225, -0.9883, -0.5864, -0.5571,  0.5776,\n",
      "         -0.2585, -0.8975,  0.0269, -0.2722,  0.3494, -0.2349,  0.6353,  0.8247,\n",
      "          0.6621, -0.7070, -0.1871, -0.0635, -0.7354,  0.7412, -0.6533, -0.8730,\n",
      "         -0.2000,  1.0000, -0.5688,  0.8887,  0.5649,  0.5127, -0.2297,  0.3298,\n",
      "          0.9312,  0.3801, -0.7339, -0.7441,  0.4187, -0.3245,  0.6646,  0.6733,\n",
      "          0.7388,  0.7490,  0.8330,  0.1803, -0.0134, -0.0375,  0.9849, -0.0669,\n",
      "         -0.0938, -0.3386, -0.1266, -0.4019, -0.0137,  1.0000,  0.2913,  0.3940,\n",
      "         -0.9834, -0.8281, -0.7876,  1.0000,  0.8027, -0.5713,  0.6655,  0.6030,\n",
      "         -0.1979,  0.5947, -0.2886, -0.3452,  0.2510,  0.1025,  0.9312, -0.5752,\n",
      "         -0.9668, -0.6226,  0.3818, -0.9448,  1.0000, -0.5684, -0.3269, -0.5420,\n",
      "          0.0138, -0.7559, -0.1031, -0.9688, -0.1220,  0.2505,  0.9248,  0.1888,\n",
      "         -0.5942, -0.6660,  0.7090,  0.7642, -0.8433, -0.9229,  0.9438, -0.9595,\n",
      "          0.4836,  1.0000,  0.4551, -0.1255,  0.2632, -0.2876,  0.3020, -0.3931,\n",
      "          0.6470, -0.9316, -0.2681, -0.1334,  0.2786, -0.0529, -0.3821,  0.5107,\n",
      "          0.2178, -0.5635, -0.6519, -0.0917,  0.2683,  0.7485, -0.1919, -0.1696,\n",
      "          0.0672, -0.0755, -0.7983, -0.3345, -0.3765, -1.0000,  0.4856, -1.0000,\n",
      "          0.4856,  0.0842, -0.1226,  0.7783,  0.5952,  0.5122, -0.5874, -0.6514,\n",
      "          0.5049,  0.5991, -0.3010, -0.0213, -0.4944,  0.2852, -0.0766,  0.1591,\n",
      "         -0.4670,  0.7344, -0.2000,  1.0000,  0.2098, -0.6338, -0.9185,  0.1533,\n",
      "         -0.2397,  1.0000, -0.6401, -0.9229,  0.3828, -0.6924, -0.7827,  0.3318,\n",
      "          0.0597, -0.7764, -0.9404,  0.8296,  0.5669, -0.6035,  0.3430, -0.3403,\n",
      "         -0.4443,  0.0626,  0.8345,  0.9849,  0.5762,  0.7695,  0.0963, -0.3455,\n",
      "          0.9595,  0.3350, -0.1836, -0.0696,  1.0000,  0.3416, -0.8989, -0.1609,\n",
      "         -0.9424, -0.2325, -0.8735,  0.2172,  0.1108,  0.8101, -0.2112,  0.9004,\n",
      "         -0.6450, -0.0334, -0.4766, -0.5908,  0.3042, -0.8848, -0.9819, -0.9761,\n",
      "          0.5356, -0.4153, -0.0042,  0.1785, -0.0246,  0.3503,  0.3589, -1.0000,\n",
      "          0.8960,  0.3674,  0.8896,  0.9282,  0.7344,  0.4763,  0.3218, -0.9731,\n",
      "         -0.8735, -0.2910, -0.3040,  0.6445,  0.5923,  0.8823,  0.3181, -0.4546,\n",
      "         -0.5254, -0.4070, -0.8730, -0.9902,  0.4463, -0.3230, -0.7715,  0.9507,\n",
      "         -0.1433, -0.1249, -0.0869, -0.7202,  0.5562,  0.6724,  0.2289,  0.1087,\n",
      "          0.4678,  0.8037,  0.8765,  0.9712, -0.8184,  0.6611, -0.4111,  0.4709,\n",
      "          0.9424, -0.9268,  0.2324,  0.4333, -0.3655,  0.2395, -0.2477, -0.8315,\n",
      "          0.6504, -0.3452,  0.6025, -0.3745,  0.1201, -0.3750, -0.0375, -0.7432,\n",
      "         -0.6470,  0.6465,  0.2791,  0.8452,  0.8984,  0.0154, -0.6162, -0.0326,\n",
      "         -0.7080, -0.9150,  0.7485, -0.0351, -0.3135,  0.7793, -0.1555,  0.9512,\n",
      "          0.3044, -0.2166, -0.3562, -0.6450,  0.6382, -0.5552, -0.5464, -0.5864,\n",
      "          0.6147,  0.2964,  1.0000, -0.7451, -0.8193, -0.3213, -0.2678,  0.5112,\n",
      "         -0.4951, -1.0000,  0.3757, -0.2971,  0.7642, -0.6982,  0.8789, -0.5591,\n",
      "         -0.9341, -0.3518,  0.6724,  0.7534, -0.4841, -0.4202,  0.5361,  0.3662,\n",
      "          0.9424,  0.7598, -0.0754,  0.1744,  0.6182, -0.8540, -0.6021,  0.8853]],\n",
      "       dtype=torch.float16, grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "inp = torch.tensor(toks['input_ids'])\n",
    "out = model(inp.unsqueeze(0))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 13, 768]), torch.Size([1, 768]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lhs, pout = out['last_hidden_state'], out['pooler_output']\n",
    "lhs.shape, pout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0029,  0.3921,  0.0427, -0.1011, -0.3064, -0.0775,  0.3308,  0.2949,\n",
      "        -0.3630,  0.1326], dtype=torch.float16, grad_fn=<SliceBackward0>)\n",
      "tensor([-0.7397, -0.4351, -0.8726,  0.6460,  0.6240, -0.1565,  0.7266,  0.2549,\n",
      "        -0.5791, -1.0000], dtype=torch.float16, grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(lhs[0, 0, :10])\n",
    "print(pout[0, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
