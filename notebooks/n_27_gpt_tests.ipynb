{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6db0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a801b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model, pipeline, GPT2LMHeadModel, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13d8ed",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3be02423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 768])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"The Fox is being late for work. The Cat is drinking the coffee. Who is more chill?\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bb481fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c6fa7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Given dictionary list values comma separated. Example. Input: {\"abc\": 1, \"def\": \"2\"}. Answer: 1, \"2\". Input: {\"key\": \"pet\", \"key2\": \"xyz\", \"ArithmeticError\": \"2+2=4\"}. Answer: 4. Input: \"x yz\", \"\\\\t{{name}}=y\\\\t\" Output: {\"hello\": {\"className\":\"DictionaryList\"}. Answer: \"{main:'},\n",
       " {'generated_text': 'Given dictionary list values comma separated. Example. Input: {\"abc\": 1, \"def\": \"2\"}. Answer: 1, \"2\". Input: {\"key\": \"pet\", \"key2\": \"xyz\", \"ArithmeticError\": \"2+2=4\"}. Answer: 1, \"2\". Input: {\"key\": \"xyz\", \"key2\": \"math\", \"arithmeticError\": \"2+1=8\"}. Result: 1, \"'},\n",
       " {'generated_text': 'Given dictionary list values comma separated. Example. Input: {\"abc\": 1, \"def\": \"2\"}. Answer: 1, \"2\". Input: {\"key\": \"pet\", \"key2\": \"xyz\", \"ArithmeticError\": \"2+2=4\"}. Answer: 1, \"4\". Example. Input: {\"foo\": \"3\", \"bar\": 1}. Answer: 3. Example. Input: {\"abc\": \"a\", \"def\":'},\n",
       " {'generated_text': 'Given dictionary list values comma separated. Example. Input: {\"abc\": 1, \"def\": \"2\"}. Answer: 1, \"2\". Input: {\"key\": \"pet\", \"key2\": \"xyz\", \"ArithmeticError\": \"2+2=4\"}. Answer: 2, \"4\".\\n\\nThe example above creates dictionary expressions that, when parsed, return the dictionary string, not the data specified by the dictionary expression. The returned dictionary results should be interpreted'},\n",
       " {'generated_text': 'Given dictionary list values comma separated. Example. Input: {\"abc\": 1, \"def\": \"2\"}. Answer: 1, \"2\". Input: {\"key\": \"pet\", \"key2\": \"xyz\", \"ArithmeticError\": \"2+2=4\"}. Answer: 2, \"2\". Input: {\"word\": \"XYZ\", \"word2\": \"XYZ\"}. Error.\\n\\nReturn value\\n\\nReturn value is set to the value of'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'The Fox is being late for work. The Cat is drinking the coffee. Who is more chill, the Fox or the Cat?'\n",
    "s = 'What is the capital of USA?'\n",
    "s = 'Given dictionary list values comma separated. Example. Input: {\"abc\": 1, \"def\": \"2\"}. Answer: 1, \"2\". Input: {\"key\": \"pet\", \"key2\": \"xyz\", \"ArithmeticError\": \"2+2=4\"}. Answer:'\n",
    "# s = 'Context: The capital of the United States of America is Washington D.C. Question: What is the capital of USA? Answer:'\n",
    "# s = 'Context 2+2=4, 2*2=4. Question: How much is 2 + 2? Answer:'\n",
    "generator(s, max_length=100, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c32d1805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5077494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lmh = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model_lmh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7424f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cfg = GenerationConfig(\n",
    "    max_new_tokens=20,\n",
    "    eos_token_id=tokenizer.sep_token_id,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    # temperature=0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adea7d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Fox is being late for work. The Cat is drinking the coffee. Who is more chill?\"\n",
    "text = 'Who is the president of the United States?'\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "input_ids = encoded_input.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a2546af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the president of the United States?\n",
      "\n",
      "The president of the United States is the president of the United States.\n",
      "\n",
      "The president\n"
     ]
    }
   ],
   "source": [
    "attention_mask = torch.ones(input_ids.shape, device=input_ids.device, dtype=bool)\n",
    "ouput_ids = model_lmh.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "out = tokenizer.decode(ouput_ids.squeeze())\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae3b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8649f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
