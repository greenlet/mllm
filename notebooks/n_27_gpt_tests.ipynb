{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6db0314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a801b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model, pipeline, GPT2LMHeadModel, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13d8ed",
   "metadata": {},
   "source": [
    "## GPT2 Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3be02423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 768])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"The Fox is being late for work. The Cat is drinking the coffee. Who is more chill?\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bb481fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c6fa7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Given dictionary list values comma separated. Example. Input: {\"abc\": 1, \"def\": \"2\"}. Answer: 1, \"2\". Input: {\"key\": \"pet\", \"key2\": \"xyz\", \"ArithmeticError\": \"2+2=4\"}. Answer: 4. Input: \"x yz\", \"\\\\t{{name}}=y\\\\t\" Output: {\"hello\": {\"className\":\"DictionaryList\"}. Answer: \"{main:'},\n",
       " {'generated_text': 'Given dictionary list values comma separated. Example. Input: {\"abc\": 1, \"def\": \"2\"}. Answer: 1, \"2\". Input: {\"key\": \"pet\", \"key2\": \"xyz\", \"ArithmeticError\": \"2+2=4\"}. Answer: 1, \"2\". Input: {\"key\": \"xyz\", \"key2\": \"math\", \"arithmeticError\": \"2+1=8\"}. Result: 1, \"'},\n",
       " {'generated_text': 'Given dictionary list values comma separated. Example. Input: {\"abc\": 1, \"def\": \"2\"}. Answer: 1, \"2\". Input: {\"key\": \"pet\", \"key2\": \"xyz\", \"ArithmeticError\": \"2+2=4\"}. Answer: 1, \"4\". Example. Input: {\"foo\": \"3\", \"bar\": 1}. Answer: 3. Example. Input: {\"abc\": \"a\", \"def\":'},\n",
       " {'generated_text': 'Given dictionary list values comma separated. Example. Input: {\"abc\": 1, \"def\": \"2\"}. Answer: 1, \"2\". Input: {\"key\": \"pet\", \"key2\": \"xyz\", \"ArithmeticError\": \"2+2=4\"}. Answer: 2, \"4\".\\n\\nThe example above creates dictionary expressions that, when parsed, return the dictionary string, not the data specified by the dictionary expression. The returned dictionary results should be interpreted'},\n",
       " {'generated_text': 'Given dictionary list values comma separated. Example. Input: {\"abc\": 1, \"def\": \"2\"}. Answer: 1, \"2\". Input: {\"key\": \"pet\", \"key2\": \"xyz\", \"ArithmeticError\": \"2+2=4\"}. Answer: 2, \"2\". Input: {\"word\": \"XYZ\", \"word2\": \"XYZ\"}. Error.\\n\\nReturn value\\n\\nReturn value is set to the value of'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'The Fox is being late for work. The Cat is drinking the coffee. Who is more chill, the Fox or the Cat?'\n",
    "s = 'What is the capital of USA?'\n",
    "s = 'Given dictionary list values comma separated. Example. Input: {\"abc\": 1, \"def\": \"2\"}. Answer: 1, \"2\". Input: {\"key\": \"pet\", \"key2\": \"xyz\", \"ArithmeticError\": \"2+2=4\"}. Answer:'\n",
    "# s = 'Context: The capital of the United States of America is Washington D.C. Question: What is the capital of USA? Answer:'\n",
    "# s = 'Context 2+2=4, 2*2=4. Question: How much is 2 + 2? Answer:'\n",
    "generator(s, max_length=100, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c32d1805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5077494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lmh = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model_lmh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcf4fa24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.51.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lmh.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7424f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cfg = GenerationConfig(\n",
    "    max_new_tokens=20,\n",
    "    eos_token_id=tokenizer.sep_token_id,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    # temperature=0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adea7d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Fox is being late for work. The Cat is drinking the coffee. Who is more chill?\"\n",
    "text = 'Who is the president of the United States?'\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "input_ids = encoded_input.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a2546af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the president of the United States?\n",
      "\n",
      "The president of the United States is the president of the United States.\n",
      "\n",
      "The president\n"
     ]
    }
   ],
   "source": [
    "attention_mask = torch.ones(input_ids.shape, device=input_ids.device, dtype=bool)\n",
    "ouput_ids = model_lmh.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "out = tokenizer.decode(ouput_ids.squeeze())\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae3b87",
   "metadata": {},
   "source": [
    "## GPT2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e674cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "from regex import T\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f678e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(os.path.expandvars('$HOME')) / 'data'\n",
    "\n",
    "gpt2_train_path = DATA_PATH / 'gpt2_train'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9bccbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "wiki_ds_name, wiki_ds_subdir = '20220301.en', 'wikipedia'\n",
    "# dataset = load_dataset(wiki_ds_subdir, wiki_ds_name, cache_dir=str(DATA_PATH))\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', cache_dir=str(DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "790d7e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5af05a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2522c3db96dc48dca837bbc3a048e78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2').to(device)\n",
    "\n",
    "# Set the EOS token as the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    inputs =  tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "    inputs['labels'] = inputs['input_ids'].copy()\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bbfef3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9180' max='9180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9180/9180 7:03:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.730300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.271700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.281100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.306800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.260600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.298400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.240100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.232700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.239700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.268900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('/home/misha/data/gpt2_train/tokenizer_config.json',\n",
       " '/home/misha/data/gpt2_train/special_tokens_map.json',\n",
       " '/home/misha/data/gpt2_train/vocab.json',\n",
       " '/home/misha/data/gpt2_train/merges.txt',\n",
       " '/home/misha/data/gpt2_train/added_tokens.json',\n",
       " '/home/misha/data/gpt2_train/tokenizer.json')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(gpt2_train_path),\n",
    "    # evaluation_strategy='epoch',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=str(gpt2_train_path),\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "# trainer.train(resume_from_checkpoint=str(gpt2_train_path))\n",
    "trainer.train()\n",
    "\n",
    "# save the model and tokenizer explicitly\n",
    "model_output_dir = str(gpt2_train_path)\n",
    "\n",
    "model.save_pretrained(model_output_dir)\n",
    "tokenizer.save_pretrained(model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02d439a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db5b1920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president of the United States is \n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "text = \"The Fox is being late for work. The Cat is drinking the coffee. Who is more chill?\"\n",
    "text = 'Who is the president of the United States?'\n",
    "text = 'The president of the United States is '\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "input_ids = encoded_input.input_ids\n",
    "attention_mask = torch.ones(input_ids.shape, device=input_ids.device, dtype=bool)\n",
    "ouput_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "out = tokenizer.decode(ouput_ids.squeeze())\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19ec447c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ''}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21023b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce474d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae424d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8649f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
