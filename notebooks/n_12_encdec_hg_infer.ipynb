{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misha/miniconda3/envs/mllm/lib/python3.10/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/misha/miniconda3/envs/mllm/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Optional\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pydantic_yaml import parse_yaml_file_as\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, AddedToken, PreTrainedTokenizer\n",
    "\n",
    "from mllm.data.wiki.dswiki import WikiDsLoader\n",
    "from mllm.exp.args import TOKENIZER_CFG_FNAME, ENCDEC_HG_MODEL_CFG_FNAME\n",
    "from mllm.model.encdec_hg import EncdecHg\n",
    "from mllm.config.model import TokenizerCfg, EncdecHgCfg\n",
    "from mllm.tokenization.chunk_tokenizer import tokenizer_from_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(os.path.expandvars('$HOME')) / 'data'\n",
    "WIKI_DS_NAME = '20200501.en'\n",
    "\n",
    "TRAIN_ENCDEC_HG_PATH = DATA_PATH / 'train_mllm_encdec_hg'\n",
    "encdec_subdir = 'encdechg-20241202_225258-ilen256-lrs8-step2-d256-h8'\n",
    "\n",
    "encdec_train_path = TRAIN_ENCDEC_HG_PATH / encdec_subdir\n",
    "encdec_snapshot_fpath = encdec_train_path / 'best.pth'\n",
    "encdec_model_cfg_fpath = encdec_train_path / ENCDEC_HG_MODEL_CFG_FNAME\n",
    "encdec_tkz_cfg_fpath = encdec_train_path / TOKENIZER_CFG_FNAME\n",
    "\n",
    "device_name = 'cpu'\n",
    "# device_name = 'cuda'\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikipedia (/home/misha/data/wikipedia/20200501.en/1.0.0/009f923d9b6dd00c00c8cdc7f408f2b47f45dd4f5fb7982a21f9448f4afbe475)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d90165bf78043f7985d0ff3248a6268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia 20200501.en docs: 6078422\n"
     ]
    }
   ],
   "source": [
    "dss = load_dataset('wikipedia', WIKI_DS_NAME, beam_runner='DirectRunner', cache_dir=str(DATA_PATH))\n",
    "ds: Dataset = dss['train']\n",
    "n_docs = len(ds)\n",
    "print(f'Wikipedia {WIKI_DS_NAME} docs: {n_docs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkz_cfg = parse_yaml_file_as(TokenizerCfg, encdec_tkz_cfg_fpath)\n",
    "tkz = tokenizer_from_config(tkz_cfg)\n",
    "model_cfg = parse_yaml_file_as(EncdecHgCfg, encdec_model_cfg_fpath)\n",
    "inp_len = model_cfg.enc_pyr.inp_len\n",
    "pad_tok = tkz_cfg.custom_tokens['pad'].ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chkpt = torch.load(encdec_snapshot_fpath, map_location=device)\n",
    "model = EncdecHg(model_cfg).to(device)\n",
    "strict = True\n",
    "# strict = False\n",
    "model.load_state_dict(chkpt['model'], strict=strict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_tokens(doc_inds: list[int], randomize: bool = False) -> torch.Tensor:\n",
    "    docs_toks = np.full((len(doc_inds), inp_len), pad_tok)\n",
    "    for i, doc_ind in enumerate(doc_inds):\n",
    "        doc = ds[doc_ind]\n",
    "        title, text = doc['title'], doc['text']\n",
    "        doc_txt = f'{title} {text}'\n",
    "        doc_toks: list[int] = tkz(doc_txt)['input_ids']\n",
    "        n_toks = len(doc_toks)\n",
    "        if n_toks > inp_len:\n",
    "            i_off = np.random.randint(n_toks - inp_len + 1) if randomize else 0\n",
    "            doc_toks = doc_toks[i_off:i_off + inp_len]\n",
    "        docs_toks[i, :len(doc_toks)] = doc_toks\n",
    "    docs_toks_t = torch.from_numpy(docs_toks).to(device)\n",
    "    return docs_toks_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 \"Yangliuqing\" Yangliuqing () is a market town in Xiqing District, in the western suburbs of Tianjin, People's Republic of China. Despite its relatively small size, it has been named since 2006 in the \"famous historical and cultural market towns in China\".\\n\\nIt is best known in China for creating nianhua or Yangl\n",
      "001 \"Orana Australia Ltd\" Orana Australia Ltd is a not-for-profit organisation that provides a diverse range of training and support services to over 650 people with disabilities and their families in South Australia.\\n\\nHistory\\nThe Mentally Retarded Children’s Society of SA Inc. was established in 1950 by a group of parent\n",
      "002 \"St. Mary's Church, Sønderborg\" The St. Mary's Church is a church owned by the Church of Denmark in Sønderborg, Denmark and the church of the parish with the same name. Thanks to its location on a hill, the church building is very iconic for the city.\\n\\nHistory \\nIn the Middle Ages there was a leper colony on a hill just outside \n",
      "003 \"Kalitta\" Kalitta may refer to:\\n\\nConnie Kalitta (born 1938), a retired American drag racer and CEO of the eponymous Kallita Air.\\nDoug Kalitta (born 1964), an American drag racer, nephew of Connie Kalitta and owner of Kalitta Charters.\\nScott Kalitta (1962-2008), an American drag racer and son of Connie Kal\n",
      "004 \"Where Is Freedom?\" Where Is Freedom? () is a 1954 Italian comedy-drama film directed by Roberto Rossellini. \\n \\nThe film had a troubled production because, after shooting some scenes, Rossellini lost interest in the film and abandoned the set. The work was completed after about a year, mainly from Mario Monicelli, wi\n"
     ]
    }
   ],
   "source": [
    "doc_inds = np.arange(5)\n",
    "doc_inds = [x.item() for x in doc_inds]\n",
    "for doc_ind in doc_inds:\n",
    "    doc = ds[doc_ind]\n",
    "    title, text = doc['title'], doc['text'].replace('\\n', '\\\\n')\n",
    "    print(f'{doc_ind:03d} \"{title}\" {text[:300]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 256, 50271])\n",
      "torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "docs_toks_in = get_batch_tokens(doc_inds)\n",
    "logits_pred = model(docs_toks_in)\n",
    "probs_pred = torch.softmax(logits_pred, dim=-1)\n",
    "# probs_pred = torch.sigmoid(logits_pred)\n",
    "print(probs_pred.shape)\n",
    "docs_toks_out = torch.argmax(probs_pred, dim=-1)\n",
    "print(docs_toks_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000  ( ( ( ( ( ( ( ( ( () is a the of of ( (,.) is the the the the of of of, of. in century. The is the the the the the the,,,,,,, the the the the the the the..\\n\\nThe is is the the the the the the ( ( ( ( ( ( ( ( ( (i. The is the the the the the the,,,,,,, the the the the the the the the the the the the., the the the the the the..,, the the the the the the the the the the the the the the the the the..\\n\\nThe is the the the the the the the the the the the.\\n\\nThe:\\n of of:\\n\\nThe:\\n ( ( ( (\\n\\n\\n�� ( ( ( ( ( (������������������\\n) is the the the ( ( ( ( ( ( ( ( (.. The the the the the the the of. of.., the the the the the the.., the the the the the the the the the of., the the the\n",
      "001  ( ( ( ( ( (.) is a a the the the,,, the the the the the the, and the the the the the the the the the the the the..\\n\\nHistory\\n\\n\\n\\n\\n\\n,,,,,,,., and the the the the the,,, the the, the the the the the the the the the the,,,,,,,, the the the to the the time.\\n\\nIn the,,,, is, the the the the the the the, the the the the the the the the, and the the the the.\\n\\nThe the is the the the the the the the,,,,, the the the,,,,,, and and ins.\\n\\nIn the,, is the the the the the the the the the the the the the..\\n\\nThe the,, the the the the the the,,,,,,,,,,,,,, and the the,.\\n\\nThe the, the the the the the the the the the,,,,, the the the the the the..\\n\\nReferences\\n\\n links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\n",
      "002  ( ( ( ( ( (,,,,,,,.) is a a the the the the the the the the,,,,,. It was the the the the the the the.. The was the the the the the the the the the the the the the the area.\\n\\nHistory\\n\\nThe the,,, was the the the the the the the the the in.. The was the the the the the the the the the the the the the the the the the the the the the,., the.. The was the the the the the the the the the,,,,, the the the the the the in,., the the the in.. The was the the the the the,,,,,,,,,, ( (, (,., was the the the the the, the the the the, the the the the the the the the the in..\\n\\nHistory\\n\\n\\n\\n\\n\\n\\n\\n,,,,, the the the the the the the the, and the the the the the the.. The the the the the the the the the,,,, the the the the the the the the the the the.. The the the\n",
      "003 C. ( ( may refer to:\\n\\nPeople ( ( ( ( (,), a a of the the the the the of, of,,,,)\\n The- ( ( (,, a),,,\\n\\n the the the the,,,,,,,)\\n The ( ( ( ( ( ()), a a the the the the the in,,\\n\\nS ( ( ( (,,,,,,,,,,),,,,, a a the of<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "004 ,, ( ( ( and (. The is the by by by by,,,, and and and and,.\\n\\nThe theThe,, is the the the the the the the the and.., is the the the the the the the.. The is the the the,, the the....,., and the the the the the,,,.., and,., and the the the the the,,, the the the the film.\\n\\nPlotception\\n\\nThe film the the the the the the \".., is the the the the, the the, the the the the the film.\\n\\nCast listing\\n\\n\\n\\n\\n as as as as as as as as \\nS as as as as as as as as \\n \". as as as\\n\\n\\n as as  \\n \" as as as\\n as\\n\\n\\n\\n\\n\\n\\n\\n as as as as as as \\n \". as as as\\n\\n as as as as \\n \". as as as\\n\\n\\n\\n as as  \\n \". as as as\\n\\n\\n\\n as as as \\nS. as as as as\\n\\n\\n\\n\\n\\n\\n\\n\n"
     ]
    }
   ],
   "source": [
    "for i, doc_ind in enumerate(doc_inds):\n",
    "    s = tkz.decode(docs_toks_out[i])\n",
    "    s = s.replace('\\n', '\\\\n')\n",
    "    print(f'{doc_ind:03d} {s}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
