{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misha/miniconda3/envs/mllm/lib/python3.10/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/misha/miniconda3/envs/mllm/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Optional\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pydantic_yaml import parse_yaml_file_as\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, AddedToken, PreTrainedTokenizer\n",
    "\n",
    "from mllm.data.wiki.dswiki import WikiDsLoader\n",
    "from mllm.exp.args import TOKENIZER_CFG_FNAME, ENCDEC_HG_MODEL_CFG_FNAME\n",
    "from mllm.model.encdec_hg import EncdecHg\n",
    "from mllm.config.model import TokenizerCfg, EncdecHgCfg\n",
    "from mllm.tokenization.chunk_tokenizer import tokenizer_from_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(os.path.expandvars('$HOME')) / 'data'\n",
    "WIKI_DS_NAME = '20200501.en'\n",
    "\n",
    "TRAIN_ENCDEC_HG_PATH = DATA_PATH / 'train_mllm_encdec_hg'\n",
    "# encdec_subdir = 'encdechg-20241202_225258-ilen256-lrs8-step2-d256-h8'\n",
    "encdec_subdir = 'encdechg-20241203_233923-ilen256-lrs8-step2-d256-h8'\n",
    "\n",
    "encdec_train_path = TRAIN_ENCDEC_HG_PATH / encdec_subdir\n",
    "encdec_snapshot_fpath = encdec_train_path / 'best.pth'\n",
    "encdec_model_cfg_fpath = encdec_train_path / ENCDEC_HG_MODEL_CFG_FNAME\n",
    "encdec_tkz_cfg_fpath = encdec_train_path / TOKENIZER_CFG_FNAME\n",
    "\n",
    "device_name = 'cpu'\n",
    "# device_name = 'cuda'\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikipedia (/home/misha/data/wikipedia/20200501.en/1.0.0/009f923d9b6dd00c00c8cdc7f408f2b47f45dd4f5fb7982a21f9448f4afbe475)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb9898d31874a438063e098bbd0db1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia 20200501.en docs: 6078422\n"
     ]
    }
   ],
   "source": [
    "dss = load_dataset('wikipedia', WIKI_DS_NAME, beam_runner='DirectRunner', cache_dir=str(DATA_PATH))\n",
    "ds: Dataset = dss['train']\n",
    "n_docs = len(ds)\n",
    "print(f'Wikipedia {WIKI_DS_NAME} docs: {n_docs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkz_cfg = parse_yaml_file_as(TokenizerCfg, encdec_tkz_cfg_fpath)\n",
    "tkz = tokenizer_from_config(tkz_cfg)\n",
    "model_cfg = parse_yaml_file_as(EncdecHgCfg, encdec_model_cfg_fpath)\n",
    "inp_len = model_cfg.enc_pyr.inp_len\n",
    "pad_tok = tkz_cfg.custom_tokens['pad'].ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncdecHg(\n",
       "  (enc_pyr): EncoderPyramid(\n",
       "    (vocab_encoder): VocabEncoder(\n",
       "      (src_word_emb): Embedding(50271, 256, padding_idx=50267)\n",
       "      (position_enc): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (enc_layers): ModuleList(\n",
       "      (0-7): 8 x EncoderLayer(\n",
       "        (slf_attn): MultiHeadAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (fc): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ffn): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rdc_layers): ModuleList(\n",
       "      (0-7): 8 x ReduceLayer(\n",
       "        (reducer): Linear(in_features=512, out_features=256, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dec_pyr): DecoderPyramid(\n",
       "    (enc_layers): ModuleList(\n",
       "      (0-7): 8 x EncoderLayer(\n",
       "        (slf_attn): MultiHeadAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (fc): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ffn): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (enh_layers): ModuleList(\n",
       "      (0-7): 8 x EnhanceLayer(\n",
       "        (enhancer): Linear(in_features=256, out_features=512, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (vocab_decoder): VocabDecoder(\n",
       "      (word_prj): Linear(in_features=256, out_features=50271, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chkpt = torch.load(encdec_snapshot_fpath, map_location=device)\n",
    "model = EncdecHg(model_cfg).to(device)\n",
    "strict = True\n",
    "# strict = False\n",
    "model.load_state_dict(chkpt['model'], strict=strict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_tokens(doc_inds: list[int], randomize: bool = False) -> torch.Tensor:\n",
    "    docs_toks = np.full((len(doc_inds), inp_len), pad_tok)\n",
    "    for i, doc_ind in enumerate(doc_inds):\n",
    "        doc = ds[doc_ind]\n",
    "        title, text = doc['title'], doc['text']\n",
    "        doc_txt = f'{title} {text}'\n",
    "        doc_toks: list[int] = tkz(doc_txt)['input_ids']\n",
    "        n_toks = len(doc_toks)\n",
    "        if n_toks > inp_len:\n",
    "            i_off = np.random.randint(n_toks - inp_len + 1) if randomize else 0\n",
    "            doc_toks = doc_toks[i_off:i_off + inp_len]\n",
    "        docs_toks[i, :len(doc_toks)] = doc_toks\n",
    "    docs_toks_t = torch.from_numpy(docs_toks).to(device)\n",
    "    return docs_toks_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 \"Yangliuqing\" Yangliuqing () is a market town in Xiqing District, in the western suburbs of Tianjin, People's Republic of China. Despite its relatively small size, it has been named since 2006 in the \"famous historical and cultural market towns in China\".\\n\\nIt is best known in China for creating nianhua or Yangl\n",
      "001 \"Orana Australia Ltd\" Orana Australia Ltd is a not-for-profit organisation that provides a diverse range of training and support services to over 650 people with disabilities and their families in South Australia.\\n\\nHistory\\nThe Mentally Retarded Children’s Society of SA Inc. was established in 1950 by a group of parent\n",
      "002 \"St. Mary's Church, Sønderborg\" The St. Mary's Church is a church owned by the Church of Denmark in Sønderborg, Denmark and the church of the parish with the same name. Thanks to its location on a hill, the church building is very iconic for the city.\\n\\nHistory \\nIn the Middle Ages there was a leper colony on a hill just outside \n",
      "003 \"Kalitta\" Kalitta may refer to:\\n\\nConnie Kalitta (born 1938), a retired American drag racer and CEO of the eponymous Kallita Air.\\nDoug Kalitta (born 1964), an American drag racer, nephew of Connie Kalitta and owner of Kalitta Charters.\\nScott Kalitta (1962-2008), an American drag racer and son of Connie Kal\n",
      "004 \"Where Is Freedom?\" Where Is Freedom? () is a 1954 Italian comedy-drama film directed by Roberto Rossellini. \\n \\nThe film had a troubled production because, after shooting some scenes, Rossellini lost interest in the film and abandoned the set. The work was completed after about a year, mainly from Mario Monicelli, wi\n"
     ]
    }
   ],
   "source": [
    "doc_inds = np.arange(5)\n",
    "doc_inds = [x.item() for x in doc_inds]\n",
    "for doc_ind in doc_inds:\n",
    "    doc = ds[doc_ind]\n",
    "    title, text = doc['title'], doc['text'].replace('\\n', '\\\\n')\n",
    "    print(f'{doc_ind:03d} \"{title}\" {text[:300]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 256, 50271])\n",
      "torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "docs_toks_in = get_batch_tokens(doc_inds)\n",
    "logits_pred = model(docs_toks_in)\n",
    "probs_pred = torch.softmax(logits_pred, dim=-1)\n",
    "# probs_pred = torch.sigmoid(logits_pred)\n",
    "print(probs_pred.shape)\n",
    "docs_toks_out = torch.argmax(probs_pred, dim=-1)\n",
    "print(docs_toks_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000  the of,,,, (,,,) is a the the of of..,, was the the the the the the the the the the the.., was a the the the the the the the the the the the the the the the the the the the..\\n\\nThe the the the the the the the the the the the the,,,, (,,,, the the the the the the..., is the to the the the the,, the the the the the the the.., the the the the the.., the the the the the the the the the the the the the the the the the the..\\n\\nThe the the the the the the the the the of the century.\\n\\nThe\\n\\n of of \\n\\nThe of,,,\\n\\n\\n\\n\\n,,,,,,,,,,,,,,,,,,,,,,,, (,, is a the the the the the the the,, of..., was the the the of the the of...., was the the the the the.., was the the the the the the of..., was the the\n",
      "001  ( ( ( ( (..) is a the the the,,, the to the the the the,, the the the the the the the the the the the the the..\\n\\nHistory\\n\\n\\n\\n\\n the the the,,,,.., was a the the the the the, the the the the the..., was the the the the,,,, the the the the the the the the the thes.\\n\\nIn the,,,, was the the the the the the,,, the the the the the.., and the the the the..\\n\\nThe the,, the the the the the the the,,,,, the the the the the the the the the the the..\\n\\nThe the,,,, the the the the the, the, the, and and,.\\n\\nThe the,, the the the the,,,,,,,,,,,,,,,,,,,..\\n\\nThe, is the the the the the the the the,,,,, the the the the the the the..\\n\\nSee\\n\\n links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\n",
      "002  the,,,,,,,,,,,,,.) was a the the the the the the the the,,,..,, the the the the the the the the.., was was to to the the, the the the the the the the the..\\n\\nHistory \\nThe the,, the the the the the the the the the the of..., was the the the the the the the the the the the.., was was the the the the the the the..., was the the the the the the the,,,,.,,, was the the the the of of.., was the the the the, the the the the the the the the,,,,.,, the,, and and (.., was a the the the the, the the the the was to to the the the the the the the the..\\n\\nThe\\n,,,,,, the the the the the,,, was was to to the the the the the the the. the the.,, was a to the the the the the the.,,, the the the the the the the the the..., was was\n",
      "003 John. John John may refer to:\\n\\nJohn ( ( ( ( (19),),),), the the of of of of of of of,,, States\\nJohn ( ( ( ( (),),),),\\n\\n\\n\\n ( ( (),),),), of,,)\\nJohn ( ( ( ( ( (),), a a the of of of of,,, States\\n The ( (, a a a a of of,, the the,,,<|pad|><|pad|> a a the the the the<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "004 , ( ( ( ( ( (.) is a by film film film directed film film by by by and and film.\\n\\nThe\\n,,, is a a the the the the the the...., is the the the the the the..,, is the the the the the the......., is a the the the the the the the,,,,, and and,,.., is is the the the the the film.\\n\\nCast \\n\\n\\n\\n\\n the the the the,,,.,, is the the the the the the the the the the the in film.\\n\\nCast\\n\\n\\n\\n\\n\\n as as as as as as as as \\n \". as as as as as as as \\n \". as as as as\\n as as as as \\n \". as as as as as\\n\\n\\n\\n\\n\\n\\n as as as as as as \\n \". as as as as\\n as as as as \\n \". as as as\\n\\n\\n as as as as \\n \". as as as as\\n\\n as as as as \\n \". as as as as as\\n\\n\\n\\n\\n\\n\\n\n"
     ]
    }
   ],
   "source": [
    "for i, doc_ind in enumerate(doc_inds):\n",
    "    s = tkz.decode(docs_toks_out[i])\n",
    "    s = s.replace('\\n', '\\\\n')\n",
    "    print(f'{doc_ind:03d} {s}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
