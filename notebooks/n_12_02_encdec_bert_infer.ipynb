{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Optional\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pydantic_yaml import parse_yaml_file_as\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, AddedToken, PreTrainedTokenizer, AutoTokenizer\n",
    "\n",
    "from mllm.data.wiki.dswiki import WikiDsLoader\n",
    "from mllm.exp.args import ENCDEC_BERT_MODEL_CFG_FNAME\n",
    "from mllm.model.encdec_ranker_hg import EncdecBert\n",
    "from mllm.config.model import EncdecBertCfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(os.path.expandvars('$HOME')) / 'data'\n",
    "WIKI_DS_NAME = '20200501.en'\n",
    "\n",
    "TRAIN_ENCDEC_BERT_PATH = DATA_PATH / 'train_mllm_encdec_bert'\n",
    "encdec_subdir = 'encdecbert-20250126_212805-bert-base-uncased-d768-emb_cls-inp128-lrs7x1-enh_mmbb-step2-h12-dp0-t0.0'\n",
    "\n",
    "encdec_train_path = TRAIN_ENCDEC_BERT_PATH / encdec_subdir\n",
    "encdec_snapshot_fpath = encdec_train_path / 'best.pth'\n",
    "encdec_model_cfg_fpath = encdec_train_path / ENCDEC_BERT_MODEL_CFG_FNAME\n",
    "\n",
    "device_name = 'cpu'\n",
    "# device_name = 'cuda'\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikipedia (/home/misha/data/wikipedia/20200501.en/1.0.0/009f923d9b6dd00c00c8cdc7f408f2b47f45dd4f5fb7982a21f9448f4afbe475)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17704a3b500411d990b305aba3e2ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia 20200501.en docs: 6078422\n"
     ]
    }
   ],
   "source": [
    "dss = load_dataset('wikipedia', WIKI_DS_NAME, beam_runner='DirectRunner', cache_dir=str(DATA_PATH))\n",
    "ds: Dataset = dss['train']\n",
    "n_docs = len(ds)\n",
    "print(f'Wikipedia {WIKI_DS_NAME} docs: {n_docs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_bert=EncBertCfg(inp_len=128, d_model=768, pretrained_model_name='bert-base-uncased', tokenizer_name='bert-base-uncased', emb_type=<BertEmbType.Cls: 'cls'>) dec_pyr=DecPyrCfg(d_model=768, n_heads=12, d_k=64, d_v=64, d_inner=3072, inp_len=128, step=2, n_layers=7, dropout_rate=0.0, n_vocab=30522, n_similar_layers=1, enhance_type=<HgEnhanceType.MatmulBeginBias: 'mmbb'>, temperature=0.0)\n",
      "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model_cfg = parse_yaml_file_as(EncdecBertCfg, encdec_model_cfg_fpath)\n",
    "tkz = AutoTokenizer.from_pretrained(model_cfg.enc_bert.pretrained_model_name)\n",
    "print(model_cfg)\n",
    "print(tkz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncdecBert(\n",
       "  (enc_bert): EncoderBert(\n",
       "    (bert_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dec_pyr): DecoderPyramid(\n",
       "    (enc_layers): ModuleList(\n",
       "      (0-6): 7 x EncoderLayer(\n",
       "        (slf_attn): MultiHeadAttention(\n",
       "          (w_qs): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (w_ks): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (w_vs): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (fc): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ffn): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (enh_beg_layer): Linear(in_features=768, out_features=98304, bias=True)\n",
       "    (vocab_decoder): VocabDecoder(\n",
       "      (word_prj): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chkpt = torch.load(encdec_snapshot_fpath, map_location=device)\n",
    "model = EncdecBert(model_cfg).to(device)\n",
    "strict = True\n",
    "# strict = False\n",
    "model.load_state_dict(chkpt['model'], strict=strict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_len = model_cfg.enc_bert.inp_len\n",
    "\n",
    "def get_batch_tokens(doc_inds: list[int], randomize: bool = False) -> torch.Tensor:\n",
    "    docs_toks = np.full((len(doc_inds), inp_len), tkz.pad_token_id)\n",
    "    for i, doc_ind in enumerate(doc_inds):\n",
    "        doc = ds[doc_ind]\n",
    "        title, text = doc['title'], doc['text']\n",
    "        doc_txt = f'{title} {text}'\n",
    "        doc_txt = text\n",
    "        doc_txt = doc_txt.lower()\n",
    "        doc_toks: list[int] = tkz(doc_txt)['input_ids']\n",
    "        n_toks = len(doc_toks)\n",
    "        if n_toks > inp_len:\n",
    "            i_off = np.random.randint(n_toks - inp_len + 1) if randomize else 0\n",
    "            doc_toks = doc_toks[i_off:i_off + inp_len]\n",
    "        docs_toks[i, :len(doc_toks)] = doc_toks\n",
    "    docs_toks_t = torch.from_numpy(docs_toks).to(device)\n",
    "    return docs_toks_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 \"Yangliuqing\" Yangliuqing () is a market town in Xiqing District, in the western suburbs of Tianjin, People's Republic of China. Despite its relatively small size, it has been named since 2006 in the \"famous historical and cultural market towns in China\".\\n\\nIt is best known in China for creating nianhua or Yangl\n",
      "001 \"Orana Australia Ltd\" Orana Australia Ltd is a not-for-profit organisation that provides a diverse range of training and support services to over 650 people with disabilities and their families in South Australia.\\n\\nHistory\\nThe Mentally Retarded Children’s Society of SA Inc. was established in 1950 by a group of parent\n",
      "002 \"St. Mary's Church, Sønderborg\" The St. Mary's Church is a church owned by the Church of Denmark in Sønderborg, Denmark and the church of the parish with the same name. Thanks to its location on a hill, the church building is very iconic for the city.\\n\\nHistory \\nIn the Middle Ages there was a leper colony on a hill just outside \n",
      "003 \"Kalitta\" Kalitta may refer to:\\n\\nConnie Kalitta (born 1938), a retired American drag racer and CEO of the eponymous Kallita Air.\\nDoug Kalitta (born 1964), an American drag racer, nephew of Connie Kalitta and owner of Kalitta Charters.\\nScott Kalitta (1962-2008), an American drag racer and son of Connie Kal\n",
      "004 \"Where Is Freedom?\" Where Is Freedom? () is a 1954 Italian comedy-drama film directed by Roberto Rossellini. \\n \\nThe film had a troubled production because, after shooting some scenes, Rossellini lost interest in the film and abandoned the set. The work was completed after about a year, mainly from Mario Monicelli, wi\n"
     ]
    }
   ],
   "source": [
    "doc_inds = np.arange(5)\n",
    "# doc_inds += 5\n",
    "doc_inds = [x.item() for x in doc_inds]\n",
    "for doc_ind in doc_inds:\n",
    "    doc = ds[doc_ind]\n",
    "    title, text = doc['title'], doc['text'].replace('\\n', '\\\\n')\n",
    "    print(f'{doc_ind:03d} \"{title}\" {text[:300]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 128, 30522])\n",
      "torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "docs_toks_in = get_batch_tokens(doc_inds)\n",
    "logits_pred = model(docs_toks_in, docs_toks_in != tkz.pad_token_id)\n",
    "probs_pred = torch.softmax(logits_pred, dim=-1)\n",
    "# probs_pred = torch.sigmoid(logits_pred)\n",
    "print(probs_pred.shape)\n",
    "docs_toks_out = torch.argmax(probs_pred, dim=-1)\n",
    "print(docs_toks_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 [CLS] yangliujiang ( ) is a high town in xijiang district, in the western suburbs of shanghai, people's republic of china. despite its very small decade, it has been named since 2006 in the many famous cultural and cultural style architecture in china \". it is best known in china for both nianhua and yangliujiang nianhua. for more than 200 years, yangliujiang was in people growing in the terms of these woodcuts for the new year. wood style shops using classic anloxia to describe traditional style of people's games without interfaong with intercut,.\n",
      "001 [CLS] orana australia ) is a not - in - profit organisation that provides a wide range of training and support services to over 200 people with accommodation and their schools in south australia. history the four repetting children ’ s organisation of saf. was established in 1950 by a group of children who provided education, employment and care opportunities for their children within the local community at a work when child care care in adelaide was a only alternative. the organisation ’ s organisation were to develop education and training programs for services with financial disabilities, to provide disabled care, and to provide charitable orphanages. a number of disabled centres were established, the the\n",
      "002 [CLS] the st. mary's church is a church owned by the church of denmark in søloborg, denmark and the church of the parish with the same name. thanks to its location on a hill, the church building is very popular for the city. history in the middle ages there was a stper chapel on a square and over the city. it was named built st george and about white the chapel of the stper chapel stands in the area of the present st. mary's church. after the old parish church of the city, the st. nicholas church, was demolished around 1530, the saint -..\n",
      "003 [CLS] kalitta may refer to : larry kalitta ( born 1938 ), a retired american jetboat and ceo of the former kallita air. larry kalitta ( born 1964 ), an american jet racer, grandson of larry kalitta and owner of kalitta airline. scott kalitta ( 1964 – 2008 ), an american jetboat and son of larry kalitta. kalitta air, a cargo airline flying fighter pilot aircraft. kalitta airline, a cargo airline flying medium - sized aircraft. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "004 , it is freedom? ( ) is a 1950 italian crime - drama film directed by roberto renellini. the film had a young production when, after prison some success, renellini sets roles in the film and spent the time. the work was released after spending a year, working by roberto moninolli, with some films are directed by valeio celci and mario callini. plot and, renellini is the best assistant director of the film. plot himself and friends of an ex - man. emodrated and diquilusioned by life, he eventually eventually leave his return to prison. plot. as\n"
     ]
    }
   ],
   "source": [
    "for i, doc_ind in enumerate(doc_inds):\n",
    "    s = tkz.decode(docs_toks_out[i])\n",
    "    s = s.replace('\\n', '\\\\n')\n",
    "    print(f'{doc_ind:03d} {s}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder embedding evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(txts: list[str]) -> torch.Tensor:\n",
    "    batch_toks = np.full((len(txts), inp_len), tkz.pad_token_id)\n",
    "    for i, txt in enumerate(txts):\n",
    "        toks: list[int] = tkz(txt)['input_ids']\n",
    "        n_toks = len(toks)\n",
    "        if n_toks > inp_len:\n",
    "            i_off = np.random.randint(n_toks - inp_len + 1)\n",
    "            toks = toks[i_off:i_off + inp_len]\n",
    "        batch_toks[i, :len(toks)] = toks\n",
    "    batch_toks_t = torch.from_numpy(batch_toks).to(device)\n",
    "    return batch_toks_t\n",
    "\n",
    "model.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 768])\n"
     ]
    }
   ],
   "source": [
    "txts = [\n",
    "    '\"Orana Australia Ltd\" Orana Australia Ltd is a not-for-profit organisation that provides a diverse range of training and support services to over 650 people with disabilities and their families in South Australia.\\n\\nHistory\\nThe Mentally Retarded Children’s Society of SA Inc. was established in 1950 by a group of parent',\n",
    "    'Australia',\n",
    "    'Orana Australia Ltd',\n",
    "    'Hello Kitty',\n",
    "]\n",
    "batch_toks = get_tokens(txts)\n",
    "embs = model.enc_bert(batch_toks, batch_toks != tkz.pad_token_id)\n",
    "# embs = embs.detach().cpu().numpy()\n",
    "embs = embs.detach().cpu()\n",
    "print(embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia [0.11428338] tensor(12.3411)\n",
      "Orana Australia Ltd [0.1988959] tensor(11.7642)\n",
      "Hello Kitty [0.07278581] tensor(12.4557)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(embs)):\n",
    "    cos_dist = F.cosine_similarity(embs[0:1], embs[i:i + 1])\n",
    "    norm_dist = torch.norm(embs[0] - embs[i])\n",
    "    print(txts[i], cos_dist.numpy(), norm_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
