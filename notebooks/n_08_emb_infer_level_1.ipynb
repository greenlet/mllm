{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Optional\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pydantic_yaml import parse_yaml_file_as\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, AddedToken, PreTrainedTokenizer\n",
    "\n",
    "from mllm.config.model import MllmRankerCfg, MllmEncdecCfg, TokenizerCfg\n",
    "from mllm.data.dsqrels_embs import DsQrelsEmbs, QrelsEmbsBatch\n",
    "from mllm.data.utils import load_qrels_datasets\n",
    "from mllm.exp.args import ENCDEC_MODEL_CFG_FNAME, RANKER_MODEL_CFG_FNAME\n",
    "from mllm.model.mllm_encdec import MllmEncdecLevel\n",
    "from mllm.model.mllm_ranker import RankProbLoss, MllmRanker, MllmRankerLevel\n",
    "from mllm.tokenization.chunk_tokenizer import gen_all_tokens, ChunkTokenizer, tokenizer_from_config\n",
    "from mllm.train.utils import find_create_train_path, calc_print_batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranker level 1 inference\n",
    "## Config and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranker cfg fpath: /home/misha/data/train_mllm_ranker_qrels_0/ranker-20241021_062053-msmarco-fever/ranker_model_cfg.yaml. Exists: True\n",
      "Ranker cfg fpath: /home/misha/data/train_mllm_ranker_qrels_1/ranker-lvl1-20241025_223420-enc-lrs2-embmatTrue-d256-h8-dec-lrs2-d256-h8-encdec-20241022_224217/ranker_model_cfg.yaml. Exists: True\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(os.path.expandvars('$HOME')) / 'data'\n",
    "DS_MSMARCO_DIR_PATH = DATA_PATH / 'msmarco'\n",
    "DS_FEVER_DIR_PATH = DATA_PATH / 'fever'\n",
    "TRAIN_RANKER_PATH = DATA_PATH / 'train_mllm_ranker_qrels_0'\n",
    "TRAIN_RANKER_EMBS_PATH = DATA_PATH / 'train_mllm_ranker_qrels_1'\n",
    "DS_WIKI_DIR_PATH = DATA_PATH / 'wiki_20200501_en/ch_100_fixed'\n",
    "DS_EMBS_DIR_PATH = DATA_PATH / 'ranker_embs_msmarco_fever'\n",
    "CFG_DIR_PATH = Path(os.path.abspath('.')).parent / 'mllm/config/cfg'\n",
    "\n",
    "tokenizer_cfg_fpath = CFG_DIR_PATH / 'tokenizer_cfg_02.yaml'\n",
    "\n",
    "ranker0_subdir = 'ranker-20241021_062053-msmarco-fever'\n",
    "ranker0_train_path = TRAIN_RANKER_PATH / ranker0_subdir\n",
    "ranker0_snapshot_fpath = ranker0_train_path / 'best.pth'\n",
    "\n",
    "ranker1_subdir = 'ranker-lvl1-20241023_220614-enc-lrs2-embmatTrue-d256-h8-dec-lrs2-d256-h8-encdec-20241022_224217'\n",
    "ranker1_subdir = 'ranker-lvl1-20241025_223420-enc-lrs2-embmatTrue-d256-h8-dec-lrs2-d256-h8-encdec-20241022_224217'\n",
    "ranker1_train_path = TRAIN_RANKER_EMBS_PATH / ranker1_subdir\n",
    "ranker1_snapshot_fpath = ranker1_train_path / 'best.pth'\n",
    "\n",
    "ranker0_model_cfg_fpath = ranker0_train_path / RANKER_MODEL_CFG_FNAME\n",
    "print(f'Ranker cfg fpath: {ranker0_model_cfg_fpath}. Exists: {ranker0_model_cfg_fpath.exists()}')\n",
    "ranker1_model_cfg_fpath = ranker1_train_path / RANKER_MODEL_CFG_FNAME\n",
    "print(f'Ranker cfg fpath: {ranker1_model_cfg_fpath}. Exists: {ranker1_model_cfg_fpath.exists()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "emb_chunk_size = 100\n",
    "embs_chunk_size = 100\n",
    "docs_batch_size = 10\n",
    "chunk_size = 100\n",
    "max_docs_embs = 10\n",
    "docs_per_chunk = chunk_size // max_docs_embs\n",
    "\n",
    "device = 'cpu'\n",
    "# device = 'cuda'\n",
    "\n",
    "device = torch.device(device)\n",
    "print(device)\n",
    "ranker0_model_cfg: MllmRankerCfg = parse_yaml_file_as(MllmRankerCfg, ranker0_model_cfg_fpath)\n",
    "ranker1_model_cfg: MllmRankerCfg = parse_yaml_file_as(MllmRankerCfg, ranker1_model_cfg_fpath)\n",
    "enc_cfg_1 = ranker1_model_cfg.encoders[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkz_cfg = parse_yaml_file_as(TokenizerCfg, tokenizer_cfg_fpath)\n",
    "ch_tkz = tokenizer_from_config(tkz_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join datasets:\n",
      "   Msmarco. Queries: 372206. Docs: 3213835. QueryDocRels: 372206\n",
      "   Fever. Queries: 123142. Docs: 5416568. QueryDocRels: 156101\n"
     ]
    }
   ],
   "source": [
    "ds_qrels = load_qrels_datasets([DS_MSMARCO_DIR_PATH, DS_FEVER_DIR_PATH], ch_tkz, emb_chunk_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_embs = DsQrelsEmbs(\n",
    "    ds_dir_path=DS_EMBS_DIR_PATH, chunk_size=embs_chunk_size, emb_size=enc_cfg_1.d_model, emb_dtype=np.float32,\n",
    "    doc_id_driven=True, max_docs_embs=max_docs_embs, device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_encoder.src_word_emb.weight (50271, 256) -0.010897174 -1.459274e-06 0.010897168\n",
      "vocab_encoder.layer_norm.weight (256,) -0.099592425 -0.0026334831 0.09836828\n",
      "vocab_encoder.layer_norm.bias (256,) -0.09884079 0.0026726224 0.09994449\n",
      "encoder.a_em () -0.088543944 -0.088543944 -0.088543944\n",
      "encoder.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.10824053 0.00010209259 0.10824627\n",
      "encoder.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.10825062 0.00021738256 0.108246036\n",
      "encoder.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.1082493 -1.4802983e-05 0.10825072\n",
      "encoder.layer_stack.0.slf_attn.fc.weight (256, 256) -0.10825028 -0.0001302238 0.10824975\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.09941882 -0.0014857117 0.0993665\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09997981 -0.004862113 0.09998429\n",
      "encoder.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.06846507 -4.7036815e-06 0.06846494\n",
      "encoder.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.099764526 0.00052069005 0.09981717\n",
      "encoder.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.06846529 -7.4806056e-05 0.06846529\n",
      "encoder.layer_stack.0.pos_ffn.w_2.bias (256,) -0.09914996 0.0028025648 0.09767157\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.09987581 -0.0010995392 0.09977174\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.09859271 0.003402635 0.09933752\n",
      "encoder.layer_stack.1.slf_attn.w_qs.weight (256, 256) -0.108253084 -0.00014070273 0.10825201\n",
      "encoder.layer_stack.1.slf_attn.w_ks.weight (256, 256) -0.10825317 -4.4111723e-05 0.10825206\n",
      "encoder.layer_stack.1.slf_attn.w_vs.weight (256, 256) -0.10823965 0.00018496314 0.108252324\n",
      "encoder.layer_stack.1.slf_attn.fc.weight (256, 256) -0.10824697 -7.31226e-05 0.10824979\n",
      "encoder.layer_stack.1.slf_attn.layer_norm.weight (256,) -0.09773878 0.0025727886 0.09940516\n",
      "encoder.layer_stack.1.slf_attn.layer_norm.bias (256,) -0.09929146 -0.0019871532 0.0996688\n",
      "encoder.layer_stack.1.pos_ffn.w_1.weight (1024, 256) -0.06846519 4.799418e-06 0.06846484\n",
      "encoder.layer_stack.1.pos_ffn.w_1.bias (1024,) -0.099150255 0.0017534728 0.099967636\n",
      "encoder.layer_stack.1.pos_ffn.w_2.weight (256, 1024) -0.06846525 8.514998e-05 0.06846523\n",
      "encoder.layer_stack.1.pos_ffn.w_2.bias (256,) -0.097713165 0.00031681987 0.09858028\n",
      "encoder.layer_stack.1.pos_ffn.layer_norm.weight (256,) -0.0993076 -0.0031230785 0.0994174\n",
      "encoder.layer_stack.1.pos_ffn.layer_norm.bias (256,) -0.09826479 0.0008039073 0.09966983\n",
      "encoder.layer_norm.weight (256,) -0.099551715 -0.0039426535 0.09957005\n",
      "encoder.layer_norm.bias (256,) -0.09970462 -0.0058604544 0.098980926\n",
      "decoder.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.10825048 -7.808481e-05 0.108251326\n",
      "decoder.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.108242795 2.4744608e-05 0.108249046\n",
      "decoder.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.10824526 -0.00018284554 0.10825173\n",
      "decoder.layer_stack.0.slf_attn.fc.weight (256, 256) -0.10825074 0.00040803524 0.10824926\n",
      "decoder.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.09957005 -0.0003607244 0.099003665\n",
      "decoder.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09949263 -0.001175652 0.099700235\n",
      "decoder.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.06846441 0.00012926874 0.068465\n",
      "decoder.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.09984056 0.0008268639 0.09981942\n",
      "decoder.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.06846489 5.28809e-05 0.06846501\n",
      "decoder.layer_stack.0.pos_ffn.w_2.bias (256,) -0.09941401 0.0036011287 0.09855037\n",
      "decoder.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.09854483 0.0025070005 0.09985413\n",
      "decoder.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.09894141 0.002889805 0.099306814\n",
      "decoder.layer_stack.1.slf_attn.w_qs.weight (256, 256) -0.1082454 -5.5206066e-05 0.10825241\n",
      "decoder.layer_stack.1.slf_attn.w_ks.weight (256, 256) -0.10825305 0.00026906742 0.10825206\n",
      "decoder.layer_stack.1.slf_attn.w_vs.weight (256, 256) -0.10825218 -8.3149644e-07 0.10825296\n",
      "decoder.layer_stack.1.slf_attn.fc.weight (256, 256) -0.10824778 -0.00019242748 0.108252466\n",
      "decoder.layer_stack.1.slf_attn.layer_norm.weight (256,) -0.09793138 0.003227192 0.09940833\n",
      "decoder.layer_stack.1.slf_attn.layer_norm.bias (256,) -0.098676674 0.0012645375 0.0994423\n",
      "decoder.layer_stack.1.pos_ffn.w_1.weight (1024, 256) -0.0684652 0.00011122009 0.068464\n",
      "decoder.layer_stack.1.pos_ffn.w_1.bias (1024,) -0.099681154 0.002525188 0.09999583\n",
      "decoder.layer_stack.1.pos_ffn.w_2.weight (256, 1024) -0.06846472 -8.865338e-06 0.06846505\n",
      "decoder.layer_stack.1.pos_ffn.w_2.bias (256,) -0.09995037 -0.001927444 0.09942633\n",
      "decoder.layer_stack.1.pos_ffn.layer_norm.weight (256,) -0.098777674 0.004869285 0.09881931\n",
      "decoder.layer_stack.1.pos_ffn.layer_norm.bias (256,) -0.09992774 -0.00017699302 0.09981942\n",
      "decoder.w.weight (256, 256) -0.10825316 9.123971e-06 0.10825263\n",
      "decoder.layer_norm.weight (256,) -0.09861945 0.0032183302 0.099374555\n",
      "decoder.layer_norm.bias (256,) -0.09971052 -0.00060098874 0.09938965\n",
      "Loading model weights from /home/misha/data/train_mllm_ranker_qrels_0/ranker-20241021_062053-msmarco-fever/best.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MllmRankerLevel:\n\tMissing key(s) in state_dict: \"decoder.layer_stack.0.slf_attn.w_qs.weight\", \"decoder.layer_stack.0.slf_attn.w_ks.weight\", \"decoder.layer_stack.0.slf_attn.w_vs.weight\", \"decoder.layer_stack.0.slf_attn.fc.weight\", \"decoder.layer_stack.0.slf_attn.layer_norm.weight\", \"decoder.layer_stack.0.slf_attn.layer_norm.bias\", \"decoder.layer_stack.0.pos_ffn.w_1.weight\", \"decoder.layer_stack.0.pos_ffn.w_1.bias\", \"decoder.layer_stack.0.pos_ffn.w_2.weight\", \"decoder.layer_stack.0.pos_ffn.w_2.bias\", \"decoder.layer_stack.0.pos_ffn.layer_norm.weight\", \"decoder.layer_stack.0.pos_ffn.layer_norm.bias\", \"decoder.layer_stack.1.slf_attn.w_qs.weight\", \"decoder.layer_stack.1.slf_attn.w_ks.weight\", \"decoder.layer_stack.1.slf_attn.w_vs.weight\", \"decoder.layer_stack.1.slf_attn.fc.weight\", \"decoder.layer_stack.1.slf_attn.layer_norm.weight\", \"decoder.layer_stack.1.slf_attn.layer_norm.bias\", \"decoder.layer_stack.1.pos_ffn.w_1.weight\", \"decoder.layer_stack.1.pos_ffn.w_1.bias\", \"decoder.layer_stack.1.pos_ffn.w_2.weight\", \"decoder.layer_stack.1.pos_ffn.w_2.bias\", \"decoder.layer_stack.1.pos_ffn.layer_norm.weight\", \"decoder.layer_stack.1.pos_ffn.layer_norm.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading model weights from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mranker0_snapshot_fpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(ranker0_snapshot_fpath, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel_ranker_0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model_ranker_0\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MllmRankerLevel:\n\tMissing key(s) in state_dict: \"decoder.layer_stack.0.slf_attn.w_qs.weight\", \"decoder.layer_stack.0.slf_attn.w_ks.weight\", \"decoder.layer_stack.0.slf_attn.w_vs.weight\", \"decoder.layer_stack.0.slf_attn.fc.weight\", \"decoder.layer_stack.0.slf_attn.layer_norm.weight\", \"decoder.layer_stack.0.slf_attn.layer_norm.bias\", \"decoder.layer_stack.0.pos_ffn.w_1.weight\", \"decoder.layer_stack.0.pos_ffn.w_1.bias\", \"decoder.layer_stack.0.pos_ffn.w_2.weight\", \"decoder.layer_stack.0.pos_ffn.w_2.bias\", \"decoder.layer_stack.0.pos_ffn.layer_norm.weight\", \"decoder.layer_stack.0.pos_ffn.layer_norm.bias\", \"decoder.layer_stack.1.slf_attn.w_qs.weight\", \"decoder.layer_stack.1.slf_attn.w_ks.weight\", \"decoder.layer_stack.1.slf_attn.w_vs.weight\", \"decoder.layer_stack.1.slf_attn.fc.weight\", \"decoder.layer_stack.1.slf_attn.layer_norm.weight\", \"decoder.layer_stack.1.slf_attn.layer_norm.bias\", \"decoder.layer_stack.1.pos_ffn.w_1.weight\", \"decoder.layer_stack.1.pos_ffn.w_1.bias\", \"decoder.layer_stack.1.pos_ffn.w_2.weight\", \"decoder.layer_stack.1.pos_ffn.w_2.bias\", \"decoder.layer_stack.1.pos_ffn.layer_norm.weight\", \"decoder.layer_stack.1.pos_ffn.layer_norm.bias\". "
     ]
    }
   ],
   "source": [
    "model_ranker_0 = MllmRankerLevel(ranker0_model_cfg, 0).to(device)\n",
    "print(f'Loading model weights from {ranker0_snapshot_fpath}')\n",
    "checkpoint = torch.load(ranker0_snapshot_fpath, map_location=device)\n",
    "model_ranker_0.load_state_dict(checkpoint['model'])\n",
    "model_ranker_0.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.108247705 6.458067e-05 0.108241946\n",
      "encoder.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.10825289 4.0275627e-06 0.10824833\n",
      "encoder.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.1082488 7.5247895e-05 0.10825012\n",
      "encoder.layer_stack.0.slf_attn.fc.weight (256, 256) -0.10824937 0.0001995811 0.10824907\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.099848844 0.0021417397 0.099555515\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09982987 -0.00022519252 0.09944629\n",
      "encoder.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.06846469 2.8559281e-05 0.068464674\n",
      "encoder.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.09975264 0.0012859779 0.09967675\n",
      "encoder.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.068464495 7.863473e-05 0.06846463\n",
      "encoder.layer_stack.0.pos_ffn.w_2.bias (256,) -0.09999155 0.0034040469 0.09917271\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.09974629 -0.0009187027 0.099436216\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.09871178 0.003854303 0.099218525\n",
      "encoder.layer_stack.1.slf_attn.w_qs.weight (256, 256) -0.108250916 -0.00019869689 0.108252324\n",
      "encoder.layer_stack.1.slf_attn.w_ks.weight (256, 256) -0.10824848 2.9696683e-05 0.10825267\n",
      "encoder.layer_stack.1.slf_attn.w_vs.weight (256, 256) -0.10825127 9.150009e-05 0.108249985\n",
      "encoder.layer_stack.1.slf_attn.fc.weight (256, 256) -0.108250305 -0.00013016677 0.10825205\n",
      "encoder.layer_stack.1.slf_attn.layer_norm.weight (256,) -0.09981108 0.0018179556 0.0994908\n",
      "encoder.layer_stack.1.slf_attn.layer_norm.bias (256,) -0.09954622 -0.0010013616 0.09931483\n",
      "encoder.layer_stack.1.pos_ffn.w_1.weight (1024, 256) -0.068464644 2.8450404e-05 0.06846483\n",
      "encoder.layer_stack.1.pos_ffn.w_1.bias (1024,) -0.09975642 0.00060731964 0.09975972\n",
      "encoder.layer_stack.1.pos_ffn.w_2.weight (256, 1024) -0.06846475 8.0409896e-05 0.06846474\n",
      "encoder.layer_stack.1.pos_ffn.w_2.bias (256,) -0.099380724 -0.0041629965 0.09951232\n",
      "encoder.layer_stack.1.pos_ffn.layer_norm.weight (256,) -0.09712919 -0.00021228351 0.099730074\n",
      "encoder.layer_stack.1.pos_ffn.layer_norm.bias (256,) -0.09922882 -0.0036309932 0.099032216\n",
      "encoder.w_em.weight (1, 100) -0.2437197 0.0022674869 0.24257994\n",
      "encoder.layer_norm.weight (256,) -0.09979796 -0.0035893125 0.09952668\n",
      "encoder.layer_norm.bias (256,) -0.09989708 -0.0014407382 0.09975916\n",
      "decoder.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.10825314 -6.516016e-05 0.10825282\n",
      "decoder.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.10824496 -8.929455e-05 0.10825245\n",
      "decoder.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.108252995 0.00024477183 0.10825067\n",
      "decoder.layer_stack.0.slf_attn.fc.weight (256, 256) -0.10825236 0.0002888238 0.10824862\n",
      "decoder.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.09943303 7.53575e-05 0.097193636\n",
      "decoder.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09984461 -0.0016440293 0.09967183\n",
      "decoder.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.06846486 3.0129997e-05 0.06846491\n",
      "decoder.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.09998691 0.0027998185 0.099893786\n",
      "decoder.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.06846413 9.544843e-05 0.068464965\n",
      "decoder.layer_stack.0.pos_ffn.w_2.bias (256,) -0.09916526 0.0026061698 0.0995212\n",
      "decoder.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.09988616 0.003994141 0.099447906\n",
      "decoder.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.099871635 -0.0068487255 0.09979411\n",
      "decoder.layer_stack.1.slf_attn.w_qs.weight (256, 256) -0.108247995 -0.000244963 0.10825049\n",
      "decoder.layer_stack.1.slf_attn.w_ks.weight (256, 256) -0.10825243 0.00017460436 0.10824566\n",
      "decoder.layer_stack.1.slf_attn.w_vs.weight (256, 256) -0.10824818 0.00013554729 0.10824078\n",
      "decoder.layer_stack.1.slf_attn.fc.weight (256, 256) -0.10823957 0.00040414475 0.10825093\n",
      "decoder.layer_stack.1.slf_attn.layer_norm.weight (256,) -0.09995569 -0.005453677 0.09912286\n",
      "decoder.layer_stack.1.slf_attn.layer_norm.bias (256,) -0.09546955 -0.0019657658 0.099892154\n",
      "decoder.layer_stack.1.pos_ffn.w_1.weight (1024, 256) -0.06846497 -4.573506e-06 0.0684652\n",
      "decoder.layer_stack.1.pos_ffn.w_1.bias (1024,) -0.09981879 -0.0010848287 0.099921346\n",
      "decoder.layer_stack.1.pos_ffn.w_2.weight (256, 1024) -0.068464465 -3.963604e-05 0.06846525\n",
      "decoder.layer_stack.1.pos_ffn.w_2.bias (256,) -0.09958229 -0.0006068549 0.09811293\n",
      "decoder.layer_stack.1.pos_ffn.layer_norm.weight (256,) -0.09979447 -0.0008259781 0.09913283\n",
      "decoder.layer_stack.1.pos_ffn.layer_norm.bias (256,) -0.09988033 -0.00020023691 0.09919121\n",
      "decoder.w.weight (256, 256) -0.10825053 0.000119169614 0.10825039\n",
      "decoder.layer_norm.weight (256,) -0.098202966 0.0008046536 0.099657\n",
      "decoder.layer_norm.bias (256,) -0.0992373 0.00093432015 0.09860861\n",
      "Loading model weights from /home/misha/data/train_mllm_ranker_qrels_1/ranker-lvl1-20241025_223420-enc-lrs2-embmatTrue-d256-h8-dec-lrs2-d256-h8-encdec-20241022_224217/best.pth\n"
     ]
    }
   ],
   "source": [
    "model_ranker_1 = MllmRankerLevel(ranker1_model_cfg, level=1).to(device)\n",
    "print(f'Loading model weights from {ranker1_snapshot_fpath}')\n",
    "checkpoint = torch.load(ranker1_snapshot_fpath, map_location=device)\n",
    "model_ranker_1.load_state_dict(checkpoint['model'])\n",
    "model_ranker_1.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dataset queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_view = ds_embs.get_embs_view(batch_size=docs_batch_size * docs_per_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_batch = 0\n",
    "embs_batch_it = embs_view.get_batch_iterator(with_queries=True)\n",
    "for _ in range(i_batch):\n",
    "    embs_batch = next(embs_batch_it)\n",
    "embs_batch = next(embs_batch_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0. Docs embs: (10, 100, 256). Queries embs: (149, 256) 149\n"
     ]
    }
   ],
   "source": [
    "assert embs_batch.qs_embs is not None and embs_batch.qs_ind_len is not None\n",
    "print(f'Batch {i_batch}. Docs embs: {embs_batch.docs_embs.shape}. Queries embs: {embs_batch.qs_embs.shape} {len(embs_batch.qs_ind_len)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(769, 149, 149)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embs_batch.df_docs_ids), len(embs_batch.df_qrels), len(embs_batch.df_qs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_rank. min, mean, max: 0.0912, 0.1000, 0.1095\n"
     ]
    }
   ],
   "source": [
    "docs_embs_t = embs_batch.get_docs_embs_tensor()\n",
    "qs_embs_t, qs_masks_t = embs_batch.get_qs_tensors()\n",
    "out_rank = model_ranker_1.run_qs_embs(docs_embs_t, qs_embs_t, embs_batch.qs_ind_len)\n",
    "print(f'out_rank. min, mean, max: {out_rank.min():0.4f}, {out_rank.mean():0.4f}, {out_rank.max():0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([149, 10])\n"
     ]
    }
   ],
   "source": [
    "print(out_rank.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 3 11791: +where does the pacific walrus ive\n"
     ]
    }
   ],
   "source": [
    "query_ind = 3\n",
    "dsqid = embs_batch.qs_ind_len[query_ind][0]\n",
    "print(f'Query {query_ind} {dsqid}: {ds_qrels.df_qs.loc[dsqid].query}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1015, 0.0988, 0.0996, 0.0993, 0.1000, 0.1005, 0.1019, 0.0990, 0.0996,\n",
       "        0.0999], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_rank[query_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False,  True, False, False])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs_masks_t[query_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1369469, 1901654,  632008, 2724055, 2305197, 1019421, 2345255,\n",
       "       2805343, 3182944,  655573])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsdids = embs_batch.ids.reshape((docs_batch_size, docs_per_chunk))\n",
    "i_doc = 7\n",
    "dsdids[i_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. Doc 1369469: Morse Code & the Telegraph. \"Early Forms of Long-Distance Communication Before the development of the electric telegraph in the 19th century revolutionized how information was transmitted across long distances, ancient civilizat\n",
      "02. Doc 1901654: When Did Mardi Gras Start?. \"Listen H urray! It's Tuesday! How often do you hear that? If you've had a bad Monday, then maybe Tuesday rolling around might brighten your spirits. Otherwise, Tuesday usually doesn't get too much at\n",
      "03. Doc 632008: Walrus. \"From Wikipedia, the free encyclopedianavigation search For other uses, see Walrus (disambiguation). Walrus Temporal range: Pleistocene to Recent Male Female with young Conservation status Vulnerable \n",
      "04. Doc 2724055: Where Is Bactria?. \"Humanities ›History & Culture Where Is Bactria? Share Flipboard Email Printvia Wikipediaby Kallie Szczepanski Updated December 27, 2017Bactria is an ancient region of Central Asia, between the Hindu \n",
      "05. Doc 2305197: Find the Routing Number on a Check. Home Banking Resources Basics Routing Number Routing Number on a Check Open a Checking Account Contact Us For Assistance Find the Routing Number on a Check At the bottom of a check, you will see three\n",
      "06. Doc 1019421: .. President John Tyler has two living grandsons At the Corner, Mark Krikorian mentions the amazing news that two grandsons of President John Tyler, who was president from 1841 to 1845, are still alive. \n",
      "07. Doc 2345255: Finding Flow. \"Finding Flow Reviews the book 'Finding Flow,' by Mihaly Csikszentmihalyi. By Mihaly Csikszentmihalyi, published July 1, 1997 - last reviewed on June 9, 2016SHARETWEETEMAILMOREWe all are capable of re\n",
      "08. Doc 2805343: Receptionist Job Description. Receptionist Job Description A receptionist's job takes an important share in the overall functioning of an organization. She is the face of a company; in fact, the first official professional to whom\n",
      "09. Doc 3182944: suffix. \"suffixsuf·fix Use suffix in a sentencenoun The definition of a suffix is a letter, syllable or group of syllables that are added to the end of a word to change it into something else. An example of s\n",
      "10. Doc 655573: Difference between nastic movements and tropism movements?. \"Science & Mathematics Biology Difference between nastic movements and tropism movements?title1 following 7 answers Answers Relevance Rating Newest Oldest Best Answer: Nastic Movement: \"\"Nastic moveme\n"
     ]
    }
   ],
   "source": [
    "for i, dsdid in enumerate(dsdids[i_doc]):\n",
    "    title, text = ds_qrels.get_doc(dsdid)\n",
    "    print(f'{i + 1:02d}. Doc {dsdid}: {title[:100]}. {text[:200]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qid                           265\n",
      "query    +what is fascia or facia\n",
      "dsid                            1\n",
      "dsqid                        9174\n",
      "Name: 9174, dtype: object\n",
      "qid         265\n",
      "did       97881\n",
      "dsid          1\n",
      "dsqid      9174\n",
      "dsdid    508390\n",
      "Name: 9174, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(ds_qrels.df_qs.loc[dsqid])\n",
    "qrel = ds_qrels.df_qrels.loc[dsqid]\n",
    "print(qrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fascia. \"fascia Also found in: Thesaurus, Medical, Legal, Encyclopedia, Wikipedia. Related to fascia: Colles fasciafas·cia (făsh′ə, fä′shə)n. pl. fas·ci·ae (făsh′ē-ē′, fä′shē-ē′)1. Anatomya. A sheet or band of fibrous connective tissue enveloping, separating, or bindingtogether muscles, organs, and other soft structures of the body.b. The tissue of which such a sheet or band is composed.2. Biology A broad\n"
     ]
    }
   ],
   "source": [
    "title, text = ds_qrels.get_doc(qrel.dsdid)\n",
    "print(f'{title[:100]}. {text[:400]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrel.dsdid in embs_batch.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ds_qrels.df_qrels.index) == set(ds_qrels.df_qs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 895028, 3085932, 1616445, 2396834,  465048,  679478, 1282538,\n",
       "       2558828, 1918243, 1184395, 1931964,  991350, 1141254, 2247720,\n",
       "       2726870, 1461296, 2434362,  127982, 1479578, 1000903,  509488,\n",
       "       1161217, 3026873,  109872, 1446407, 1152228,  598469, 2971199,\n",
       "       2951585, 1365772, 2635634, 2588840, 2495189,   38604, 1537726,\n",
       "        699367, 2268633,  517965,  796713,  416362,  299777, 1742059,\n",
       "        544317, 2520629, 1436264, 2858298,  412377,  456861,  688057,\n",
       "        826758,  294285, 3081347,  650865, 2410836, 1107353, 3195243,\n",
       "       1505331, 1627906, 2145371,  739799, 1234207,  508390,  198804,\n",
       "       1895725,  239631, 1388082, 2056134,   51362,  760521,   38219,\n",
       "       1369469, 1901654,  632008, 2724055, 2305197, 1019421, 2345255,\n",
       "       2805343, 3182944,  655573,  308315,  580376, 1664242, 3115493,\n",
       "       2165184, 2810761,   58275,  525091,  919745, 1647851,  406509,\n",
       "       1163151, 2521349,  207467, 1206529,  803368,  597726,  874445,\n",
       "        314351, 1573595])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs_batch.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "for i, did in enumerate(embs_batch.ids):\n",
    "    if did == 508390:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>offset</th>\n",
       "      <th>dsid</th>\n",
       "      <th>dsdid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dsdid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1555982</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301595</td>\n",
       "      <td>1852</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1359209</td>\n",
       "      <td>7973</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2147834</td>\n",
       "      <td>23656</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1568809</td>\n",
       "      <td>31104</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           did  offset  dsid  dsdid\n",
       "dsdid                              \n",
       "0      1555982       0     1      0\n",
       "1       301595    1852     1      1\n",
       "2      1359209    7973     1      2\n",
       "3      2147834   23656     1      3\n",
       "4      1568809   31104     1      4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_off = ds_qrels.df_off\n",
    "df_off.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>offset</th>\n",
       "      <th>dsid</th>\n",
       "      <th>dsdid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dsdid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301595</td>\n",
       "      <td>1852</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1555982</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2147834</td>\n",
       "      <td>23656</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1359209</td>\n",
       "      <td>7973</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1568809</td>\n",
       "      <td>31104</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           did  offset  dsid  dsdid\n",
       "dsdid                              \n",
       "1       301595    1852     1      1\n",
       "0      1555982       0     1      0\n",
       "3      2147834   23656     1      3\n",
       "2      1359209    7973     1      2\n",
       "4      1568809   31104     1      4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_off.loc[[1,0,3,2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index([1, 0, 3, 2, 4], dtype='int64', name='dsdid'),\n",
       " dsdid\n",
       " 1     301595\n",
       " 0    1555982\n",
       " 3    2147834\n",
       " 2    1359209\n",
       " 4    1568809\n",
       " Name: did, dtype: int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_off.loc[[1,0,3,2,4]]\n",
    "df.index, df.did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_tok = tkz_cfg.custom_tokens['pad']\n",
    "inp_len = ranker1_model_cfg.vocab_encoder.inp_len\n",
    "\n",
    "def tokten_to_text(tokens: torch.Tensor) -> str:\n",
    "    tokens = tokens.flatten()\n",
    "    tokens = tokens[tokens != pad_tok]\n",
    "    tokens = list(tokens)\n",
    "    s = ch_tkz.decode(tokens)\n",
    "    return s\n",
    "\n",
    "def distance(x: np.ndarray, y: np.ndarray, cosine: bool = False):\n",
    "    if not cosine:\n",
    "        return np.linalg.norm(x - y)\n",
    "    x_norm, y_norm = np.linalg.norm(x), np.linalg.norm(y)\n",
    "    return np.sum(x * y) / (x_norm * y_norm)\n",
    "\n",
    "def text_to_tokens(s: str, qbeg_tok: Optional[int] = None, qend_tok: Optional[int] = None) -> torch.Tensor:\n",
    "    tokens = ch_tkz(s)['input_ids']\n",
    "    if qbeg_tok is not None:\n",
    "        assert qend_tok is not None\n",
    "        tokens = [qbeg_tok, *tokens, qend_tok]\n",
    "    n_tokens = len(tokens)\n",
    "    n_padded = n_tokens // inp_len + (n_tokens % inp_len > 0)\n",
    "    res = np.full((n_padded * inp_len, ), pad_tok, dtype=np.int32)\n",
    "    res[:n_tokens] = tokens\n",
    "    res = torch.from_numpy(res).to(device)\n",
    "    res = res.reshape(n_padded, inp_len)\n",
    "    return res\n",
    "\n",
    "def print_dist(target_embs: torch.Tensor, docs_embs: torch.Tensor, target_mask: torch.Tensor, cosine: bool = True):\n",
    "    for i, docs_emb in enumerate(docs_embs.detach().numpy()):\n",
    "        for target_emb in target_embs.detach().numpy():\n",
    "            dist = distance(target_emb, docs_emb, cosine)\n",
    "            print(f'{dist:0.6f} ', end='')\n",
    "        sfx = 'T' if target_mask[i] else 'F'\n",
    "        print(sfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
