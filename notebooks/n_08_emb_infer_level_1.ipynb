{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Optional\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pydantic_yaml import parse_yaml_file_as\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, AddedToken, PreTrainedTokenizer\n",
    "\n",
    "from mllm.config.model import MllmRankerCfg, MllmEncdecCfg, TokenizerCfg\n",
    "from mllm.data.dsqrels_embs import DsQrelsEmbs, QrelsEmbsBatch\n",
    "from mllm.data.utils import load_qrels_datasets\n",
    "from mllm.model.mllm_encdec import MllmEncdecLevel\n",
    "from mllm.model.mllm_ranker import RankProbLoss, MllmRanker, MllmRankerLevel\n",
    "from mllm.tokenization.chunk_tokenizer import gen_all_tokens, ChunkTokenizer, tokenizer_from_config\n",
    "from mllm.train.utils import find_create_train_path, calc_print_batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranker level 1 inference\n",
    "## Config and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranker cfg fpath: /home/misha/prog/mllm/mllm/config/cfg/ranker_model_cfg_01.yaml. Exists: True\n",
      "Ranker cfg fpath: /home/misha/prog/mllm/mllm/config/cfg/ranker_model_cfg_02.yaml. Exists: True\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(os.path.expandvars('$HOME')) / 'data'\n",
    "DS_MSMARCO_DIR_PATH = DATA_PATH / 'msmarco'\n",
    "DS_FEVER_DIR_PATH = DATA_PATH / 'fever'\n",
    "TRAIN_RANKER_PATH = DATA_PATH / 'train_mllm_ranker_qrels'\n",
    "TRAIN_RANKER_EMBS_PATH = DATA_PATH / 'train_mllm_ranker_qrels_1'\n",
    "DS_WIKI_DIR_PATH = DATA_PATH / 'wiki_20200501_en/ch_100_fixed'\n",
    "DS_EMBS_DIR_PATH = DATA_PATH / 'ranker_embs_msmarco_fever'\n",
    "CFG_DIR_PATH = Path(os.path.abspath('.')).parent / 'mllm/config/cfg'\n",
    "\n",
    "tokenizer_cfg_fpath = CFG_DIR_PATH / 'tokenizer_cfg_01.yaml'\n",
    "\n",
    "ranker0_subdir = 'ranker-20240903_215749-msmarco-fever'\n",
    "ranker0_train_path = TRAIN_RANKER_PATH / ranker0_subdir\n",
    "ranker0_snapshot_fpath = ranker0_train_path / 'best.pth'\n",
    "\n",
    "ranker1_subdir = 'ranker-l1-20241012_102220-encdec-l1-20241005_175446-msmarco-fever'\n",
    "ranker1_train_path = TRAIN_RANKER_EMBS_PATH / ranker1_subdir\n",
    "ranker1_snapshot_fpath = ranker1_train_path / 'best.pth'\n",
    "\n",
    "ranker0_model_cfg_fpath = CFG_DIR_PATH / 'ranker_model_cfg_01.yaml'\n",
    "print(f'Ranker cfg fpath: {ranker0_model_cfg_fpath}. Exists: {ranker0_model_cfg_fpath.exists()}')\n",
    "ranker1_model_cfg_fpath = CFG_DIR_PATH / 'ranker_model_cfg_02.yaml'\n",
    "print(f'Ranker cfg fpath: {ranker1_model_cfg_fpath}. Exists: {ranker1_model_cfg_fpath.exists()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "emb_chunk_size = 100\n",
    "embs_chunk_size = 100\n",
    "docs_batch_size = 10\n",
    "chunk_size = 100\n",
    "max_docs_embs = 10\n",
    "docs_per_chunk = chunk_size // max_docs_embs\n",
    "\n",
    "device = 'cpu'\n",
    "# device = 'cuda'\n",
    "\n",
    "device = torch.device(device)\n",
    "print(device)\n",
    "ranker0_model_cfg: MllmRankerCfg = parse_yaml_file_as(MllmRankerCfg, ranker0_model_cfg_fpath)\n",
    "ranker1_model_cfg: MllmRankerCfg = parse_yaml_file_as(MllmRankerCfg, ranker1_model_cfg_fpath)\n",
    "enc_cfg_1 = ranker1_model_cfg.encoders[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkz_cfg = parse_yaml_file_as(TokenizerCfg, tokenizer_cfg_fpath)\n",
    "ch_tkz = tokenizer_from_config(tkz_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join datasets:\n",
      "   Msmarco. Queries: 372206. Docs: 3213835. QueryDocRels: 372206\n",
      "   Fever. Queries: 123142. Docs: 5416568. QueryDocRels: 156101\n"
     ]
    }
   ],
   "source": [
    "ds_qrels = load_qrels_datasets([DS_MSMARCO_DIR_PATH, DS_FEVER_DIR_PATH], ch_tkz, emb_chunk_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_embs = DsQrelsEmbs(\n",
    "    ds_dir_path=DS_EMBS_DIR_PATH, chunk_size=embs_chunk_size, emb_size=enc_cfg_1.d_model, emb_dtype=np.float32,\n",
    "    doc_id_driven=True, max_docs_embs=max_docs_embs, device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_encoder.src_word_emb.weight (50270, 256) -0.010897282 2.9392197e-06 0.010897271\n",
      "vocab_encoder.layer_norm.weight (256,) -0.096466795 -0.0017280374 0.099555515\n",
      "vocab_encoder.layer_norm.bias (256,) -0.09798672 -0.00054148736 0.09971434\n",
      "encoders.0.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.10824995 0.00019093549 0.10825305\n",
      "encoders.0.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.10825143 3.1608957e-05 0.108252026\n",
      "encoders.0.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.1082509 0.00039153098 0.10825262\n",
      "encoders.0.layer_stack.0.slf_attn.fc.weight (256, 256) -0.10825164 0.0003312789 0.10825141\n",
      "encoders.0.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.099763334 -0.00023879157 0.09967537\n",
      "encoders.0.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09985304 -0.007849719 0.09995482\n",
      "encoders.0.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.06846457 2.4513172e-05 0.068465054\n",
      "encoders.0.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.099718705 0.0007249722 0.09987944\n",
      "encoders.0.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.068464555 3.5101653e-05 0.068464555\n",
      "encoders.0.layer_stack.0.pos_ffn.w_2.bias (256,) -0.098592736 -0.0077332933 0.09943139\n",
      "encoders.0.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.09990603 -0.00037785212 0.09661069\n",
      "encoders.0.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.099900365 -0.003824678 0.09949585\n",
      "encoders.0.w_em.weight (1, 100) -0.24080496 0.002961248 0.24073035\n",
      "encoders.0.layer_norm.weight (256,) -0.09948897 -0.0013423256 0.09857243\n",
      "encoders.0.layer_norm.bias (256,) -0.09933039 -0.0035146908 0.09921583\n",
      "decoders.0.w.weight (256, 256) -0.108252764 -0.00015286515 0.10825021\n",
      "decoders.0.layer_norm.weight (256,) -0.09947635 -0.0031015766 0.098994814\n",
      "decoders.0.layer_norm.bias (256,) -0.09906155 -0.004239325 0.09958371\n",
      "Loading model weights from /home/misha/data/train_mllm_ranker_qrels/ranker-20240903_215749-msmarco-fever/best.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ranker_0 = MllmRanker(ranker0_model_cfg).to(device)\n",
    "print(f'Loading model weights from {ranker0_snapshot_fpath}')\n",
    "checkpoint = torch.load(ranker0_snapshot_fpath, map_location=device)\n",
    "model_ranker_0.load_state_dict(checkpoint['model'])\n",
    "model_ranker_0.eval()\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.a_em () 0.09329428 0.09329428 0.09329428\n",
      "encoder.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.108248286 0.00027569226 0.108248144\n",
      "encoder.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.10825138 0.00023005717 0.1082424\n",
      "encoder.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.10825206 -0.00019636814 0.108246066\n",
      "encoder.layer_stack.0.slf_attn.fc.weight (256, 256) -0.10825005 -0.00022430725 0.10824982\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.09946926 0.0004833392 0.09980132\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.099841535 0.001078122 0.09991244\n",
      "encoder.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.06846433 -1.557562e-05 0.06846516\n",
      "encoder.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.09993149 -0.0031034076 0.09998745\n",
      "encoder.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.06846388 -7.280875e-05 0.068465166\n",
      "encoder.layer_stack.0.pos_ffn.w_2.bias (256,) -0.0996262 0.0022787298 0.0997855\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.09932965 -0.001374675 0.09973686\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.09910103 0.0032066186 0.099266104\n",
      "encoder.layer_norm.weight (256,) -0.09959914 0.0016693684 0.09927893\n",
      "encoder.layer_norm.bias (256,) -0.099542156 0.0031044856 0.09956556\n",
      "decoder.w.weight (256, 256) -0.10825192 -0.0003178062 0.108242966\n",
      "decoder.layer_norm.weight (256,) -0.098809205 -0.007543707 0.099706426\n",
      "decoder.layer_norm.bias (256,) -0.09896588 -0.0062030293 0.099975705\n",
      "Loading model weights from /home/misha/data/train_mllm_ranker_qrels_1/ranker-l1-20241012_102220-encdec-l1-20241005_175446-msmarco-fever/best.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ranker_1 = MllmRankerLevel(ranker1_model_cfg, level=1).to(device)\n",
    "print(f'Loading model weights from {ranker1_snapshot_fpath}')\n",
    "checkpoint = torch.load(ranker1_snapshot_fpath, map_location=device)\n",
    "model_ranker_1.load_state_dict(checkpoint['model'])\n",
    "model_ranker_1.eval()\n",
    "''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dataset queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embs_view = ds_embs.get_embs_view(batch_size=docs_batch_size * docs_per_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_batch = 0\n",
    "embs_batch_it = embs_view.get_batch_iterator(with_queries=True)\n",
    "for _ in range(i_batch):\n",
    "    embs_batch = next(embs_batch_it)\n",
    "embs_batch = next(embs_batch_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0. Docs embs: (10, 100, 256). Queries embs: (149, 256) 149\n"
     ]
    }
   ],
   "source": [
    "assert embs_batch.qs_embs is not None and embs_batch.qs_ind_len is not None\n",
    "print(f'Batch {i_batch}. Docs embs: {embs_batch.docs_embs.shape}. Queries embs: {embs_batch.qs_embs.shape} {len(embs_batch.qs_ind_len)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(769, 149, 149)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embs_batch.df_docs_ids), len(embs_batch.df_qrels), len(embs_batch.df_qs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_rank. min, mean, max: 0.0761, 0.1000, 0.1243\n"
     ]
    }
   ],
   "source": [
    "docs_embs_t = embs_batch.get_docs_embs_tensor()\n",
    "qs_embs_t, qs_masks_t = embs_batch.get_qs_tensors()\n",
    "out_rank = model_ranker_1.run_qs_embs(docs_embs_t, qs_embs_t, embs_batch.qs_ind_len)\n",
    "print(f'out_rank. min, mean, max: {out_rank.min():0.4f}, {out_rank.mean():0.4f}, {out_rank.max():0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([149, 10])\n"
     ]
    }
   ],
   "source": [
    "print(out_rank.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0 9174: +what is fascia or facia\n"
     ]
    }
   ],
   "source": [
    "query_ind = 0\n",
    "dsqid = embs_batch.qs_ind_len[query_ind][0]\n",
    "print(f'Query {query_ind} {dsqid}: {ds_qrels.df_qs.loc[dsqid].query}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1040, 0.1021, 0.1028, 0.0898, 0.0994, 0.1016, 0.1004, 0.0985, 0.1004,\n",
       "        0.1010], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_rank[query_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False,  True, False, False, False])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs_masks_t[query_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 895028, 3085932, 1616445, 2396834,  465048,  679478, 1282538,\n",
       "       2558828, 1918243, 1184395])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsdids = embs_batch.ids.reshape((docs_batch_size, docs_per_chunk))\n",
    "dsdids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. Doc 895028: Visual cortex. \"From Wikipedia, the free encyclopedianavigation search Visual cortex View of the brain from behind. Red = Brodmann area 17 (primary visual cortex); orange = area 18; yellow = area 19Brain shown from \n",
      "02. Doc 3085932: Electroconvulsive Therapy (ECT). Electroconvulsive Therapy (ECT)Treatment Overview Electroconvulsive therapy (ECT) is a procedure used to treat severe depression. It may be used in people who have symptoms such as delusions, hallucin\n",
      "03. Doc 1616445: Cholera. \"From Wikipedia, the free encyclopedianavigation search This article is about the bacterial disease. For the dish, see Cholera (food). Cholera A person with severe dehydration due to cholera causing s\n",
      "04. Doc 2396834: The ABO blood type system in humans is an example of:?. Science & Mathematics Biology The ABO blood type system in humans is an example of:? The ABO blood type system in humans is an example of: a) multiple alleles and codominance. b) balanced polymorphism\n",
      "05. Doc 465048: What is beriberi?. What is beriberi? Beriberi is a disease caused by a vitamin B-1 deficiency, also known as thiamine deficiency. There are two types of the disease: wet beriberi and dry beriberi. Wet beriberi affects t\n",
      "06. Doc 679478: How long do you REHEAT LASAGNA for in the oven, when you make them the day before?. Food & Drink Cooking & Recipes How long do you REHEAT LASAGNA for in the oven, when you make them the day before?what temperature too? heard that they taste even better when you reheat them the next d\n",
      "07. Doc 1282538: Nucleic acid. \"From Wikipedia, the free encyclopedianavigation search The Swiss scientist Friedrich Miescher discovered nucleic acids ( DNA) in 1869. [notes 1] Later, he raised the idea that they could be involved \n",
      "08. Doc 2558828: Context (language use). \"In semiotics, linguistics, sociology and anthropology, context refers to those objects or entities which surround a focal event, in these disciplines typically a communicative event, of some kind. Co\n",
      "09. Doc 1918243: What Is Medical Dosimetry?. What Is Medical Dosimetry? If you want to work with high-tech medical equipment and contribute to the treatment of patients with cancer, you might train to work in the field of medical dosimetry. Medi\n",
      "10. Doc 1184395: .. Error. Page cannot be displayed. Please contact your service provider for more details. (20) \n"
     ]
    }
   ],
   "source": [
    "for i, dsdid in enumerate(dsdids[0]):\n",
    "    title, text = ds_qrels.get_doc(dsdid)\n",
    "    print(f'{i + 1:02d}. Doc {dsdid}: {title[:100]}. {text[:200]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qid                           265\n",
      "query    +what is fascia or facia\n",
      "dsid                            1\n",
      "dsqid                        9174\n",
      "Name: 9174, dtype: object\n",
      "qid         265\n",
      "did       97881\n",
      "dsid          1\n",
      "dsqid      9174\n",
      "dsdid    508390\n",
      "Name: 9174, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(ds_qrels.df_qs.loc[dsqid])\n",
    "qrel = ds_qrels.df_qrels.loc[dsqid]\n",
    "print(qrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fascia. \"fascia Also found in: Thesaurus, Medical, Legal, Encyclopedia, Wikipedia. Related to fascia: Colles fasciafas·cia (făsh′ə, fä′shə)n. pl. fas·ci·ae (făsh′ē-ē′, fä′shē-ē′)1. Anatomya. A sheet or band of fibrous connective tissue enveloping, separating, or bindingtogether muscles, organs, and other soft structures of the body.b. The tissue of which such a sheet or band is composed.2. Biology A broad\n"
     ]
    }
   ],
   "source": [
    "title, text = ds_qrels.get_doc(qrel.dsdid)\n",
    "print(f'{title[:100]}. {text[:400]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrel.dsdid in embs_batch.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ds_qrels.df_qrels.index) == set(ds_qrels.df_qs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 895028, 3085932, 1616445, 2396834,  465048,  679478, 1282538,\n",
       "       2558828, 1918243, 1184395, 1931964,  991350, 1141254, 2247720,\n",
       "       2726870, 1461296, 2434362,  127982, 1479578, 1000903,  509488,\n",
       "       1161217, 3026873,  109872, 1446407, 1152228,  598469, 2971199,\n",
       "       2951585, 1365772, 2635634, 2588840, 2495189,   38604, 1537726,\n",
       "        699367, 2268633,  517965,  796713,  416362,  299777, 1742059,\n",
       "        544317, 2520629, 1436264, 2858298,  412377,  456861,  688057,\n",
       "        826758,  294285, 3081347,  650865, 2410836, 1107353, 3195243,\n",
       "       1505331, 1627906, 2145371,  739799, 1234207,  508390,  198804,\n",
       "       1895725,  239631, 1388082, 2056134,   51362,  760521,   38219,\n",
       "       1369469, 1901654,  632008, 2724055, 2305197, 1019421, 2345255,\n",
       "       2805343, 3182944,  655573,  308315,  580376, 1664242, 3115493,\n",
       "       2165184, 2810761,   58275,  525091,  919745, 1647851,  406509,\n",
       "       1163151, 2521349,  207467, 1206529,  803368,  597726,  874445,\n",
       "        314351, 1573595])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs_batch.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "for i, did in enumerate(embs_batch.ids):\n",
    "    if did == 508390:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_tok = tkz_cfg.custom_tokens['pad']\n",
    "inp_len = ranker1_model_cfg.vocab_encoder.inp_len\n",
    "\n",
    "def tokten_to_text(tokens: torch.Tensor) -> str:\n",
    "    tokens = tokens.flatten()\n",
    "    tokens = tokens[tokens != pad_tok]\n",
    "    tokens = list(tokens)\n",
    "    s = ch_tkz.decode(tokens)\n",
    "    return s\n",
    "\n",
    "def distance(x: np.ndarray, y: np.ndarray, cosine: bool = False):\n",
    "    if not cosine:\n",
    "        return np.linalg.norm(x - y)\n",
    "    x_norm, y_norm = np.linalg.norm(x), np.linalg.norm(y)\n",
    "    return np.sum(x * y) / (x_norm * y_norm)\n",
    "\n",
    "def text_to_tokens(s: str, qbeg_tok: Optional[int] = None, qend_tok: Optional[int] = None) -> torch.Tensor:\n",
    "    tokens = ch_tkz(s)['input_ids']\n",
    "    if qbeg_tok is not None:\n",
    "        assert qend_tok is not None\n",
    "        tokens = [qbeg_tok, *tokens, qend_tok]\n",
    "    n_tokens = len(tokens)\n",
    "    n_padded = n_tokens // inp_len + (n_tokens % inp_len > 0)\n",
    "    res = np.full((n_padded * inp_len, ), pad_tok, dtype=np.int32)\n",
    "    res[:n_tokens] = tokens\n",
    "    res = torch.from_numpy(res).to(device)\n",
    "    res = res.reshape(n_padded, inp_len)\n",
    "    return res\n",
    "\n",
    "def print_dist(target_embs: torch.Tensor, docs_embs: torch.Tensor, target_mask: torch.Tensor, cosine: bool = True):\n",
    "    for i, docs_emb in enumerate(docs_embs.detach().numpy()):\n",
    "        for target_emb in target_embs.detach().numpy():\n",
    "            dist = distance(target_emb, docs_emb, cosine)\n",
    "            print(f'{dist:0.6f} ', end='')\n",
    "        sfx = 'T' if target_mask[i] else 'F'\n",
    "        print(sfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
