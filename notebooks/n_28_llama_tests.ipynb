{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1387f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb86f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import fire\n",
    "\n",
    "import torch\n",
    "\n",
    "from accelerate.utils import is_xpu_available\n",
    "from llama_cookbook.inference.model_utils import load_model, load_peft_model\n",
    "\n",
    "from llama_cookbook.inference.safety_utils import AgentType, get_safety_checker\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "382e928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-3.2-1B',\n",
    "# peft_model: str = None,\n",
    "quantization = None, # Options: 4bit, 8bit\n",
    "# max_new_tokens=100,  # The maximum numbers of tokens to generate\n",
    "# prompt_file: str = None,\n",
    "# seed: int = 42,  # seed value for reproducibility\n",
    "# do_sample: bool = True,  # Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "# min_length: int = None,  # The minimum length of the sequence to be generated, input prompt + min_new_tokens\n",
    "# use_cache: bool = True,  # [optional] Whether or not the model should use the past last key/values attentions Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.\n",
    "# top_p: float = 1.0,  # [optional] If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "# temperature: float = 1.0,  # [optional] The value used to modulate the next token probabilities.\n",
    "# top_k: int = 50,  # [optional] The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "# repetition_penalty: float = 1.0,  # The parameter for repetition penalty. 1.0 means no penalty.\n",
    "# length_penalty: int = 1,  # [optional] Exponential penalty to the length that is used with beam-based generation.\n",
    "# enable_azure_content_safety: bool = False,  # Enable safety check with Azure content safety api\n",
    "# enable_sensitive_topics: bool = False,  # Enable check for sensitive topics using AuditNLG APIs\n",
    "# enable_salesforce_content_safety: bool = True,  # Enable safety check with Salesforce safety flan t5\n",
    "# enable_llamaguard_content_safety: bool = False,\n",
    "# max_padding_length: int = None,  # the max padding length to be used with tokenizer padding the prompts.\n",
    "use_fast_kernels = False,  # Enable using SDPA from PyTroch Accelerated Transformers, make use Flash Attention and Xformer memory-efficient kernels\n",
    "# share_gradio: bool = False, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221af583",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_name, quantization, use_fast_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9d81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de81664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e811b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa4e890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc2a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91660fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd2829f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
