{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Optional\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pydantic_yaml import parse_yaml_file_as\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, AddedToken, PreTrainedTokenizer\n",
    "\n",
    "from mllm.data.wiki.dswiki import WikiDsLoader\n",
    "from mllm.exp.args import TOKENIZER_CFG_FNAME, ENCDEC_MODEL_CFG_FNAME, RANKER_MODEL_CFG_FNAME\n",
    "from mllm.model.mllm_encdec import MllmEncdecLevel\n",
    "from mllm.model.mllm_ranker import MllmRankerLevel\n",
    "from mllm.config.model import TokenizerCfg, MllmEncdecCfg, MllmRankerCfg\n",
    "from mllm.tokenization.chunk_tokenizer import calc_max_inp_size, gen_all_tokens, ChunkTokenizer, tokenizer_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(os.path.expandvars('$HOME')) / 'data'\n",
    "DS_DIR_PATH = DATA_PATH / 'wiki_20200501_en' / 'ch_100_fixed'\n",
    "\n",
    "TRAIN_ENCDEC_0_PATH = DATA_PATH / 'train_mllm_encdec_0'\n",
    "TRAIN_ENCDEC_1_PATH = DATA_PATH / 'train_mllm_encdec_1'\n",
    "encdec_0_subdir = 'encdec-20241018_092135-wiki_20200501_en-ch_100_fixed'\n",
    "encdec_1_subdir = 'encdec-lvl1-20241022_224217-msmarco-fever-enc-lrs2-embmatTrue-d256-h8-dec-lrs2-seqlen100-d256-h8'\n",
    "\n",
    "encdec_0_train_path = TRAIN_ENCDEC_0_PATH / encdec_0_subdir\n",
    "encdec_1_train_path = TRAIN_ENCDEC_1_PATH / encdec_1_subdir\n",
    "encdec_0_snapshot_fpath = encdec_0_train_path / 'best.pth'\n",
    "encdec_1_snapshot_fpath = encdec_1_train_path / 'best.pth'\n",
    "encdec_0_tkz_cfg_fpath = encdec_0_train_path / TOKENIZER_CFG_FNAME\n",
    "encdec_0_model_cfg_fpath = encdec_0_train_path / ENCDEC_MODEL_CFG_FNAME\n",
    "encdec_1_model_cfg_fpath = encdec_1_train_path / ENCDEC_MODEL_CFG_FNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encdec_tkz_cfg = parse_yaml_file_as(TokenizerCfg, encdec_0_tkz_cfg_fpath)\n",
    "tokenizer = tokenizer_from_config(encdec_tkz_cfg)\n",
    "tok_dict = encdec_tkz_cfg.custom_tokens\n",
    "pad_tok, qbeg_tok, qend_tok = tok_dict['pad'].ind, tok_dict['query_begin'].ind, tok_dict['query_end'].ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "docs_batch_size = 5\n",
    "max_chunks_per_doc = 3\n",
    "model_level = 0\n",
    "device_name = 'cpu'\n",
    "# device_name = 'cuda'\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cache from /home/misha/data/wiki_20200501_en/ch_100_fixed/.mllm/ds.csv\n",
      "Loaded dataset size: 50989207\n"
     ]
    }
   ],
   "source": [
    "ds_loader = WikiDsLoader(\n",
    "    ds_path=DS_DIR_PATH, docs_batch_size=docs_batch_size, max_chunks_per_doc=max_chunks_per_doc,\n",
    "    pad_tok=pad_tok, qbeg_tok=qbeg_tok, qend_tok=qend_tok, device=device,\n",
    ")\n",
    "ds_loader.shuffle(train=True)\n",
    "ds_loader.shuffle(train=False)\n",
    "inp_len = ds_loader.emb_chunk_size if ds_loader.fixed_size else calc_max_inp_size(ds_loader.emb_chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_text(tokens: torch.Tensor) -> str:\n",
    "    tokens = tokens.flatten()\n",
    "    tokens = tokens[tokens != pad_tok]\n",
    "    tokens = list(tokens)\n",
    "    s = tokenizer.decode(tokens)\n",
    "    return s\n",
    "\n",
    "def distance(x: np.ndarray, y: np.ndarray, cosine: bool = False):\n",
    "    if not cosine:\n",
    "        return np.linalg.norm(x - y)\n",
    "    x_norm, y_norm = np.linalg.norm(x), np.linalg.norm(y)\n",
    "    return np.sum(x * y) / (x_norm * y_norm)\n",
    "\n",
    "def text_to_tokens(s: str, qbeg_tok: Optional[int] = None, qend_tok: Optional[int] = None) -> torch.Tensor:\n",
    "    tokens = tokenizer(s)['input_ids']\n",
    "    if qbeg_tok is not None:\n",
    "        assert qend_tok is not None\n",
    "        tokens = [qbeg_tok, *tokens, qend_tok]\n",
    "    n_tokens = len(tokens)\n",
    "    n_padded = n_tokens // inp_len + (n_tokens % inp_len > 0)\n",
    "    res = np.full((n_padded * inp_len, ), pad_tok, dtype=np.int32)\n",
    "    res[:n_tokens] = tokens\n",
    "    res = torch.from_numpy(res).to(device)\n",
    "    res = res.reshape(n_padded, inp_len)\n",
    "    return res\n",
    "\n",
    "def print_dist(target_embs: torch.Tensor, docs_embs: torch.Tensor, target_mask: torch.Tensor, cosine: bool = True):\n",
    "    for i, docs_emb in enumerate(docs_embs.detach().numpy()):\n",
    "        for target_emb in target_embs.detach().numpy():\n",
    "            dist = distance(target_emb, docs_emb, cosine)\n",
    "            print(f'{dist:0.6f} ', end='')\n",
    "        sfx = 'T' if target_mask[i] else 'F'\n",
    "        print(sfx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_encoder.src_word_emb.weight (50271, 256) -0.010897174 1.0290058e-06 0.010897167\n",
      "vocab_encoder.layer_norm.weight (256,) -0.09767299 0.0045641935 0.098901965\n",
      "vocab_encoder.layer_norm.bias (256,) -0.099590324 0.0021672586 0.099857084\n",
      "vocab_decoder.word_prj.weight (50271, 256) -0.010897174 9.409382e-07 0.010897167\n",
      "encoder.a_em () -0.08215385 -0.08215385 -0.08215385\n",
      "encoder.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.10825258 -5.4940843e-05 0.10824862\n",
      "encoder.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.10825103 0.00015384433 0.108227864\n",
      "encoder.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.10825148 -5.2423693e-05 0.10825284\n",
      "encoder.layer_stack.0.slf_attn.fc.weight (256, 256) -0.10825139 -3.4555997e-05 0.10825178\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.09948003 0.0055500614 0.099358074\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09889706 -0.005347998 0.09974853\n",
      "encoder.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.06846524 -8.4352e-06 0.06846412\n",
      "encoder.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.099981 1.6166421e-05 0.09997362\n",
      "encoder.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.06846505 3.0675277e-05 0.06846519\n",
      "encoder.layer_stack.0.pos_ffn.w_2.bias (256,) -0.09983621 -0.00030289555 0.09996893\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.099708155 -0.00318681 0.09989506\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.0980703 -0.003980205 0.09740738\n",
      "encoder.layer_stack.1.slf_attn.w_qs.weight (256, 256) -0.10824646 0.0006427176 0.10825146\n",
      "encoder.layer_stack.1.slf_attn.w_ks.weight (256, 256) -0.10824804 0.0003295138 0.108246826\n",
      "encoder.layer_stack.1.slf_attn.w_vs.weight (256, 256) -0.10825155 -0.00042558755 0.10824574\n",
      "encoder.layer_stack.1.slf_attn.fc.weight (256, 256) -0.10825148 -1.8295075e-05 0.10825222\n",
      "encoder.layer_stack.1.slf_attn.layer_norm.weight (256,) -0.098019026 0.0073615634 0.09968996\n",
      "encoder.layer_stack.1.slf_attn.layer_norm.bias (256,) -0.0989905 -9.994162e-05 0.099269964\n",
      "encoder.layer_stack.1.pos_ffn.w_1.weight (1024, 256) -0.068464704 -9.484346e-05 0.06846455\n",
      "encoder.layer_stack.1.pos_ffn.w_1.bias (1024,) -0.09963824 0.00152518 0.099921204\n",
      "encoder.layer_stack.1.pos_ffn.w_2.weight (256, 1024) -0.06846487 3.42449e-05 0.06846516\n",
      "encoder.layer_stack.1.pos_ffn.w_2.bias (256,) -0.09953528 -0.0030389805 0.09701663\n",
      "encoder.layer_stack.1.pos_ffn.layer_norm.weight (256,) -0.09927466 -0.0046771965 0.0975788\n",
      "encoder.layer_stack.1.pos_ffn.layer_norm.bias (256,) -0.09851589 -0.0048097493 0.09797712\n",
      "encoder.layer_norm.weight (256,) -0.09975065 -0.0027870585 0.09968864\n",
      "encoder.layer_norm.bias (256,) -0.099597946 -0.0064982967 0.09831903\n",
      "decoder.A_emb2sec (100, 256, 256) -0.008113917 -1.3689366e-06 0.008113914\n",
      "decoder.att_layers.0.slf_attn.w_qs.weight (256, 256) -0.108246565 4.8382917e-05 0.10825284\n",
      "decoder.att_layers.0.slf_attn.w_ks.weight (256, 256) -0.10825138 -0.00058397715 0.108251974\n",
      "decoder.att_layers.0.slf_attn.w_vs.weight (256, 256) -0.1082509 0.00012670348 0.10825016\n",
      "decoder.att_layers.0.slf_attn.fc.weight (256, 256) -0.10824721 -0.00020226653 0.108250886\n",
      "decoder.att_layers.0.slf_attn.layer_norm.weight (256,) -0.09823942 0.005237187 0.09957653\n",
      "decoder.att_layers.0.slf_attn.layer_norm.bias (256,) -0.09938612 0.0019852826 0.099554196\n",
      "decoder.att_layers.0.pos_ffn.w_1.weight (1024, 256) -0.06846531 1.4607223e-05 0.0684652\n",
      "decoder.att_layers.0.pos_ffn.w_1.bias (1024,) -0.09967774 9.010115e-05 0.0999732\n",
      "decoder.att_layers.0.pos_ffn.w_2.weight (256, 1024) -0.068464905 -4.9205453e-05 0.06846439\n",
      "decoder.att_layers.0.pos_ffn.w_2.bias (256,) -0.09983879 -0.0027008625 0.09863893\n",
      "decoder.att_layers.0.pos_ffn.layer_norm.weight (256,) -0.09674477 -0.002235183 0.09881339\n",
      "decoder.att_layers.0.pos_ffn.layer_norm.bias (256,) -0.099697664 -0.00053573144 0.0998491\n",
      "decoder.att_layers.1.slf_attn.w_qs.weight (256, 256) -0.108251944 -9.017564e-05 0.108245045\n",
      "decoder.att_layers.1.slf_attn.w_ks.weight (256, 256) -0.10824561 0.00012776356 0.108249664\n",
      "decoder.att_layers.1.slf_attn.w_vs.weight (256, 256) -0.1082519 0.0002147467 0.10825275\n",
      "decoder.att_layers.1.slf_attn.fc.weight (256, 256) -0.108250864 1.0625707e-05 0.10824193\n",
      "decoder.att_layers.1.slf_attn.layer_norm.weight (256,) -0.09921717 0.00027621377 0.09864303\n",
      "decoder.att_layers.1.slf_attn.layer_norm.bias (256,) -0.099591374 -0.001849892 0.0985772\n",
      "decoder.att_layers.1.pos_ffn.w_1.weight (1024, 256) -0.068465285 -6.281519e-05 0.06846443\n",
      "decoder.att_layers.1.pos_ffn.w_1.bias (1024,) -0.099856555 0.000516979 0.099899076\n",
      "decoder.att_layers.1.pos_ffn.w_2.weight (256, 1024) -0.06846494 -0.00016828935 0.068465136\n",
      "decoder.att_layers.1.pos_ffn.w_2.bias (256,) -0.09965072 -0.0014509144 0.09966612\n",
      "decoder.att_layers.1.pos_ffn.layer_norm.weight (256,) -0.09976343 0.0039073434 0.09981258\n",
      "decoder.att_layers.1.pos_ffn.layer_norm.bias (256,) -0.09882019 -0.0023414115 0.09995013\n"
     ]
    }
   ],
   "source": [
    "model_encdec_0_cfg = parse_yaml_file_as(MllmEncdecCfg, encdec_0_model_cfg_fpath)\n",
    "model_encdec_0 = MllmEncdecLevel(model_encdec_0_cfg, 0).to(device)\n",
    "checkpoint_encdec_0 = torch.load(encdec_0_snapshot_fpath)\n",
    "model_encdec_0.load_state_dict(checkpoint_encdec_0['model'], strict=False)\n",
    "model_encdec_0.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.108239725 1.862934e-05 0.10824919\n",
      "encoder.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.10825277 0.00013587641 0.10824961\n",
      "encoder.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.1082463 -0.00013216451 0.1082527\n",
      "encoder.layer_stack.0.slf_attn.fc.weight (256, 256) -0.108250424 -2.480892e-05 0.10824795\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.09999209 -0.002995777 0.09966638\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09989907 -0.004461446 0.099620536\n",
      "encoder.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.06846472 -4.0456533e-05 0.06846482\n",
      "encoder.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.099666536 -0.0029983278 0.09997972\n",
      "encoder.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.06846484 9.350548e-05 0.06846313\n",
      "encoder.layer_stack.0.pos_ffn.w_2.bias (256,) -0.09855949 -0.0028014882 0.09998498\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.09982752 0.004085768 0.09990708\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.098291114 0.00040055043 0.099300444\n",
      "encoder.layer_stack.1.slf_attn.w_qs.weight (256, 256) -0.10824591 -0.000358445 0.1082518\n",
      "encoder.layer_stack.1.slf_attn.w_ks.weight (256, 256) -0.10825284 -0.00029009703 0.10825169\n",
      "encoder.layer_stack.1.slf_attn.w_vs.weight (256, 256) -0.108251095 -0.00035814487 0.108251534\n",
      "encoder.layer_stack.1.slf_attn.fc.weight (256, 256) -0.108240604 1.89196e-06 0.10825129\n",
      "encoder.layer_stack.1.slf_attn.layer_norm.weight (256,) -0.099958315 0.0028456084 0.09811511\n",
      "encoder.layer_stack.1.slf_attn.layer_norm.bias (256,) -0.09988747 -0.0037058324 0.09989363\n",
      "encoder.layer_stack.1.pos_ffn.w_1.weight (1024, 256) -0.06846515 6.207666e-05 0.0684651\n",
      "encoder.layer_stack.1.pos_ffn.w_1.bias (1024,) -0.09987195 -0.0009100188 0.09966483\n",
      "encoder.layer_stack.1.pos_ffn.w_2.weight (256, 1024) -0.06846491 -8.819489e-05 0.06846524\n",
      "encoder.layer_stack.1.pos_ffn.w_2.bias (256,) -0.09812403 0.0031841537 0.09992138\n",
      "encoder.layer_stack.1.pos_ffn.layer_norm.weight (256,) -0.09922594 0.0039007356 0.09834676\n",
      "encoder.layer_stack.1.pos_ffn.layer_norm.bias (256,) -0.09956558 0.0032124794 0.09809977\n",
      "encoder.w_em.weight (1, 100) -0.24132542 0.029445643 0.24278423\n",
      "encoder.layer_norm.weight (256,) -0.09852456 0.005853204 0.09961046\n",
      "encoder.layer_norm.bias (256,) -0.0991748 0.00035346253 0.0968638\n",
      "decoder.A_emb2sec (100, 256, 256) -0.008113915 -7.2696486e-07 0.008113915\n",
      "decoder.att_layers.0.slf_attn.w_qs.weight (256, 256) -0.108251624 -6.771369e-05 0.10824765\n",
      "decoder.att_layers.0.slf_attn.w_ks.weight (256, 256) -0.108250685 0.0006243082 0.108252764\n",
      "decoder.att_layers.0.slf_attn.w_vs.weight (256, 256) -0.108252026 0.00043198478 0.108244\n",
      "decoder.att_layers.0.slf_attn.fc.weight (256, 256) -0.10824968 7.0634764e-05 0.10824977\n",
      "decoder.att_layers.0.slf_attn.layer_norm.weight (256,) -0.0996718 0.004220606 0.099540666\n",
      "decoder.att_layers.0.slf_attn.layer_norm.bias (256,) -0.099504985 -0.00039713876 0.099979855\n",
      "decoder.att_layers.0.pos_ffn.w_1.weight (1024, 256) -0.06846477 0.00010202016 0.06846458\n",
      "decoder.att_layers.0.pos_ffn.w_1.bias (1024,) -0.09995284 -0.0003691296 0.099828996\n",
      "decoder.att_layers.0.pos_ffn.w_2.weight (256, 1024) -0.06846516 -8.996081e-05 0.06846412\n",
      "decoder.att_layers.0.pos_ffn.w_2.bias (256,) -0.099469386 0.0009872721 0.099748574\n",
      "decoder.att_layers.0.pos_ffn.layer_norm.weight (256,) -0.09810197 -0.00014642964 0.099709965\n",
      "decoder.att_layers.0.pos_ffn.layer_norm.bias (256,) -0.099936984 -0.001262113 0.09998336\n",
      "decoder.att_layers.1.slf_attn.w_qs.weight (256, 256) -0.10821877 -0.00022024265 0.10825174\n",
      "decoder.att_layers.1.slf_attn.w_ks.weight (256, 256) -0.10825262 -0.0005204999 0.10825272\n",
      "decoder.att_layers.1.slf_attn.w_vs.weight (256, 256) -0.10824937 0.00012990706 0.10825208\n",
      "decoder.att_layers.1.slf_attn.fc.weight (256, 256) -0.10824859 0.00010096483 0.10825257\n",
      "decoder.att_layers.1.slf_attn.layer_norm.weight (256,) -0.09894111 6.832532e-05 0.09877046\n",
      "decoder.att_layers.1.slf_attn.layer_norm.bias (256,) -0.09906717 0.008434482 0.09861904\n",
      "decoder.att_layers.1.pos_ffn.w_1.weight (1024, 256) -0.068464875 -7.397797e-05 0.06846526\n",
      "decoder.att_layers.1.pos_ffn.w_1.bias (1024,) -0.09976876 0.0023569087 0.09935917\n",
      "decoder.att_layers.1.pos_ffn.w_2.weight (256, 1024) -0.06846454 -3.5207042e-05 0.068465225\n",
      "decoder.att_layers.1.pos_ffn.w_2.bias (256,) -0.09940787 0.0029174774 0.099339984\n",
      "decoder.att_layers.1.pos_ffn.layer_norm.weight (256,) -0.09899398 0.00063764106 0.09948887\n",
      "decoder.att_layers.1.pos_ffn.layer_norm.bias (256,) -0.09946706 -0.0040020114 0.099659435\n"
     ]
    }
   ],
   "source": [
    "model_encdec_1_cfg = parse_yaml_file_as(MllmEncdecCfg, encdec_1_model_cfg_fpath)\n",
    "model_encdec_1 = MllmEncdecLevel(model_encdec_1_cfg, 1).to(device)\n",
    "checkpoint_encdec_1 = torch.load(encdec_1_snapshot_fpath)\n",
    "model_encdec_1.load_state_dict(checkpoint_encdec_1['model'], strict=False)\n",
    "model_encdec_1.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([14, 100]),\n",
       " torch.Size([3, 100]),\n",
       " tensor([False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True, False, False, False]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 10\n",
    "batch = ds_loader.get_batch(i, train=True)\n",
    "docs_chunks, target_chunks, target_mask = batch.gen_tensors()\n",
    "docs_chunks.shape, target_chunks.shape, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|query_begin|> North Caucasian Legion / Mountain-Caucasian LegionThe North Caucasian Legion (Germ.Legion Nordkaukasien) and the Mountain-Caucasian Legion (Germ.Bergkaukasien Legion) legions were created in accordance with the order of 19 February 1942. Initially, its soldiers, recruited from the camps of prisoners of war, deserters, and partly from representatives of emigration were included in the Caucasian-Mohammedan Legion (Germ. Kaukasisch-Mohammedanische Legion). On 2 August 1942, in accordance with the order of 19 February 1942, all fighters of Muslim North Caucasian and Mountain Caucasian (both Muslims and Christians) origin were separated from the Caucasian-Mohamedan Legion into separate North Caucasian / Mountain-Caucasian legions. These Legions consisted of Abkhazians, Circassians, Kabardians, Balkars, Karachais, Chechens, Ingushes, and the peoples of Daghestan. The Kurds, Talyshs and North Ossetians appeared later. According to the researcher Traho R. \"The total number of North Caucasian volunteers since the beginning of the war against the USSR and until 1945 amounted to 28-30 thousand people\". Formally, both of these legions were the Armed Forces of the so-called North Caucasus National Committee <|query_end|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_target = tokens_to_text(target_chunks)\n",
    "s_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|doc_begin|> <|doc_id_begin|> 1920586 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Shrewsbury Flower Show <|doc_title_end|> <|doc_body_begin|> The Shrewsbury Flower Sh\n",
      "<|doc_begin|> <|doc_id_begin|> 1920586 <|doc_id_end|> <|doc_offset_begin|> 92 <|doc_offset_end|>  in the United Kingdom. It is also one of the longest-running shows in the country and featured in the \n",
      "<|doc_begin|> <|doc_id_begin|> 1920586 <|doc_id_end|> <|doc_offset_begin|> 184 <|doc_offset_end|>  show jumping, various forms of music and entertainment, which includes a large firework display on bo\n",
      "<|doc_begin|> <|doc_id_begin|> 137392 <|doc_id_end|> <|doc_offset_begin|> 92 <|doc_offset_end|>  there are a large number of hiking destinations nearby, including Mount Abel, the West Bowl, and East B\n",
      "<|doc_begin|> <|doc_id_begin|> 137392 <|doc_id_end|> <|doc_offset_begin|> 184 <|doc_offset_end|>  directors changes every year. Directors resign and are voted on each year at the Annual General Meetin\n",
      "<|doc_begin|> <|doc_id_begin|> 137392 <|doc_id_end|> <|doc_offset_begin|> 276 <|doc_offset_end|>  in 1979, Mount Cain has grown very little. Starting with one T-bar and one rope-tow, the only thing ad\n",
      "<|doc_begin|> <|doc_id_begin|> 4659978 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Bad Women <|doc_title_end|> <|doc_body_begin|> Bad Women () is a 2001 Italian prison\n",
      "<|doc_begin|> <|doc_id_begin|> 4659978 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|> :Italian films\\nCategory:Italian drama films\\nCategory:2000s prison drama films <|doc_body_end|> <|doc_en\n",
      "<|doc_begin|> <|doc_id_begin|> 3767477 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> North Caucasian Legion / Mountain-Caucasian Legion <|doc_title_end|> <|doc_body_begi\n",
      "<|doc_begin|> <|doc_id_begin|> 3767477 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>  Caucasian-Mohammedan Legion (Germ. Kaukasisch-Mohammedanische Legion). On 2 August 1942, in accordance\n",
      "<|doc_begin|> <|doc_id_begin|> 3767477 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>  Balkars, Karachais, Chechens, Ingushes, and the peoples of Daghestan. The Kurds, Talyshs and North Os\n",
      "<|doc_begin|> <|doc_id_begin|> 4527626 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>  has her sister Ysobel and her soon to be husband the rich Craig Allen give the pair jobs.  Chip tells \n",
      "<|doc_begin|> <|doc_id_begin|> 4527626 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|> Guinn \"Big Boy\" Williams as \"Teddy\" Bear\\nFuzzy Knight as Fuzzy\\nDorothy Christy as Lulubelle\\nLucien Lit\n",
      "<|doc_begin|> <|doc_id_begin|> 4527626 <|doc_id_end|> <|doc_offset_begin|> 273 <|doc_offset_end|>  Nolan as Bob Nolan (Leader, Sons of the Pioneers)\\nSons of the Pioneers as Musicians, Ranch hands\\n\\nSou\n"
     ]
    }
   ],
   "source": [
    "for toks in docs_chunks:\n",
    "    s = tokens_to_text(toks)\n",
    "    print(s[:200].replace('\\n', '\\\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([14, 100, 50271]), torch.float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_chunks_pred = model_encdec_0(docs_chunks)\n",
    "docs_chunks_pred = torch.sigmoid(docs_chunks_pred)\n",
    "docs_chunks_pred.shape, docs_chunks_pred.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 100])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_toks_pred = torch.argmax(docs_chunks_pred, dim=-1)\n",
    "dc_toks_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 478 <|doc_id_begin|> <|doc_id_begin|> 2727 <|doc_id_end|> <|doc_id_begin|> <|doc_id_end|> <|doc_id_end|> <|doc_title_begin|> <|doc_body_begin|> ton ( \" Good <|doc_title_end|> <|doc_title_end|> <|doc_title_end|> <|doc_body_begin|> <|doc_body_begin|>  (,, <|doc_title_end|> <|doc_title_end|>  is is is a a,\\n\\n\\n. the the the,, the the the the,,, the the the,\\n,,., the the the the the\\n- St,,, in the the the, Lon.\\n\\n\\n the the a the the,\\n St L---,,\\n\\n the is a a features the the\n",
      "100 392 <|doc_id_begin|> <|doc_id_begin|> 2727 <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> \\n\\n\\n\\n\\n\\n the is the the the the theised, is the the the the the the the the (,,\\n theThe the the the a a the the the the the the the thebody, the\\n\\n the is is the the the the the <|doc_begin|>,,. the the the the \" \"a., is the is the a a a is a a and of a is the the a rock,, the the music\n",
      "100 339 <|doc_begin|> <|doc_id_begin|> 2727 <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> <|doc_id_end|>  a a, the a a the the,, the the the the a to,, theicated,, the the screens, the the the, the the the the the\\n\\n \"\\n's, the the\\n,\\n\\n\\n \",,, and and the and the the the the\\n\\n\\n\\n\\n\\n\\n\\n\\n E,,\\n\\n,\\n\\n\\n\\nCategory\\n\\n\\n\\n\\n\\nigagaCategory\\n\n",
      "100 412 <|doc_begin|> <|doc_id_begin|> 2727 <|doc_id_end|> <|doc_id_begin|> <|doc_id_end|> <|doc_id_end|>  the is the is the the water,, the theTheheny,, the,,,,,,, <|doc_begin|>,, is the the to the the the the the the Meadows,, the the,,,, River,\\n the the park, the the the the the,, the the the theMorgan,,, the the the the the the <|doc_begin|>,,,, the the public public the the the public public the the the the the\n",
      "100 397 <|doc_id_begin|> <|doc_id_begin|> 2727 <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> <|doc_id_end|>  the public the the the Sunday Sunday the the the the the the the the thehouse,, the the the the the to lobby, the the the the to to public, the\\n\\nThe Outer., the the the's... the the the the,-,, the the the the, shop, the,,,,,, the,, to housing the the the., the the to the the.\\n the the the the\n",
      "100 369 <|doc_begin|> <|doc_id_begin|> 2727 <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> <|doc_id_end|>  the the the,'s, to to to to camp, the.on. the a and was... the a to, the the the the..... the the the the the, the the the the, to to to to the the was. the the the the, the the the the meanwhile was the the the the to was..\\n. the the the the the.y.. the the a boat.. the\n",
      "100 301 <|doc_id_begin|> <|doc_id_begin|> 272791 <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> <|doc_offset_end|> <|doc_title_begin|> Al (\\n\\nCast ())\\n is a film,\\n\\n\\n\\n\\n\\n ( ( (\\n\\n\\n\\n\\n (,,, dos, (\\n\\n,áájui,\\n,, (\\n Kardoii\\n\\n\\n\\n ( (i (\\n\\n\\n\\n\\n-ju (\\n\\n Lashi\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCategory\\n\\n\\n\\n\\n\n",
      "100 167 <|doc_id_begin|> <|doc_id_begin|> 272791 <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> <|doc_offset_end|> \\n\\n\\n\\n\\nCategory\\n films\\n\\nCategoryCategory\\n of of of of\\n\n",
      "100 482 <|doc_id_begin|> <|doc_id_begin|> 272791 <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> <|doc_offset_end|> <|doc_offset_end|> <|doc_body_begin|>  (, ( <|doc_title_end|> <|doc_body_begin|> <|doc_body_begin|> <|doc_begin|>  ( <|doc_title_end|> <|doc_title_end|> \\n the the91 (\\n (Sigi ( (--tan ()\\n\\n the\\n\\n Priv (\\n ( ( ( J J,--tan,\\n,184 the the the military of the the the\\n\\n the the of the the of of of of of of of of the the the ofian ( the the of the the the-ated of the the the\n",
      "100 334 <|doc_id_begin|> <|doc_id_begin|> 2727 <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> <|doc_offset_end|>  (,,,, ( (\\n (-u,,,\\n, (, ( ()\\n\\n\\n\\n the the of of the the the\\n\\n\\n the of of the the of (\\n,, of of of of the the of the the the the,,,,, of of the the of,\\n\\n\\n\\n airstrikes,, the pursuant of\\n,uu-\\n\\n\\n,uuu (\\n\n",
      "100 385 <|doc_begin|> <|doc_id_begin|> 272791 <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> <|doc_offset_end|>  of,,-,i-- Lash,, <|doc_id_end|>, the the the the,,, the the184,,uu,,,,,\\n the the the the the the ofui.\\n the the the the the184 of the the\\n the the the the the of of the the denomination of the the the of of the ofian, the the themostly of the the of of the the the of of of of of\n",
      "100 281 <|doc_begin|> <|doc_id_begin|> 293391 <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> <|doc_offset_end|>  the the,,lie,. the the to the the,,,,, to to,,,,,,, to,,,, rage and to to the and and,,,,,,,,,,,,,,, to a as\\n\\n\\n\\n,\\n\\n\\n\\n chuck Joe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n,,\\n\\n\\n\\n,\\n\\n\n",
      "100 267 <|doc_id_begin|> <|doc_id_begin|> 272791 <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> <|doc_offset_end|> ig (\\n\\n\\n\\n\\n\\n\\n\\nGraham\\n\\n\\n\\nyy (\\n\\nyie\\n\\n\\n\\n (\\n\\n\\n Hzzzie,,,\\n\\n,,,,,,ie,,\\n\\n\\n\\n,,\\n,,\\n\\n Mc,\\n\\n\\n\\n\\n\\n\\n,\\n\\n\\n\\n\\n,\\n\\n\\n\\n\\n\\n\\n\\n,\\n\\n\\n\\n\\n\\n\n",
      "100 304 <|doc_id_begin|> <|doc_id_begin|> 292791 <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> <|doc_offset_end|> \\n,,,, Connor,,\\n\\n Talking,\\n\\n\\n\\n\\n\\n,\\n\\n\\n\\n\\n Back, the film\\n\\n\\n\\n\\n,\\n\\n \"ipa\\n\\n\\n\\n \" Lou,\\n\\n\\n\\n,\\n\\n by the\\n\\n\\n (\\n and film\\n \" R Has,\\n\\n \"unc,\\n\\n\\n\\n \",ii (\\n\\n\\n\\n \"o,\" film film\n"
     ]
    }
   ],
   "source": [
    "for toks in dc_toks_pred:\n",
    "    s = tokens_to_text(toks)\n",
    "    s = s.replace('\\n', '\\\\n')\n",
    "    print(len(toks), len(s), s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = [\n",
    "    'Hello, my name is Mikhail',\n",
    "    'Malaga is a city in Spain',\n",
    "    'LLM stands for Large Language Model',\n",
    "    'You\\'d better learn new modeling approaches first, Mikhail from Malaga, Spain!',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "Hello, my name is Mikhail\n",
      "Malaga is a city in Spain\n",
      "LLM stands for Large Language Model\n",
      "You'd better learn new modeling approaches first, Mikhail from Malaga, Spain!\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for txt in txts:\n",
    "    toks = text_to_tokens(txt)\n",
    "    print(toks.shape)\n",
    "    chunks.append(toks)\n",
    "\n",
    "chunks = torch.concat(chunks)\n",
    "for toks in chunks:\n",
    "    s = tokens_to_text(toks)\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100, 50271]) torch.float32\n",
      "torch.Size([4, 100])\n"
     ]
    }
   ],
   "source": [
    "chunks_pred = model_encdec_0(chunks)\n",
    "# chunks_pred = torch.sigmoid(chunks_pred)\n",
    "chunks_pred = torch.softmax(chunks_pred, dim=-1)\n",
    "print(chunks_pred.shape, chunks_pred.dtype)\n",
    "toks_pred = torch.argmax(chunks_pred, dim=-1)\n",
    "print(toks_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of <|query_end|> <|query_end|> <|query_end|> CategoryM <|doc_body_end|> <|query_end|> <|doc_body_end|>\n",
      "igi <|doc_body_end|> <|doc_body_end|> <|doc_body_end|> <|query_end|> <|doc_body_end|> <|query_end|> <|doc_body_end|> <|doc_body_end|>\n",
      "\\n\\n <|doc_body_end|> -\\n <|doc_body_end|> Category <|query_end|> Category\n",
      " of the of of the the the of\\n\\nhia <|query_end|> <|query_end|> <|query_end|>\n"
     ]
    }
   ],
   "source": [
    "for toks in toks_pred:\n",
    "    s = tokens_to_text(toks)\n",
    "    s = s.replace('\\n', '\\\\n')\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0121, 0.0132, 0.0157, 0.0234, 0.0613, 0.0251, 0.0228, 0.0181],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "[',', '-', '\\n', ' the', ' of', ' in', ' and', ' (']\n"
     ]
    }
   ],
   "source": [
    "probs = chunks_pred[0][0]\n",
    "inds = torch.arange(len(probs))\n",
    "probs_mask = probs >= 0.95\n",
    "print(probs[probs_mask])\n",
    "toks = inds[probs_mask]\n",
    "strs = []\n",
    "for tok in toks:\n",
    "    toks_ = torch.Tensor([tok])\n",
    "    s = tokens_to_text(toks_)\n",
    "    strs.append(s)\n",
    "print(strs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
