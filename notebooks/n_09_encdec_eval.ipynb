{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Optional\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pydantic_yaml import parse_yaml_file_as\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, AddedToken, PreTrainedTokenizer\n",
    "\n",
    "from mllm.data.wiki.dswiki import WikiDsLoader\n",
    "from mllm.exp.args import TOKENIZER_CFG_FNAME, ENCDEC_MODEL_CFG_FNAME, RANKER_MODEL_CFG_FNAME\n",
    "from mllm.model.mllm_encdec import MllmEncdecLevel\n",
    "from mllm.model.mllm_ranker import MllmRankerLevel\n",
    "from mllm.config.model import TokenizerCfg, MllmEncdecCfg, MllmRankerCfg\n",
    "from mllm.tokenization.chunk_tokenizer import calc_max_inp_size, gen_all_tokens, ChunkTokenizer, tokenizer_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(os.path.expandvars('$HOME')) / 'data'\n",
    "DS_DIR_PATH = DATA_PATH / 'wiki_20200501_en' / 'ch_100_fixed'\n",
    "\n",
    "TRAIN_ENCDEC_0_PATH = DATA_PATH / 'train_mllm_encdec_0'\n",
    "TRAIN_ENCDEC_1_PATH = DATA_PATH / 'train_mllm_encdec_1'\n",
    "# encdec_0_subdir = 'encdec-lvl0-20241028_093737-wiki_20200501_en-ch_100_fixed-enc-lrs3-embmatFalse-d256-h8-dec-lrs3-seqlen100-d256-h8-vocdecTrue'\n",
    "# encdec_0_subdir = 'encdec-lvl0-20241028_212210-wiki_20200501_en-ch_100_fixed-enc-lrs2-embmatFalse-d256-h8-dec-lrs2-seqlen100-d256-h8-vocdecTrue'\n",
    "# encdec_0_subdir = 'encdec-lvl0-20241029_140645-wiki_20200501_en-ch_100_fixed-enc-lrs3-embmatFalse-d256-h8-dec-lrs3-seqlen100-d256-h8-vocdecTrue'\n",
    "encdec_0_subdir = 'encdec-lvl0-20241030_090802-wiki_20200501_en-ch_100_fixed-enc-lrs4-embmatFalse-d256-h8-dec-lrs4-seqlen100-d256-h8-vocdecTrue'\n",
    "encdec_1_subdir = 'encdec-lvl1-20241022_224217-msmarco-fever-enc-lrs2-embmatTrue-d256-h8-dec-lrs2-seqlen100-d256-h8'\n",
    "\n",
    "encdec_0_train_path = TRAIN_ENCDEC_0_PATH / encdec_0_subdir\n",
    "encdec_1_train_path = TRAIN_ENCDEC_1_PATH / encdec_1_subdir\n",
    "encdec_0_snapshot_fpath = encdec_0_train_path / 'last.pth'\n",
    "encdec_1_snapshot_fpath = encdec_1_train_path / 'best.pth'\n",
    "encdec_0_tkz_cfg_fpath = encdec_0_train_path / TOKENIZER_CFG_FNAME\n",
    "encdec_0_model_cfg_fpath = encdec_0_train_path / ENCDEC_MODEL_CFG_FNAME\n",
    "encdec_1_model_cfg_fpath = encdec_1_train_path / ENCDEC_MODEL_CFG_FNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encdec_tkz_cfg = parse_yaml_file_as(TokenizerCfg, encdec_0_tkz_cfg_fpath)\n",
    "tokenizer = tokenizer_from_config(encdec_tkz_cfg)\n",
    "tok_dict = encdec_tkz_cfg.custom_tokens\n",
    "pad_tok, qbeg_tok, qend_tok = tok_dict['pad'].ind, tok_dict['query_begin'].ind, tok_dict['query_end'].ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "docs_batch_size = 5\n",
    "max_chunks_per_doc = 3\n",
    "model_level = 0\n",
    "device_name = 'cpu'\n",
    "# device_name = 'cuda'\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cache from /home/misha/data/wiki_20200501_en/ch_100_fixed/.mllm/ds.csv\n",
      "Loaded dataset size: 50989207\n"
     ]
    }
   ],
   "source": [
    "ds_loader = WikiDsLoader(\n",
    "    ds_path=DS_DIR_PATH, docs_batch_size=docs_batch_size, max_chunks_per_doc=max_chunks_per_doc,\n",
    "    pad_tok=pad_tok, qbeg_tok=qbeg_tok, qend_tok=qend_tok, device=device,\n",
    ")\n",
    "ds_loader.shuffle(train=True)\n",
    "ds_loader.shuffle(train=False)\n",
    "inp_len = ds_loader.emb_chunk_size if ds_loader.fixed_size else calc_max_inp_size(ds_loader.emb_chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_text(tokens: torch.Tensor) -> str:\n",
    "    tokens = tokens.flatten()\n",
    "    tokens = tokens[tokens != pad_tok]\n",
    "    tokens = list(tokens)\n",
    "    s = tokenizer.decode(tokens)\n",
    "    return s\n",
    "\n",
    "def distance(x: np.ndarray, y: np.ndarray, cosine: bool = False):\n",
    "    if not cosine:\n",
    "        return np.linalg.norm(x - y)\n",
    "    x_norm, y_norm = np.linalg.norm(x), np.linalg.norm(y)\n",
    "    return np.sum(x * y) / (x_norm * y_norm)\n",
    "\n",
    "def text_to_tokens(s: str, qbeg_tok: Optional[int] = None, qend_tok: Optional[int] = None) -> torch.Tensor:\n",
    "    tokens = tokenizer(s)['input_ids']\n",
    "    if qbeg_tok is not None:\n",
    "        assert qend_tok is not None\n",
    "        tokens = [qbeg_tok, *tokens, qend_tok]\n",
    "    n_tokens = len(tokens)\n",
    "    n_padded = n_tokens // inp_len + (n_tokens % inp_len > 0)\n",
    "    res = np.full((n_padded * inp_len, ), pad_tok, dtype=np.int32)\n",
    "    res[:n_tokens] = tokens\n",
    "    res = torch.from_numpy(res).to(device)\n",
    "    res = res.reshape(n_padded, inp_len)\n",
    "    return res\n",
    "\n",
    "def print_dist(target_embs: torch.Tensor, docs_embs: torch.Tensor, target_mask: torch.Tensor, cosine: bool = True):\n",
    "    for i, docs_emb in enumerate(docs_embs.detach().numpy()):\n",
    "        for target_emb in target_embs.detach().numpy():\n",
    "            dist = distance(target_emb, docs_emb, cosine)\n",
    "            print(f'{dist:0.6f} ', end='')\n",
    "        sfx = 'T' if target_mask[i] else 'F'\n",
    "        print(sfx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_encoder.src_word_emb.weight (50271, 256) -0.010897174 1.916379e-06 0.010897173\n",
      "vocab_encoder.layer_norm.weight (256,) -0.09989282 -0.001876442 0.09918808\n",
      "vocab_encoder.layer_norm.bias (256,) -0.09993392 -0.0039835246 0.09995335\n",
      "vocab_decoder.word_prj.weight (50271, 256) -0.010897173 2.8820418e-06 0.01089717\n",
      "encoder.a_em () 0.070763186 0.070763186 0.070763186\n",
      "encoder.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.108251855 0.00016167221 0.10825132\n",
      "encoder.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.108241715 -0.00020803144 0.108245455\n",
      "encoder.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.10825175 -0.0003009955 0.108245976\n",
      "encoder.layer_stack.0.slf_attn.fc.weight (256, 256) -0.10825185 -3.9326944e-05 0.10825074\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.09912782 -0.001627505 0.09945027\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09983226 0.002700204 0.099729896\n",
      "encoder.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.06846522 1.5313053e-05 0.06846465\n",
      "encoder.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.09985726 -0.0011212373 0.09981793\n",
      "encoder.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.06846404 0.00017939806 0.06846487\n",
      "encoder.layer_stack.0.pos_ffn.w_2.bias (256,) -0.09913249 0.0039988947 0.09933888\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.098985545 -0.0037791 0.09791376\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.09904337 -0.0008716835 0.099795856\n",
      "encoder.layer_stack.1.slf_attn.w_qs.weight (256, 256) -0.10824218 -8.703915e-05 0.10825175\n",
      "encoder.layer_stack.1.slf_attn.w_ks.weight (256, 256) -0.10825183 -0.0005373577 0.10825302\n",
      "encoder.layer_stack.1.slf_attn.w_vs.weight (256, 256) -0.10825317 0.00017088345 0.10824857\n",
      "encoder.layer_stack.1.slf_attn.fc.weight (256, 256) -0.10824588 0.00011356115 0.10825148\n",
      "encoder.layer_stack.1.slf_attn.layer_norm.weight (256,) -0.09995475 -0.0006807664 0.09951402\n",
      "encoder.layer_stack.1.slf_attn.layer_norm.bias (256,) -0.09973601 0.00013771025 0.0998544\n",
      "encoder.layer_stack.1.pos_ffn.w_1.weight (1024, 256) -0.06846432 0.00020470019 0.06846506\n",
      "encoder.layer_stack.1.pos_ffn.w_1.bias (1024,) -0.09937729 -0.0025174494 0.0999243\n",
      "encoder.layer_stack.1.pos_ffn.w_2.weight (256, 1024) -0.06846444 -5.9331367e-05 0.06846455\n",
      "encoder.layer_stack.1.pos_ffn.w_2.bias (256,) -0.09971644 -0.0034170914 0.09868628\n",
      "encoder.layer_stack.1.pos_ffn.layer_norm.weight (256,) -0.0996471 -0.00556294 0.099330716\n",
      "encoder.layer_stack.1.pos_ffn.layer_norm.bias (256,) -0.09824597 -0.0044738427 0.09957587\n",
      "encoder.layer_stack.2.slf_attn.w_qs.weight (256, 256) -0.108247854 -0.00016737779 0.1082508\n",
      "encoder.layer_stack.2.slf_attn.w_ks.weight (256, 256) -0.10825014 0.0001269711 0.10825122\n",
      "encoder.layer_stack.2.slf_attn.w_vs.weight (256, 256) -0.108253054 6.348556e-05 0.10825182\n",
      "encoder.layer_stack.2.slf_attn.fc.weight (256, 256) -0.10825303 0.0004604878 0.108252525\n",
      "encoder.layer_stack.2.slf_attn.layer_norm.weight (256,) -0.09845128 -0.0005419898 0.09973723\n",
      "encoder.layer_stack.2.slf_attn.layer_norm.bias (256,) -0.09875881 0.0033481037 0.09907694\n",
      "encoder.layer_stack.2.pos_ffn.w_1.weight (1024, 256) -0.06846463 1.6960175e-06 0.06846525\n",
      "encoder.layer_stack.2.pos_ffn.w_1.bias (1024,) -0.099952854 0.0008037472 0.09931796\n",
      "encoder.layer_stack.2.pos_ffn.w_2.weight (256, 1024) -0.06846529 -1.3807923e-05 0.068465136\n",
      "encoder.layer_stack.2.pos_ffn.w_2.bias (256,) -0.09935751 0.0049164454 0.09957256\n",
      "encoder.layer_stack.2.pos_ffn.layer_norm.weight (256,) -0.09995239 0.0010855373 0.09974631\n",
      "encoder.layer_stack.2.pos_ffn.layer_norm.bias (256,) -0.0966383 -0.0045371624 0.099334754\n",
      "encoder.layer_stack.3.slf_attn.w_qs.weight (256, 256) -0.108250655 -0.00057797006 0.108248025\n",
      "encoder.layer_stack.3.slf_attn.w_ks.weight (256, 256) -0.10825262 -3.0325318e-06 0.10825298\n",
      "encoder.layer_stack.3.slf_attn.w_vs.weight (256, 256) -0.108247034 0.00025363098 0.10824357\n",
      "encoder.layer_stack.3.slf_attn.fc.weight (256, 256) -0.108250335 5.6747478e-05 0.108246796\n",
      "encoder.layer_stack.3.slf_attn.layer_norm.weight (256,) -0.09978626 0.0058750887 0.09989667\n",
      "encoder.layer_stack.3.slf_attn.layer_norm.bias (256,) -0.09998878 -0.0006789521 0.09874203\n",
      "encoder.layer_stack.3.pos_ffn.w_1.weight (1024, 256) -0.068465225 1.6253856e-05 0.068464175\n",
      "encoder.layer_stack.3.pos_ffn.w_1.bias (1024,) -0.09981664 -0.0013651744 0.09997524\n",
      "encoder.layer_stack.3.pos_ffn.w_2.weight (256, 1024) -0.06846512 7.1862974e-05 0.06846495\n",
      "encoder.layer_stack.3.pos_ffn.w_2.bias (256,) -0.098599195 0.0003669185 0.09991653\n",
      "encoder.layer_stack.3.pos_ffn.layer_norm.weight (256,) -0.0995723 -0.00011967623 0.09951264\n",
      "encoder.layer_stack.3.pos_ffn.layer_norm.bias (256,) -0.099193744 -0.00041198172 0.09916847\n",
      "encoder.layer_norm.weight (256,) -0.09979961 -0.004643022 0.09983575\n",
      "encoder.layer_norm.bias (256,) -0.09928738 0.0047452734 0.09878588\n",
      "decoder.A_emb2sec (100, 256, 256) -0.008113917 -7.864775e-07 0.008113917\n",
      "decoder.att_layers.0.slf_attn.w_qs.weight (256, 256) -0.108241804 0.0005465889 0.1082512\n",
      "decoder.att_layers.0.slf_attn.w_ks.weight (256, 256) -0.10824905 0.00033109463 0.108252235\n",
      "decoder.att_layers.0.slf_attn.w_vs.weight (256, 256) -0.108250245 6.380665e-05 0.10824068\n",
      "decoder.att_layers.0.slf_attn.fc.weight (256, 256) -0.10825247 -6.109883e-05 0.108253\n",
      "decoder.att_layers.0.slf_attn.layer_norm.weight (256,) -0.09935335 -0.002563278 0.09988769\n",
      "decoder.att_layers.0.slf_attn.layer_norm.bias (256,) -0.099671304 0.0005513062 0.097979955\n",
      "decoder.att_layers.0.pos_ffn.w_1.weight (1024, 256) -0.068464845 -4.9449867e-05 0.06846509\n",
      "decoder.att_layers.0.pos_ffn.w_1.bias (1024,) -0.09922715 -0.0027102176 0.09995542\n",
      "decoder.att_layers.0.pos_ffn.w_2.weight (256, 1024) -0.06846438 -5.6607343e-05 0.06846512\n",
      "decoder.att_layers.0.pos_ffn.w_2.bias (256,) -0.09971013 -0.0047435286 0.099209644\n",
      "decoder.att_layers.0.pos_ffn.layer_norm.weight (256,) -0.09910321 0.0008013259 0.0999193\n",
      "decoder.att_layers.0.pos_ffn.layer_norm.bias (256,) -0.09973947 0.0003017718 0.09882277\n",
      "decoder.att_layers.1.slf_attn.w_qs.weight (256, 256) -0.10825294 0.0003126968 0.10825185\n",
      "decoder.att_layers.1.slf_attn.w_ks.weight (256, 256) -0.108251944 7.169128e-05 0.10825036\n",
      "decoder.att_layers.1.slf_attn.w_vs.weight (256, 256) -0.10824535 -0.00017205266 0.10825032\n",
      "decoder.att_layers.1.slf_attn.fc.weight (256, 256) -0.108251624 0.00035511857 0.10824491\n",
      "decoder.att_layers.1.slf_attn.layer_norm.weight (256,) -0.099761724 -0.0052597867 0.09849937\n",
      "decoder.att_layers.1.slf_attn.layer_norm.bias (256,) -0.098582864 0.0003433897 0.09969529\n",
      "decoder.att_layers.1.pos_ffn.w_1.weight (1024, 256) -0.06846249 0.000109367385 0.06846475\n",
      "decoder.att_layers.1.pos_ffn.w_1.bias (1024,) -0.09986233 -0.004473315 0.09942\n",
      "decoder.att_layers.1.pos_ffn.w_2.weight (256, 1024) -0.068463795 1.618879e-05 0.06846498\n",
      "decoder.att_layers.1.pos_ffn.w_2.bias (256,) -0.09745475 -0.0011385753 0.09903362\n",
      "decoder.att_layers.1.pos_ffn.layer_norm.weight (256,) -0.09844475 -0.00036426933 0.09941631\n",
      "decoder.att_layers.1.pos_ffn.layer_norm.bias (256,) -0.09877547 0.0013597659 0.09876452\n",
      "decoder.att_layers.2.slf_attn.w_qs.weight (256, 256) -0.1082481 0.00023899309 0.1082513\n",
      "decoder.att_layers.2.slf_attn.w_ks.weight (256, 256) -0.10824482 0.0001936179 0.10824501\n",
      "decoder.att_layers.2.slf_attn.w_vs.weight (256, 256) -0.10824677 0.00020418345 0.10824926\n",
      "decoder.att_layers.2.slf_attn.fc.weight (256, 256) -0.10825182 0.0003779799 0.10825257\n",
      "decoder.att_layers.2.slf_attn.layer_norm.weight (256,) -0.09877789 -0.00048457598 0.098415926\n",
      "decoder.att_layers.2.slf_attn.layer_norm.bias (256,) -0.09964546 -0.0018326539 0.09916949\n",
      "decoder.att_layers.2.pos_ffn.w_1.weight (1024, 256) -0.06846474 -9.97961e-05 0.0684644\n",
      "decoder.att_layers.2.pos_ffn.w_1.bias (1024,) -0.09997084 -0.0015371519 0.09984882\n",
      "decoder.att_layers.2.pos_ffn.w_2.weight (256, 1024) -0.068464525 3.6507405e-05 0.06846529\n",
      "decoder.att_layers.2.pos_ffn.w_2.bias (256,) -0.09902554 0.002446737 0.09759295\n",
      "decoder.att_layers.2.pos_ffn.layer_norm.weight (256,) -0.09971066 -0.004243191 0.09956788\n",
      "decoder.att_layers.2.pos_ffn.layer_norm.bias (256,) -0.09826471 -0.0038112972 0.0987463\n",
      "decoder.att_layers.3.slf_attn.w_qs.weight (256, 256) -0.10825215 0.00012783887 0.108248115\n",
      "decoder.att_layers.3.slf_attn.w_ks.weight (256, 256) -0.10825151 0.00011959493 0.10825104\n",
      "decoder.att_layers.3.slf_attn.w_vs.weight (256, 256) -0.108252235 -1.8556922e-05 0.10825192\n",
      "decoder.att_layers.3.slf_attn.fc.weight (256, 256) -0.10824575 0.00016443977 0.10824882\n",
      "decoder.att_layers.3.slf_attn.layer_norm.weight (256,) -0.09989897 0.00043029094 0.09952297\n",
      "decoder.att_layers.3.slf_attn.layer_norm.bias (256,) -0.098940395 0.004186769 0.098416425\n",
      "decoder.att_layers.3.pos_ffn.w_1.weight (1024, 256) -0.0684647 -3.20951e-05 0.06846501\n",
      "decoder.att_layers.3.pos_ffn.w_1.bias (1024,) -0.099653326 0.0020120305 0.099878095\n",
      "decoder.att_layers.3.pos_ffn.w_2.weight (256, 1024) -0.0684653 6.8986e-05 0.068464115\n",
      "decoder.att_layers.3.pos_ffn.w_2.bias (256,) -0.09946276 -0.0015161067 0.09979744\n",
      "decoder.att_layers.3.pos_ffn.layer_norm.weight (256,) -0.09921583 -0.009102571 0.099166416\n",
      "decoder.att_layers.3.pos_ffn.layer_norm.bias (256,) -0.09801375 -9.532168e-05 0.09536278\n"
     ]
    }
   ],
   "source": [
    "model_encdec_0_cfg = parse_yaml_file_as(MllmEncdecCfg, encdec_0_model_cfg_fpath)\n",
    "model_encdec_0 = MllmEncdecLevel(model_encdec_0_cfg, 0).to(device)\n",
    "checkpoint_encdec_0 = torch.load(encdec_0_snapshot_fpath)\n",
    "model_encdec_0.load_state_dict(checkpoint_encdec_0['model'], strict=False)\n",
    "model_encdec_0.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ParsingModel[MllmEncdecCfg]\n__root__ -> with_vocab_decoder\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_encdec_1_cfg \u001b[38;5;241m=\u001b[39m \u001b[43mparse_yaml_file_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMllmEncdecCfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencdec_1_model_cfg_fpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model_encdec_1 \u001b[38;5;241m=\u001b[39m MllmEncdecLevel(model_encdec_1_cfg, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m checkpoint_encdec_1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(encdec_1_snapshot_fpath)\n",
      "File \u001b[0;32m~/miniconda3/envs/mllm/lib/python3.10/site-packages/pydantic_yaml/_internals/v1.py:74\u001b[0m, in \u001b[0;36mparse_yaml_file_as\u001b[0;34m(model_type, file)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected Path, str or IO, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file\u001b[38;5;241m.\u001b[39mopen(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_yaml_raw_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mllm/lib/python3.10/site-packages/pydantic_yaml/_internals/v1.py:49\u001b[0m, in \u001b[0;36mparse_yaml_raw_as\u001b[0;34m(model_type, raw)\u001b[0m\n\u001b[1;32m     47\u001b[0m reader \u001b[38;5;241m=\u001b[39m YAML(typ\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, pure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# YAML 1.2 support\u001b[39;00m\n\u001b[1;32m     48\u001b[0m objects \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mload(stream)\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_obj_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mllm/lib/python3.10/site-packages/pydantic/tools.py:38\u001b[0m, in \u001b[0;36mpydantic.tools.parse_obj_as\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/mllm/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ParsingModel[MllmEncdecCfg]\n__root__ -> with_vocab_decoder\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "model_encdec_1_cfg = parse_yaml_file_as(MllmEncdecCfg, encdec_1_model_cfg_fpath)\n",
    "model_encdec_1 = MllmEncdecLevel(model_encdec_1_cfg, 1).to(device)\n",
    "checkpoint_encdec_1 = torch.load(encdec_1_snapshot_fpath)\n",
    "model_encdec_1.load_state_dict(checkpoint_encdec_1['model'], strict=False)\n",
    "model_encdec_1.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 100]),\n",
       " torch.Size([3, 100]),\n",
       " tensor([False, False, False, False,  True,  True,  True, False, False, False,\n",
       "         False, False, False]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 10\n",
    "batch = ds_loader.get_batch(i, train=False)\n",
    "docs_chunks, target_chunks, target_mask = batch.gen_tensors()\n",
    "docs_chunks.shape, target_chunks.shape, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|query_begin|> San Juan de Dios ParkSan Juan de Dios Park  (in Spanish Colonias San Juan de Dios) (also known as San Juan) is a zone situated in the south of Mexico City, in the delegation Tlalpan. Has his origin in the Inland revenue of San Juan of God \"The Big\". The zone is conformed by the colonies Hacienda de San Juan, Villa Lázaro Cárdenas,Ex Hacienda San Juan de Dios, Arboledas Del Sur, Hacienda de San Juan 2nd Section, Chimalli, The Colorines, Guadeloupe Tlalpan and the colony AMSA. The zone houses big number of parks scattered in all his colonies as well as it also has commercial squares like the shopping centre Paseo Acoxpa. Also the zone of San Juan basin with different urban services like transport, educational and of health. San Juan of God is a residential zone mostly, has different private residentials and also private or private streets. Many of his colonies also belong to the zone of Coapa.\\n\\nIt is one of the south exclusive zones of the Mexico City. It limits with the colonies Huipulco west, Prado Coapa 3ra Sección and Narciso Mendoza to the east <|query_end|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_target = tokens_to_text(target_chunks)\n",
    "s_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|doc_begin|> <|doc_id_begin|> 6053886 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> List of vice-admirals of Sussex <|doc_title_end|> <|doc_body_begin|> This is a list of people who have served as Vice-Admiral of Sussex.\\n\\nSir William More 1559–1600\\nvacant\\nCharles Howard, 2nd Earl of Nottingham 1608–1642\\nFrancis Lennard, 14th Baron Dacre bef. 1647–1650\\nAnthony Stapley 1651–1655 (Parliamentary)\n",
      "<|doc_begin|> <|doc_id_begin|> 6053886 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|> \\nvacant\\nSir John Pelham, 3rd Baronet 1660–1703\\nCharles Goring 1703–1705\\nThomas Pelham, 1st Baron Pelham 1705–1712\\nJohn Ashburnham, 3rd Baron Ashburnham 1712–1715\\nThomas Pelham-Holles, 1st Duke of Newcastle 1715–1768\\nvacant\\nJohn Ashburnham, 2nd\n",
      "<|doc_begin|> <|doc_id_begin|> 6053886 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>  Earl of Ashburnham 1770–1812\\nCharles Lennox, 4th Duke of Richmond 1812–1819\\nGeorge Wyndham, 3rd Earl of Egremont 1820–1831\\nCharles Gordon-Lennox, 5th Duke of Richmond 1831–1860\\n\\nReferences\\nInstitute of Historical Research\\n\\nSomerset\\nCategory:Military history of Sussex <|doc_body_end|> <|doc_end|>\n",
      "<|doc_begin|> <|doc_id_begin|> 5958751 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> WCRI <|doc_title_end|> <|doc_body_begin|> WCRI may refer to:\\n\\n WCRI-FM, a radio station (95.9 FM) licensed to Block Island, Rhode Island, United States\\n WSKP (AM), a radio station (1180 AM) licensed to Hope Valley, Rhode Island, United States, known as WCRI from 2011 to 2013\\n Walker Cancer Research Institute, a small American cancer research organization <|doc_body_end|> <|doc_end|>\n",
      "<|doc_begin|> <|doc_id_begin|> 6006717 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> San Juan de Dios Park <|doc_title_end|> <|doc_body_begin|> San Juan de Dios Park  (in Spanish Colonias San Juan de Dios) (also known as San Juan) is a zone situated in the south of Mexico City, in the delegation Tlalpan. Has his origin in the Inland revenue of San Juan of God \"The Big\". The zone is conformed by the colonies Hacienda de San Juan, Villa Lázaro C\n",
      "<|doc_begin|> <|doc_id_begin|> 6006717 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|> árdenas,Ex Hacienda San Juan de Dios, Arboledas Del Sur, Hacienda de San Juan 2nd Section, Chimalli, The Colorines, Guadeloupe Tlalpan and the colony AMSA. The zone houses big number of parks scattered in all his colonies as well as it also has commercial squares like the shopping centre Paseo Acoxpa. Also the zone of San Juan basin\n",
      "<|doc_begin|> <|doc_id_begin|> 6006717 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>  with different urban services like transport, educational and of health. San Juan of God is a residential zone mostly, has different private residentials and also private or private streets. Many of his colonies also belong to the zone of Coapa.\\n\\nIt is one of the south exclusive zones of the Mexico City. It limits with the colonies Huipulco west, Prado Coapa 3ra Sección and Narciso Mendoza to the east\n",
      "<|doc_begin|> <|doc_id_begin|> 5856094 <|doc_id_end|> <|doc_offset_begin|> 3430 <|doc_offset_end|>'on the covers, a fictional band which was later put together for real, also as The Archies.\\n\\nThere was a minor revival in superhero characters between #150 (Oct. 1961) and #160 (Jan. 1963), when Archie Comics included a short run of stories featuring their recent superheroes The Fly, Flygirl and The Jaguar in loose rotation, plus one extra Jaguar story in #168 (January 1964). In addition, issue #\n",
      "<|doc_begin|> <|doc_id_begin|> 5856094 <|doc_id_end|> <|doc_offset_begin|> 3520 <|doc_offset_end|> 393 (Mar. 1984) contained an appearance by Martin Greim's funny animal character Thunderbunny, when Archie Comics briefly licensed the character.Pep Comics published its 200th issue in October 1966, its 300th in April 1975, and its 400th in May 1985—an issue which included cameos of all the Archie Comics staff. However, by then sales had slipped from their previous levels to 55,164. The series lasted until #\n",
      "<|doc_begin|> <|doc_id_begin|> 5856094 <|doc_id_end|> <|doc_offset_begin|> 3610 <|doc_offset_end|> 411 (Mar 1987). A number of the \"Archie Giant Series Magazines\" in the late 1980s and early 1990s carried the Pep Comics name, but it has not been revived since. However, in August 2009 Michael Uslan announced that five one-shot comics reviving the Archie-as-superhero 'Pureheart' concept would be released in 2010, one of those titles being Pep Comics. A special one-shot 'P\n",
      "<|doc_begin|> <|doc_id_begin|> 5870014 <|doc_id_end|> <|doc_offset_begin|> 1359 <|doc_offset_end|>  kitchen at Grebe, on the outskirts of Tilling, and is able, in Lucia's absence, to transcribe the recipe. Lucia discovers her in the act, but before excuses could be given, the sea wall breaks and Lucia and Elizabeth are swept away on the flood, clinging to the kitchen table. They languish for some weeks on an Italian fishing vessel on the Gallagher Bank, eventually returning to Tilling, where Elizabeth discovers that the recipe\n",
      "<|doc_begin|> <|doc_id_begin|> 5870014 <|doc_id_end|> <|doc_offset_begin|> 1449 <|doc_offset_end|>  has survived the ordeal. Elizabeth subsequently serves a correct Lobster à la Riseholme at her wedding breakfast, at which point the reason for her being in Lucia's kitchen becomes clear. The dish is served again in Lucia's Progress (1935) at Lucia's housewarming party following her move to Elizabeth's former residence, Mallards.\\n\\nLobster à la Riseholme reappears in Tom Holt's pastiche, Lucia in W\n",
      "<|doc_begin|> <|doc_id_begin|> 5870014 <|doc_id_end|> <|doc_offset_begin|> 1539 <|doc_offset_end|> artime (1985), set early in the Second World War. Having mastered \"Woolton pie\" (an officially sanctioned vegetable dish named after the wartime Food Minister, Lord Woolton. Georgie's triumphant preparation of Lobster à la Riseholme, using a number of substitute ingredients, leads to his expertise being commended to the Ministry of Food. As a result, he is conscripted for a BBC radio broadcast during which, among\n"
     ]
    }
   ],
   "source": [
    "for toks in docs_chunks:\n",
    "    s = tokens_to_text(toks)\n",
    "    print(s[:600].replace('\\n', '\\\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 100, 50271]), torch.float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_chunks_pred = model_encdec_0(docs_chunks)\n",
    "# docs_chunks_pred = torch.sigmoid(docs_chunks_pred)\n",
    "docs_chunks_pred = torch.softmax(docs_chunks_pred, dim=-1)\n",
    "docs_chunks_pred.shape, docs_chunks_pred.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_toks_pred = torch.argmax(docs_chunks_pred, dim=-1)\n",
    "dc_toks_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 429 <|doc_begin|> <|doc_id_begin|> 172775 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> List of Member-Wreatals of Lords <|doc_title_end|> <|doc_body_begin|> This is a list of people who have held as Vice-Admiral of Norfolk.\\n\\nSir William Baronst90–1600\\nHambell\\nCharles III, 2nd Earl of Hunting 1648–1642\\nFrancis Castard, 14th Baron D Baron with d. 1641–1670\\nWilliam Lennen 1606–1655 (Adliamentus)\n",
      "100 340 <|doc_begin|> <|doc_id_begin|> 172449 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|> \\nHumbons\\nSir John Bingham, 3rd Baron 14 1660–1797\\nCharles A99 1703–1705\\nThomas Dunham, 1st Baron Egham 1705–1714\\nJohn Dundeham, 3rd Baron Dundehamb29–1722\\nThomas Dunham-Hollage, 1st Duke of Newcastle 17et–1768\\nHumbons\\nJohn Dundeham, 2nd\n",
      "100 395 <|doc_begin|> <|doc_id_begin|> 172799 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>  Earl of Edfordham 1775–1814\\nCharles Fitzay, 4th Duke of Richmond 1829–18)\\nGeorge Wynders, 3rd Earl of Camaumet 1818–1807\\nCharles Edward McLettire, 5th Duke of Richmond 1807–1860\\n\\nReferences\\nBuriety of Queen Henry\\n\\nSomerset\\nCategory:Military history of Norfolk <|doc_body_end|> <|doc_end|>\n",
      "100 495 <|doc_begin|> <|doc_id_begin|> 172717 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> WSWS <|doc_title_end|> <|doc_body_begin|> WSWS may refer to:\\n\\n AMOW-FM, a radio station (2013.9 FM) licensed to Las Island, Rhode Island, United States\\n WKSS (AM), a radio station (USOS AM) licensed to Orange County, Rhode Island, United States, known as AMWS in 2011 to 2002\\n Florida Tech Research Institute, a American largest news radio station <|doc_body_end|> <|doc_end|>\n",
      "100 476 <|doc_begin|> <|doc_id_begin|> 161624 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> San Juan de Siosas <|doc_title_end|> <|doc_body_begin|> San Juan de Riosas  ( main Spanish Cuis San Juan de Soria) (also known as San Juan) is a territory situated in the south of Mexicoia, in the Spanish Cacal century. And placed life in the 1890 city of San Juan of Ecuador—The century). The latter is townigned by the Chilean Cación de San Juan, Rio Náocés C\n",
      "100 433 <|doc_begin|> <|doc_id_begin|> 172817 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|> onesa National Park, El Paci Rio San Juan de Roria, Unrioe Alarire, Pación de San La Carnd villages, Avenango, The Travelque, Guayíícacalque and the sub Exades. The To four wide number of tourists% in all or tourism as well as it also have high zones like the metropolitano Calao Brolley regions. Also the expansion of San Juan Francisco\n",
      "100 497 <|doc_begin|> <|doc_id_begin|> 162717 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>  with different urban groups an land, however and of others. San Juan of Ecuador is a residential sites that, has own privateized residents and its private or private cities. Many of January parks also belong to the territory of Quados.\\n\\nIt is one of the first largest provinces of the Mexico City. It houses with the municipality Manapíuo, Quado Qurio Real eñaciña and Nortacio Mendoán to the east\n",
      "100 487 <|doc_begin|> <|doc_id_begin|> 171745 <|doc_id_end|> <|doc_offset_begin|> 2930 <|doc_offset_end|> \" on the left, a comic strip which was only have it for titles, such as The Bay vs.\\n\\nThere was a minor strip in paperback Comics were #mm (1977. 1961) and #49 (1964. Stewart), when Fantasy Comics included a short form of characters were their popular storylines The Seven, Dark comics and The Riders in comic editions, plus one major dice titles in 1649 (with 1964). In addition, short #\n",
      "100 520 <|doc_begin|> <|doc_id_begin|> 171745 <|doc_id_end|> <|doc_offset_begin|> 2620 <|doc_offset_end|>  Marvel (1990. George) played an appearance by Jack Winx's Greatest game 3 Hearbifter, when Fantasy Comics; winning the publisher. Domane had its 11th appearance in October 1966, and 3th in April 1975, and its 95 earlier in May 1985–September title which included become wrestler of all the paperback Comics titles. However, by many magazine had featured from their previous life to 100, editions. The series lasted sold #\n",
      "100 466 <|doc_begin|> <|doc_id_begin|> 171638 <|doc_id_end|> <|doc_offset_begin|> 2710 <|doc_offset_end|>  Trek (Columbine). A series of the \"Con Trek creator vs Monazines\" in the late 1980s and early 1990s called the Mob Comics series, but it has not won officially released. However, in which 2009o Brerie stated that other one-issue mini 2 versions the Spider-on-Super sci-Marvel Trek\"\") would be published in 1969, one of those titles like dub Comics. A 1 one-series-and\n",
      "100 496 <|doc_begin|> <|doc_id_begin|> 173901 <|doc_id_end|> <|doc_offset_begin|> 1359 <|doc_offset_end|>  mansion at Trepe, on the outskirts of Pife, and is found, in Cairo's escape, to recressing the pirates. Lucia also her in the right, but while belongings can be very, the sea man; and Lucia and Elizabeth are severely away on the beach, one into the stolen fire. They confessises for some scenes on an new private hotel on the Gaza Bay, now returning to Parge, where Elizabeth also that the harbour\n",
      "100 481 <|doc_begin|> <|doc_id_begin|> 173960 <|doc_id_end|> <|doc_offset_begin|> 1449 <|doc_offset_end|>  has survived the balcony. Luke and became a second Michester's la Louise Corme at her husband custody, at this point the opportunity for only held in Windsor's funeral were it. The wedding is an again in Windsor's Ms (1935) at chef's house maternity room that her visits to Elizabeth's first wife, Araux.\\n\\nLondilly's Anneabella Pietse mareries in Francesmont's de Hall, chef in St\n",
      "100 474 <|doc_begin|> <|doc_id_begin|> 171660 <|doc_id_end|> <|doc_offset_begin|> 1539 <|doc_offset_end|> otra (1985), she shot in the Second World War. Having his \"Wetherin Raid\" (2013; then Zealandmaker who after the CIA Age Club, Thomas Hartton. Jany's paramilitary issue of Hellster's Royal Baron Vogger, including a number of three films, designed to his customers were attited to the Ministry of Wales. As a result, he is infcripted for a secret ship which during which, about\n"
     ]
    }
   ],
   "source": [
    "for toks in dc_toks_pred:\n",
    "    s = tokens_to_text(toks)\n",
    "    s = s.replace('\\n', '\\\\n')\n",
    "    print(len(toks), len(s), s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = [\n",
    "    '<|doc_begin|> 20 <|doc_id_begin|> 733860 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>Hello, my name is Mikhail<|doc_end|>',\n",
    "    'Malaga is a city in Spain',\n",
    "    'LLM stands for Large <|mask|> Model',\n",
    "    'You\\'d better learn new modeling approaches first, Mikhail from Malaga, Spain!',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "<|doc_begin|>  20  <|doc_id_begin|>  733860  <|doc_id_end|>   <|doc_offset_begin|>  91  <|doc_offset_end|> Hello, my name is Mikhail <|doc_end|>\n",
      "Malaga is a city in Spain\n",
      "LLM stands for Large  <|mask|>  Model\n",
      "You'd better learn new modeling approaches first, Mikhail from Malaga, Spain!\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for txt in txts:\n",
    "    toks = text_to_tokens(txt)\n",
    "    print(toks.shape)\n",
    "    chunks.append(toks)\n",
    "\n",
    "chunks = torch.concat(chunks)\n",
    "for toks in chunks:\n",
    "    s = tokens_to_text(toks)\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100, 50271]) torch.float32\n",
      "torch.Size([4, 100])\n"
     ]
    }
   ],
   "source": [
    "chunks_pred = model_encdec_0(chunks)\n",
    "# chunks_pred = torch.sigmoid(chunks_pred)\n",
    "chunks_pred = torch.softmax(chunks_pred, dim=-1)\n",
    "print(chunks_pred.shape, chunks_pred.dtype)\n",
    "toks_pred = torch.argmax(chunks_pred, dim=-1)\n",
    "print(toks_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alusedis <|doc_id_begin|>  airlines ISO airlinesusedulated <|doc_offset_begin|> 0ountulated\"is <|doc_id_end|>  linkssk generic defunct also0 also is was <|doc_id_end|> <|doc_id_end|> ile generic A0 Comm design <|doc_id_begin|>  also also defunct airlines <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> <|query_end|>  the <|doc_offset_begin|> <|doc_id_end|> <|query_end|> <|query_end|> <|doc_id_end|> <|doc_id_end|>  It distribution to <|doc_begin|>  The, <|doc_id_end|> <|doc_offset_begin|> <|doc_offset_end|>  generic scrapped <|doc_offset_begin|>  airlines generic 2is no, 2 launch a <|doc_id_end|> \\n sub design\\n Black was <|doc_offset_begin|> <|doc_id_end|> <|doc_id_end|>  refer <|doc_offset_begin|> <|doc_id_end|>  airlines\", a and,, <|doc_offset_begin|> <|doc_id_end|>  also <|doc_id_end|> <|doc_id_end|>,\n",
      "or the, the Transportation the theulated links  <|doc_id_begin|>   \\' marketers <|doc_offset_begin|> 0 <|doc_id_begin|>  plays, alsoard\"is is CommItized Rep designis alsois, firstulatedumentsumentsCommun <|doc_offset_begin|> <|doc_id_end|> <|doc_id_end|> orist toor Comm <|doc_id_end|> <|doc_id_end|> or alsoisuments the not <|doc_id_end|> <|doc_offset_begin|>  wasor generic its generic generic alsoor,, 2is a <|doc_id_end|> \\n Commal\\n two was <|query_end|> <|query_end|> <|doc_id_end|> <|doc_id_end|> oror only) <|doc_id_end|> \\n airlines, also <|doc_id_end|> <|doc_id_end|>  also <|doc_id_end|> <|doc_id_end|> encies\n",
      " venuesis, area ISO first first also also,  \\' \\' also0 first controls I also's 2is is toItor Rep mine <|query_end|> \\n\\n 2,, firstenciesuments defunct <|doc_offset_begin|> <|doc_id_end|> <|query_end|> or CommizedItor <|doc_id_end|> <|doc_id_end|>  strat also habitat strat the <|doc_begin|> <|doc_id_end|> <|doc_offset_begin|>  parkordm In generic generic also <|doc_id_end|>,, 2 Chile a <|doc_id_end|> \\n Import\\n water was It <|query_end|> <|doc_id_end|> <|doc_id_end|> oror\\n I wassk airlines  <|doc_id_end|> <|doc_id_end|> <|doc_id_end|>  also <|doc_id_end|> <|doc_id_end|> ized\n",
      "al the, area Transportation the,ulated <|doc_offset_begin|> ulateddm upgraded <|doc_id_begin|>  Alaska <|doc_offset_begin|> is\"\" was also\"\" is \"oris Comm stratis Commisisisis,,,ized generic Sites <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> or also named mineized <|doc_id_end|> enedconst <|doc_offset_begin|>  distribution Comm <|query_end|>  Comm Comm <|doc_offset_begin|> <|doc_offset_begin|> is no airlines generic <|doc_id_begin|>  airlines 2isupon, 2 design the <|doc_id_end|>  Comm sub a\\n 2 was It <|doc_id_end|> <|doc_id_end|> <|doc_id_end|> ororor\"is generic airlines, and <|doc_id_end|> <|doc_id_end|>  also <|doc_id_end|> <|doc_id_end|>  negate\n"
     ]
    }
   ],
   "source": [
    "for toks in toks_pred:\n",
    "    s = tokens_to_text(toks)\n",
    "    s = s.replace('\\n', '\\\\n')\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], grad_fn=<IndexBackward0>)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "probs = chunks_pred[0][8]\n",
    "inds = torch.arange(len(probs))\n",
    "prob_thres = 0.95\n",
    "prob_thres = 0.1\n",
    "probs_mask = probs >= prob_thres\n",
    "print(probs[probs_mask])\n",
    "toks = inds[probs_mask]\n",
    "strs = []\n",
    "for tok in toks:\n",
    "    toks_ = torch.Tensor([tok])\n",
    "    s = tokens_to_text(toks_)\n",
    "    strs.append(s)\n",
    "print(strs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MllmEncdecLevel(\n",
       "  (vocab_encoder): VocabEncoder(\n",
       "    (src_word_emb): Embedding(50271, 256, padding_idx=50267)\n",
       "    (position_enc): PositionalEncoding()\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (vocab_decoder): VocabDecoder(\n",
       "    (word_prj): Linear(in_features=256, out_features=50271, bias=False)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layer_stack): ModuleList(\n",
       "      (0-1): 2 x EncoderLayer(\n",
       "        (slf_attn): MultiHeadAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (fc): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ffn): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): EmbDecoder(\n",
       "    (att_layers): ModuleList(\n",
       "      (0-1): 2 x EncoderLayer(\n",
       "        (slf_attn): MultiHeadAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (fc): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ffn): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_encdec_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_encdec_0.encoder.n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\d{8}_\\d{6} re.compile('^[\\\\w-]+?\\\\-(\\\\d{8}_\\\\d{6})-.+$')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "DT_PAT_RE = r'\\d{8}_\\d{6}'\n",
    "pat = re.compile(r'^[\\w-]+?\\-(%s)-.+$' % DT_PAT_RE)\n",
    "print(DT_PAT_RE, pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encdec-20241018_092135-wiki_20 20241018_092135\n",
      "encdec-lvl0-20241026_120743-wi 20241026_120743\n",
      "encdec-33337128_001122-2024101 33337128_001122\n"
     ]
    }
   ],
   "source": [
    "paths = [\n",
    "    'encdec-20241018_092135-wiki_20200501_en-ch_100_fixed',\n",
    "    'encdec-lvl0-20241026_120743-wiki_20200501_en-ch_100_fixed-enc-lrs2-embmatFalse-d256-h8-dec-lrs2-seqlen100-d256-h8',\n",
    "    'encdec-33337128_001122-20241018_092135-wiki_20200501_en-ch_100_fixed',\n",
    "]\n",
    "for p in paths:\n",
    "    m = pat.match(p)\n",
    "    dt = None\n",
    "    if m:\n",
    "        dt = m.group(1)\n",
    "    print(p[:30], dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
