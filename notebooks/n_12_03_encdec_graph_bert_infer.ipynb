{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pydantic_yaml import parse_yaml_file_as\n",
    "import torch\n",
    "from torch import nn, dist\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, AddedToken, PreTrainedTokenizer, AutoTokenizer, BatchEncoding\n",
    "\n",
    "from mllm.data.wiki.dswiki import WikiDsLoader\n",
    "from mllm.data.utils import RandomInputTokenizer, RandomInputTokenizerV2, TokensSubset, TokensSubsetV2, tokens_subsets_to_tensors, tokens_subsets_v2_to_tensors\n",
    "from mllm.exp.args import ENCDEC_GRAPH_BERT_MODEL_CFG_FNAME\n",
    "from mllm.model.encdec_ranker_hg import EncdecGraphBert\n",
    "from mllm.config.model import EncdecGraphBertCfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# DATA_PATH = Path(os.path.expandvars('$HOME')) / 'data'\n",
    "DATA_PATH = Path('Q:/data')\n",
    "# WIKI_DS_NAME = '20200501.en'\n",
    "WIKI_DS_NAME = '20220301.en'\n",
    "\n",
    "TRAIN_ENCDEC_GRAPH_BERT_PATH = DATA_PATH / 'train_mllm_encdec_graph_bert'\n",
    "encdec_subdir = ''\n",
    "\n",
    "encdec_train_path = TRAIN_ENCDEC_GRAPH_BERT_PATH / encdec_subdir\n",
    "encdec_snapshot_fpath = encdec_train_path / 'best.pth'\n",
    "encdec_graph_model_cfg_fpath = encdec_train_path / ENCDEC_GRAPH_BERT_MODEL_CFG_FNAME\n",
    "\n",
    "device_name = 'cpu'\n",
    "# device_name = 'cuda'\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipeida dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8d5caa220b44bcb8c5957490384930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia 20220301.en docs: 6458670\n"
     ]
    }
   ],
   "source": [
    "# dss = load_dataset('wikipedia', WIKI_DS_NAME, beam_runner='DirectRunner', cache_dir=str(DATA_PATH))\n",
    "dss = load_dataset('wikipedia', WIKI_DS_NAME, cache_dir=str(DATA_PATH), trust_remote_code=True)\n",
    "ds: Dataset = dss['train']\n",
    "n_docs = len(ds)\n",
    "print(f'Wikipedia {WIKI_DS_NAME} docs: {n_docs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkz = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tkz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8351 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS: [CLS], 101. SEP: [SEP], 102. PAD: [PAD]. 0.\n",
      "[0] Anarchism\n",
      "Anarchism is a political philosophy and movement that is sceptical of authority and rejects all involuntary, coercive forms of hierarchy. Anarchism calls for the abolition of the state, which it holds\n",
      "8351 [101, 9617, 11140, 2964, 2003, 1037, 2576, 4695, 1998, 2929] [8780, 21615, 2591, 8106, 14649, 2521, 1011, 2187, 4331, 102]\n",
      "---\n",
      "[10] Academy Awards\n",
      "The Academy Awards, popularly known as the Oscars, are awards for artistic and technical merit in the film industry. They are regarded by many as the most prestigious and significant awards in the ent\n",
      "9434 [101, 1996, 2914, 2982, 1010, 16071, 2124, 2004, 1996, 7436] [3349, 5365, 2381, 1998, 3226, 2137, 2444, 2547, 3065, 102]\n",
      "---\n",
      "[20] Anthropology\n",
      "Anthropology is the scientific study of humanity, concerned with human behavior, human biology, cultures, societies, and linguistics, in both the present and past, including past human species. Social\n",
      "9831 [101, 12795, 2003, 1996, 4045, 2817, 1997, 8438, 1010, 4986] [3145, 9373, 2573, 6327, 6971, 1006, 9932, 2080, 1007, 102]\n",
      "---\n",
      "[30] Austroasiatic languages\n",
      "The Austroasiatic languages , also known as Mon–Khmer , are a large language family in  Mainland Southeast Asia and South Asia. These languages are scattered throughout parts of Thailand, India, Bangl\n",
      "6137 [101, 1996, 16951, 15396, 4588, 4155, 1010, 2036, 2124, 2004] [3512, 4155, 2653, 2945, 19432, 1011, 16951, 20281, 4155, 102]\n",
      "---\n",
      "[40] Ada\n",
      "Ada may refer to:\\n\\nPlaces\\n\\nAfrica\\n Ada Foah or Ada, Ghana, a town\\n Ada (Ghana parliament constituency)\\n Ada, Osun, a town in Osun State, Nigeria\\n\\nAsia\\n Adeh, Urmia, also known as Ada, a village in Wes\n",
      "710 [101, 15262, 2089, 6523, 2000, 1024, 3182, 3088, 15262, 1042] [2011, 1996, 2586, 3212, 2076, 1996, 2137, 2942, 2162, 102]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "inds = [0, 10, 20, 30, 40]\n",
    "ttls = [ds[i]['title'] for i in inds]\n",
    "txts = [ds[i]['text'] for i in inds]\n",
    "batch: BatchEncoding = tkz(txts, padding=False, truncation=False)\n",
    "print(f'CLS: {tkz.cls_token}, {tkz.cls_token_id}. SEP: {tkz.sep_token}, {tkz.sep_token_id}. PAD: {tkz.pad_token}. {tkz.pad_token_id}.')\n",
    "for i in range(len(inds)):\n",
    "    print(f'[{inds[i]}] {ttls[i]}')\n",
    "    print(txts[i][:200].replace('\\n', '\\\\n'))\n",
    "    tok_ids = batch['input_ids'][i]\n",
    "    assert tok_ids[0] == tkz.cls_token_id and tok_ids[-1] == tkz.sep_token_id\n",
    "    print(len(tok_ids), tok_ids[:10], tok_ids[-10:])\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[101, 9617, 11140, 102],\n",
       " [101, 2914, 2982, 102],\n",
       " [101, 12795, 102, 0],\n",
       " [101, 16951, 15396, 102],\n",
       " [101, 15262, 102, 0]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch2 = tkz(ttls, padding=True, truncation=True, max_length=4)\n",
    "print(type(batch2['input_ids']))\n",
    "batch2['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ana', '##rch', '[SEP]']\n",
      "['[CLS]', 'academy', 'awards', '[SEP]']\n",
      "['[CLS]', 'anthropology', '[SEP]', '[PAD]']\n",
      "['[CLS]', 'austro', '##asia', '[SEP]']\n",
      "['[CLS]', 'ada', '[SEP]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "for toks in batch2['input_ids']:\n",
    "    print(tkz.convert_ids_to_tokens(toks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random tokenzied substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch(batch: List[TokensSubset], tkz: PreTrainedTokenizer):\n",
    "    for i, toks_sub in enumerate(batch):\n",
    "        toks_src_str = ' '.join(tkz.convert_ids_to_tokens(toks_sub.toks_src[:200]))\n",
    "        toks_inp_str = ' '.join(tkz.convert_ids_to_tokens(toks_sub.toks_inp))\n",
    "        toks_inp2_str = ' '.join(tkz.convert_ids_to_tokens(toks_sub.toks_src[toks_sub.inp_beg_ind:toks_sub.inp_end_ind]))\n",
    "        print(f'[{i}] src ({len(toks_sub.toks_src)}): {toks_src_str}')\n",
    "        print(f'     inp ({len(toks_sub.toks_inp)}): {toks_inp_str}')\n",
    "        print(f'     inp2({len(toks_inp2_str)}): {toks_inp2_str}')\n",
    "        if toks_sub.toks_cite is not None:\n",
    "            toks_sub_str = ' '.join(tkz.convert_ids_to_tokens(toks_sub.toks_src[toks_sub.cite_beg_ind:toks_sub.cite_end_ind]))\n",
    "            print(f'     cite ({len(toks_sub_str)}): {toks_sub_str}')\n",
    "        print('---')\n",
    "\n",
    "def print_batch_v2(batch: List[TokensSubsetV2], tkz: PreTrainedTokenizer):\n",
    "    for i, toks_sub in enumerate(batch):\n",
    "        toks_src_str = ' '.join(tkz.convert_ids_to_tokens(toks_sub.toks_src[:200]))\n",
    "        toks_inp_str = ' '.join(tkz.convert_ids_to_tokens(toks_sub.toks_inp))\n",
    "        toks_inp2_str = ' '.join(tkz.convert_ids_to_tokens(toks_sub.toks_src[toks_sub.inp_beg_ind:toks_sub.inp_end_ind]))\n",
    "        print(f'[{i}] src ({len(toks_sub.toks_src)}): {toks_src_str}')\n",
    "        print(f'    inp ({len(toks_sub.toks_inp)}): {toks_inp_str}')\n",
    "        print(f'    inp2({len(toks_inp2_str)}): {toks_inp2_str}')\n",
    "\n",
    "        toks_sub_str = ' '.join(tkz.convert_ids_to_tokens(toks_sub.toks_src[toks_sub.cite_beg_ind:toks_sub.cite_end_ind]))\n",
    "        cite_beg = tkz.convert_ids_to_tokens(toks_sub.toks_cite_beg)\n",
    "        cite_end = tkz.convert_ids_to_tokens(toks_sub.toks_cite_end)\n",
    "        cite_beg_str, cite_end_str = ' '.join(cite_beg), ' '.join(cite_end)\n",
    "        print(f'    special tokens begin, end: {toks_sub.toks_cite_beg} = \"{cite_beg_str}\", {toks_sub.toks_cite_end} = \"{cite_end_str}\"')\n",
    "        print(f'    cite ({len(toks_sub_str)}): {toks_sub_str}')\n",
    "        print(f'    prompt ({len(toks_sub.toks_prompt)}): ' + ' '.join(tkz.convert_ids_to_tokens(toks_sub.toks_prompt)))\n",
    "        print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAG_BEG: [1031, 6415, 1035, 11693, 1033], TAG_END: [1031, 6415, 1035, 2203, 1033]\n",
      "TAG_BEG: [1026, 21893, 1028], TAG_END: [1026, 1013, 21893, 1028]\n"
     ]
    }
   ],
   "source": [
    "tag_beg, tag_end = tkz(['[TAG_BEG]', '[TAG_END]'], add_special_tokens=False).input_ids\n",
    "print(f'TAG_BEG: {tag_beg}, TAG_END: {tag_end}')\n",
    "tag_beg, tag_end = tkz(['<cite>', '</cite>'], add_special_tokens=False).input_ids\n",
    "print(f'TAG_BEG: {tag_beg}, TAG_END: {tag_end}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] src (8349): ana ##rch ##ism is a political philosophy and movement that is sc ##ept ##ical of authority and rejects all involuntary , coe ##rc ##ive forms of hierarchy . ana ##rch ##ism calls for the abolition of the state , which it holds to be unnecessary , und ##es ##ira ##ble , and harmful . as a historically left - wing movement , placed on the far ##thest left of the political spectrum , it is usually described alongside communal ##ism and libertarian marxism as the libertarian wing ( libertarian socialism ) of the socialist movement , and has a strong historical association with anti - capitalism and socialism . humans lived in societies without formal hi ##era ##rch ##ies long before the establishment of formal states , realms , or empires . with the rise of organised hierarchical bodies , sc ##ept ##ici ##sm toward authority also rose . although traces of anarchist thought are found throughout history , modern ana ##rch ##ism emerged from the enlightenment . during the latter half of the 19th and the first decades of the 20th century , the anarchist movement flourished in most parts of the world and had a significant role in\n",
      "     inp (20): [CLS] in popularity < cite > and influence within anti - capitalist , anti - < / cite > [SEP]\n",
      "     inp2(61): in popularity and influence within anti - capitalist , anti -\n",
      "     cite (47): and influence within anti - capitalist , anti -\n",
      "---\n",
      "[1] src (9432): the academy awards , popularly known as the oscar ##s , are awards for artistic and technical merit in the film industry . they are regarded by many as the most prestigious and significant awards in the entertainment industry worldwide . given annually by the academy of motion picture arts and sciences ( amp ##as ) , the awards are an international recognition of excellence in cinematic achievements , as assessed by the academy ' s voting membership . the various category winners are awarded a copy of a golden statue ##tte as a trophy , officially called the \" academy award of merit \" , although more commonly referred to by its nickname , the \" oscar \" . the statue ##tte depicts a knight rendered in the art deco style . the award was originally sculpted by george stanley from a design sketch by cedric gibbons . amp ##as first presented it in 1929 at a private dinner hosted by douglas fairbanks in the hollywood roosevelt hotel in what would become known as the 1st academy awards . the academy awards ceremony was first broadcast by radio in 1930 and was televised for the first time in 1953\n",
      "     inp (20): [CLS] films . with the fourth ceremony , however , the system changed , and professionals were honored for [SEP]\n",
      "     inp2(100): films . with the fourth ceremony , however , the system changed , and professionals were honored for\n",
      "---\n",
      "[2] src (9829): anthropology is the scientific study of humanity , concerned with human behavior , human biology , cultures , societies , and linguistics , in both the present and past , including past human species . social anthropology studies patterns of behaviour , while cultural anthropology studies cultural meaning , including norms and values . a port ##man ##tea ##u socio ##cultural anthropology is commonly used today . linguistic anthropology studies how language influences social life . biological or physical anthropology studies the biological development of humans . archaeological anthropology , often termed as ' anthropology of the past ' , studies human activity through investigation of physical evidence . it is considered a branch of anthropology in north america and asia , while in europe archaeology is viewed as a discipline in its own right or grouped under other related disciplines , such as history . etymology the abstract noun anthropology is first attested in reference to history . its present use first appeared in renaissance germany in the works of magnus hu ##ndt and otto cas ##mann . their new latin derived from the combining forms of the greek words ant ##hr ##op ##os ( , \" human \"\n",
      "     inp (20): [CLS] ##hn ##ography , cyber ##ant ##hr ##op < cite > ##ology , and < / cite > virtual [SEP]\n",
      "     inp2(60): ##hn ##ography , cyber ##ant ##hr ##op ##ology , and virtual\n",
      "     cite (13): ##ology , and\n",
      "---\n",
      "[3] src (6135): the austro ##asia ##tic languages , also known as mon – khmer , are a large language family in mainland southeast asia and south asia . these languages are scattered throughout parts of thailand , india , bangladesh , nepal , and southern china . there are around 117 million speakers of austro ##asia ##tic languages . of these languages , only vietnamese , khmer , and mon have a long - established recorded history . only two have official status as modern national languages : vietnamese in vietnam and khmer in cambodia . the mon language is a recognized indigenous language in myanmar and thailand . in myanmar , the wa language is the de facto official language of wa state . santa ##li is one of the 22 scheduled languages of india . the rest of the languages are spoken by minority groups and have no official status . et ##hn ##olo ##gue identifies 168 austro ##asia ##tic languages . these form thirteen established families ( plus perhaps sho ##mp ##en , which is poorly attested , as a fourteenth ) , which have traditionally been grouped into two , as mon – khmer , and mu ##nda\n",
      "     inp (20): [CLS] ##ann ##ica and — except for the breakup of southern mon – khmer — in et ##hn ##olo [SEP]\n",
      "     inp2(83): ##ann ##ica and — except for the breakup of southern mon – khmer — in et ##hn ##olo\n",
      "---\n",
      "[4] src (708): ada may refer to : places africa ada f ##oa ##h or ada , ghana , a town ada ( ghana parliament constituency ) ada , os ##un , a town in os ##un state , nigeria asia ad ##eh , ur ##mia , also known as ada , a village in west azerbaijan province ada , kara ##man , a village in kara ##man province , turkey australia and new zealand ada river ( di ##sam ##bi ##gua ##tion ) , three rivers europe ada , bosnia and herzegovina , a village ada , croatia , a village ada , serbia , a town and municipality ada ci ##gan ##li ##ja or ada , a river island artificial ##ly turned into a peninsula in belgrade , serbia north america united states ada , alabama , an unincorporated community ada county , idaho ada , kansas , an unincorporated community ada township , michigan ada , minnesota , a city ada township , dick ##ey county , north dakota ada , ohio , a village ada , oklahoma , a city ada , oregon , an unincorporated community ada township , perkins county , south dakota ada , west virginia\n",
      "     inp (20): [CLS] , an es ##cher ##ichi ##a coli adaptive response protein ada ##h ( di ##sam ##bi ##gua ##tion [SEP]\n",
      "     inp2(93): , an es ##cher ##ichi ##a coli adaptive response protein ada ##h ( di ##sam ##bi ##gua ##tion\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "rand_tkz = RandomInputTokenizer(tkz, max_len=20)\n",
    "batch = rand_tkz(txts, n_items_to_cite=2)\n",
    "print_batch(batch, tkz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] src (3): ana ##rch ##ism\n",
      "     inp (5): [CLS] ana ##rch ##ism [SEP]\n",
      "     inp2(15): ana ##rch ##ism\n",
      "---\n",
      "[1] src (2): academy awards\n",
      "     inp (11): [CLS] < cite > academy < / cite > awards [SEP]\n",
      "     inp2(14): academy awards\n",
      "     cite (7): academy\n",
      "---\n",
      "[2] src (1): anthropology\n",
      "     inp (3): [CLS] anthropology [SEP]\n",
      "     inp2(12): anthropology\n",
      "---\n",
      "[3] src (4): austro ##asia ##tic languages\n",
      "     inp (12): [CLS] ##asia < cite > ##tic < / cite > languages [SEP]\n",
      "     inp2(22): ##asia ##tic languages\n",
      "     cite (5): ##tic\n",
      "---\n",
      "[4] src (1): ada\n",
      "     inp (10): [CLS] < cite > ada < / cite > [SEP]\n",
      "     inp2(3): ada\n",
      "     cite (3): ada\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  9617, 11140,  2964,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  1026, 21893,  1028,  2914,  1026,  1013, 21893,  1028,  2982,\n",
       "            102,     0],\n",
       "         [  101, 12795,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 15396,  1026, 21893,  1028,  4588,  1026,  1013, 21893,  1028,\n",
       "           4155,   102],\n",
       "         [  101,  1026, 21893,  1028, 15262,  1026,  1013, 21893,  1028,   102,\n",
       "              0,     0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_tkz = RandomInputTokenizer(tkz, max_len=12)\n",
    "batch_ttl = rand_tkz(ttls, n_items_to_cite=3)\n",
    "print_batch(batch_ttl, tkz)\n",
    "\n",
    "inp_toks_t, att_mask_t = tokens_subsets_to_tensors(batch_ttl, pad_token_id=tkz.pad_token_id, device=device)\n",
    "inp_toks_t, att_mask_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random tokenized substring v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tkz))\n",
    "tkz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused985] [unused986] [unused987] [unused988] [unused989] [unused990] [unused991] [unused992] [unused993] ! \" # $ % & \\' ( ) * + , - . / 0 1 2 3 4 5'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 990\n",
    "inds = list(range(i, i + 30))\n",
    "' '.join(tkz.convert_ids_to_tokens(inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] src (3): ana ##rch ##ism\n",
      "    inp (11): [CLS] ##vance ##ₖ remarried ana ##rch ##ism ##ets email originated [SEP]\n",
      "    inp2(15): ana ##rch ##ism\n",
      "    special tokens begin, end: [21789, 30094, 19316] = \"##vance ##ₖ remarried\", [8454, 10373, 7940] = \"##ets email originated\"\n",
      "    cite (15): ana ##rch ##ism\n",
      "    prompt (36): [CLS] cite tag begin : \" # # vance # # ₖ remarried \" . cite tag end : \" # # et ##s email originated \" . produce output text between these tags . [SEP]\n",
      "---\n",
      "[1] src (2): academy awards\n",
      "    inp (10): [CLS] mac ##nt tides academy awards technically ##kon contemporaries [SEP]\n",
      "    inp2(14): academy awards\n",
      "    special tokens begin, end: [6097, 3372, 22487] = \"mac ##nt tides\", [10892, 19648, 16682] = \"technically ##kon contemporaries\"\n",
      "    cite (14): academy awards\n",
      "    prompt (34): [CLS] cite tag begin : \" mac # # nt tides \" . cite tag end : \" technically # # ko ##n contemporaries \" . produce output text between these tags . [SEP]\n",
      "---\n",
      "[2] src (1): anthropology\n",
      "    inp (9): [CLS] fiscal tchaikovsky judged anthropology collins motioned overture [SEP]\n",
      "    inp2(12): anthropology\n",
      "    special tokens begin, end: [10807, 25900, 13224] = \"fiscal tchaikovsky judged\", [6868, 13054, 25052] = \"collins motioned overture\"\n",
      "    cite (12): anthropology\n",
      "    prompt (29): [CLS] cite tag begin : \" fiscal tchaikovsky judged \" . cite tag end : \" collins motioned overture \" . produce output text between these tags . [SEP]\n",
      "---\n",
      "[3] src (4): austro ##asia ##tic languages\n",
      "    inp (12): [CLS] peshawar ##icio firstly austro ##asia ##tic languages bengal bourbon ba [SEP]\n",
      "    inp2(29): austro ##asia ##tic languages\n",
      "    special tokens begin, end: [28777, 27113, 15847] = \"peshawar ##icio firstly\", [8191, 15477, 8670] = \"bengal bourbon ba\"\n",
      "    cite (29): austro ##asia ##tic languages\n",
      "    prompt (32): [CLS] cite tag begin : \" peshawar # # ic ##io firstly \" . cite tag end : \" bengal bourbon ba \" . produce output text between these tags . [SEP]\n",
      "---\n",
      "[4] src (1): ada\n",
      "    inp (9): [CLS] bai becomes frantically ada temperance dale ##nda [SEP]\n",
      "    inp2(3): ada\n",
      "    special tokens begin, end: [21790, 4150, 16460] = \"bai becomes frantically\", [23372, 8512, 8943] = \"temperance dale ##nda\"\n",
      "    cite (3): ada\n",
      "    prompt (32): [CLS] cite tag begin : \" bai becomes frantically \" . cite tag end : \" temperance dale # # n ##da \" . produce output text between these tags . [SEP]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "rand_tkz = RandomInputTokenizerV2(tkz, max_len=30, n_random_toks=3)\n",
    "batch_ttl = rand_tkz(ttls)\n",
    "print_batch_v2(batch_ttl, tkz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101, 21789, 30094, 19316,  9617, 11140,  2964,  8454, 10373,  7940,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,  6097,  3372, 22487,  2914,  2982, 10892, 19648, 16682,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101, 10807, 25900, 13224, 12795,  6868, 13054, 25052,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101, 28777, 27113, 15847, 16951, 15396,  4588,  4155,  8191, 15477,\n",
       "           8670,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101, 21790,  4150, 16460, 15262, 23372,  8512,  8943,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101, 21893,  6415,  4088,  1024,  1000,  1001,  1001, 16672,  1001,\n",
       "           1001,  1565, 19316,  1000,  1012, 21893,  6415,  2203,  1024,  1000,\n",
       "           1001,  1001,  3802,  2015, 10373,  7940,  1000,  1012,  3965,  6434,\n",
       "           3793,  2090,  2122, 22073,  1012,   102],\n",
       "         [  101, 21893,  6415,  4088,  1024,  1000,  6097,  1001,  1001, 23961,\n",
       "          22487,  1000,  1012, 21893,  6415,  2203,  1024,  1000, 10892,  1001,\n",
       "           1001, 12849,  2078, 16682,  1000,  1012,  3965,  6434,  3793,  2090,\n",
       "           2122, 22073,  1012,   102,     0,     0],\n",
       "         [  101, 21893,  6415,  4088,  1024,  1000, 10807, 25900, 13224,  1000,\n",
       "           1012, 21893,  6415,  2203,  1024,  1000,  6868, 13054, 25052,  1000,\n",
       "           1012,  3965,  6434,  3793,  2090,  2122, 22073,  1012,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101, 21893,  6415,  4088,  1024,  1000, 28777,  1001,  1001, 24582,\n",
       "           3695, 15847,  1000,  1012, 21893,  6415,  2203,  1024,  1000,  8191,\n",
       "          15477,  8670,  1000,  1012,  3965,  6434,  3793,  2090,  2122, 22073,\n",
       "           1012,   102,     0,     0,     0,     0],\n",
       "         [  101, 21893,  6415,  4088,  1024,  1000, 21790,  4150, 16460,  1000,\n",
       "           1012, 21893,  6415,  2203,  1024,  1000, 23372,  8512,  1001,  1001,\n",
       "           1050,  2850,  1000,  1012,  3965,  6434,  3793,  2090,  2122, 22073,\n",
       "           1012,   102,     0,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]),\n",
       " tensor([[0, 1, 2, 3, 4],\n",
       "         [5, 5, 5, 5, 5]]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_toks_t, att_mask_t, edge_inds_t = tokens_subsets_v2_to_tensors(batch_ttl, tkz=tkz, device=device)\n",
    "inp_toks_t, att_mask_t, edge_inds_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_bert=EncBertCfg(inp_len=128, d_model=768, pad_token_id=0, pretrained_model_name='bert-base-uncased', tokenizer_name='bert-base-uncased', emb_type=<BertEmbType.Cls: 'cls'>, emb2_tok_name='') dec_pyr=DecPyrCfg(d_model=768, n_heads=12, d_k=64, d_v=64, d_inner=3072, inp_len=128, step=2, n_layers=7, dropout_rate=0.0, n_vocab=30522, n_similar_layers=1, enhance_type=<HgEnhanceType.MatmulBeginBias: 'mmbb'>, temperature=0.0)\n",
      "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_cfg = parse_yaml_file_as(EncdecGraphBertCfg, encdec_graph_model_cfg_fpath)\n",
    "tkz = AutoTokenizer.from_pretrained(model_cfg.enc_bert.pretrained_model_name)\n",
    "print(model_cfg)\n",
    "print(tkz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt = torch.load(encdec_snapshot_fpath, map_location=device)\n",
    "model = EncdecGraphBert(model_cfg).to(device)\n",
    "strict = True\n",
    "# strict = False\n",
    "model.load_state_dict(chkpt['model'], strict=strict)\n",
    "del chkpt\n",
    "model.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp_len: 128\n"
     ]
    }
   ],
   "source": [
    "inp_len = model_cfg.enc_bert.inp_len\n",
    "print('inp_len:', inp_len)\n",
    "\n",
    "def get_batch_tokens(doc_inds: list[int], randomize: bool = False, mask_toks_len_max: int = 5) -> torch.Tensor:\n",
    "    docs_toks = np.full((len(doc_inds), inp_len), tkz.pad_token_id)\n",
    "    for i, doc_ind in enumerate(doc_inds):\n",
    "        doc = ds[doc_ind]\n",
    "        title, text = doc['title'], doc['text']\n",
    "        doc_txt = f'{title} {text}'\n",
    "        doc_txt = text\n",
    "        doc_txt = doc_txt.lower()\n",
    "        doc_toks: list[int] | np.ndarray = tkz(doc_txt)['input_ids']\n",
    "        n_toks = len(doc_toks)\n",
    "        if n_toks > inp_len:\n",
    "            i_off = np.random.randint(1, n_toks - inp_len + 1) if randomize else 1\n",
    "            doc_toks = np.concatenate([doc_toks[:1], doc_toks[i_off:i_off + inp_len - 2], doc_toks[-1:]])\n",
    "            # print(doc_toks)\n",
    "\n",
    "        mask_toks_len = np.random.randint(1, mask_toks_len_max + 1)\n",
    "        mask_toks_off = np.random.randint(len(doc_toks) - mask_toks_len + 1)\n",
    "        masked_toks = doc_toks[mask_toks_off:mask_toks_off + mask_toks_len]\n",
    "        masked_substr = tkz.decode(masked_toks)\n",
    "        print(f'{doc_ind:03d}. {masked_substr}')\n",
    "        doc_toks[mask_toks_off:mask_toks_off + mask_toks_len] = tkz.mask_token_id\n",
    "\n",
    "        docs_toks[i, :len(doc_toks)] = doc_toks\n",
    "    docs_toks_t = torch.from_numpy(docs_toks).to(device)\n",
    "    return docs_toks_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 \"Anarchism\" Anarchism is a political philosophy and movement that is sceptical of authority and rejects all involuntary, coercive forms of hierarchy. Anarchism calls for the abolition of the state, which it holds to be unnecessary, undesirable, and harmful. As a historically left-wing movement, placed on the farthest left of the political spectrum, it is usually described alongside communalism and libertarian\n",
      "001 \"Autism\" Autism is a neurodevelopmental disorder characterized by difficulties with social interaction and communication, and by restricted and repetitive behavior. Parents often notice signs during the first three years of their child's life. These signs often develop gradually, though some autistic children experience regression in their communication and social skills after reaching developmental milest\n",
      "002 \"Albedo\" Albedo (; ) is the measure of the diffuse reflection of solar radiation out of the total solar radiation and measured on a scale from 0, corresponding to a black body that absorbs all incident radiation, to 1, corresponding to a body that reflects all incident radiation.\\n\\nSurface albedo is defined as the ratio of radiosity Je to the irradiance Ee (flux per unit area) received by a surface. The p\n",
      "003 \"A\" A, or a, is the first letter and the first vowel of the modern English alphabet and the ISO basic Latin alphabet. Its name in English is a (pronounced ), plural aes. It is similar in shape to the Ancient Greek letter alpha, from which it derives. The uppercase version consists of the two slanting sides of a triangle, crossed in the middle by a horizontal bar. The lowercase version can be written i\n",
      "004 \"Alabama\" Alabama () is a state in the Southeastern region of the United States, bordered by Tennessee to the north; Georgia to the east; Florida and the Gulf of Mexico to the south; and Mississippi to the west. Alabama is the 30th largest by area and the 24th-most populous of the U.S. states. With a total of  of inland waterways, Alabama has among the most of any state.\\n\\nAlabama is nicknamed the Yellowha\n"
     ]
    }
   ],
   "source": [
    "doc_inds = np.arange(5)\n",
    "# doc_inds += 5\n",
    "doc_inds = [x.item() for x in doc_inds]\n",
    "for doc_ind in doc_inds:\n",
    "    doc = ds[doc_ind]\n",
    "    title, text = doc['title'], doc['text'].replace('\\n', '\\\\n')\n",
    "    print(f'{doc_ind:03d} \"{title}\" {text[:400]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000. formal hierarch\n",
      "001. first\n",
      "002. albedo (\n",
      "003. written\n",
      "004. yellowhammer state,\n",
      "tensor([  101,  9617, 11140,  2964,  2003,  1037,  2576,  4695,  1998,  2929,\n",
      "         2008,  2003,  8040, 23606,  7476,  1997,  3691,  1998, 19164,  2035,\n",
      "        26097,  1010, 24873, 11890,  3512,  3596,  1997, 12571,  1012,  9617,\n",
      "        11140,  2964,  4455,  2005,  1996, 15766,  1997,  1996,  2110,  1010,\n",
      "         2029,  2009,  4324,  2000,  2022, 14203,  1010,  6151,  2229,  7895,\n",
      "         3468,  1010,  1998, 17631,  1012,  2004,  1037,  7145,  2187,  1011,\n",
      "         3358,  2929,  1010,  2872,  2006,  1996,  2521, 20515,  2187,  1997,\n",
      "         1996,  2576,  8674,  1010,  2009,  2003,  2788,  2649,  4077, 15029,\n",
      "         2964,  1998, 19297, 27255,  2004,  1996, 19297,  3358,  1006, 19297,\n",
      "        14649,  1007,  1997,  1996,  6102,  2929,  1010,  1998,  2038,  1037,\n",
      "         2844,  3439,  2523,  2007,  3424,  1011, 16498,  1998, 14649,  1012,\n",
      "         4286,  2973,  1999,  8384,  2302,   103,   103,   103,   103,  3111,\n",
      "         2146,  2077,  1996,  5069,  1997,  5337,  2163,   102])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([5, 128, 30522])\n",
      "torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "docs_toks_in = get_batch_tokens(doc_inds)\n",
    "print(docs_toks_in[0])\n",
    "print(docs_toks_in.shape)\n",
    "logits_pred = model(docs_toks_in, docs_toks_in != tkz.pad_token_id)\n",
    "probs_pred = torch.softmax(logits_pred, dim=-1)\n",
    "# probs_pred = torch.sigmoid(logits_pred)\n",
    "print(probs_pred.shape)\n",
    "docs_toks_out = torch.argmax(probs_pred, dim=-1)\n",
    "print(docs_toks_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 [CLS] anarchism is a political philosophy and movement that is sceptical of authority and rejects all involuntary, coercive forms of hierarchy. anarchism calls for the abolition of the state, which it holds to be unnecessary, undesirable, and harmful. as a historically left - wing movement, placed on the farthest left of the political spectrum, it is usually described alongside communalism and libertarian marxism as the libertarian wing ( libertarian socialism ) of the socialist movement, and has a strong historical association with anti - capitalism and socialism. humans lived in societies without [MASK] [MASK] [MASK] [MASK]ies long before the establishment of formal states [SEP]\n",
      "001 [CLS] autism is a neurodevelopmental disorder characterized by difficulties with social interaction and communication, and by restricted and repetitive behavior. parents often notice signs during the [MASK] three years of their child ' s life. these signs often develop gradually, though some autistic children experience regression in their communication and social skills after reaching developmental milestones at a normal pace. autism is associated with a combination of genetic and environmental factors. risk factors during pregnancy include certain infections, such as rubella, toxins including valproic acid, alcohol, cocaine, pesticides, lead, and air pollution, fetal growth restriction, and autoimmun [SEP]\n",
      "002 [CLS] [MASK] [MASK] [MASK] ; ) is the measure of the diffuse reflection of solar radiation out of the total solar radiation and measured on a scale from 0, corresponding to a black body that absorbs all incident radiation, to 1, corresponding to a body that reflects all incident radiation. surface albedo is defined as the ratio of radiosity je to the irradiance ee ( flux per unit area ) received by a surface. the proportion reflected is not only determined by properties of the surface itself, but also by the spectral and angular distribution of solar radiation reaching the earth ' s surface. these factors vary with atmospheric composition, geographic location, and [SEP]\n",
      "003 [CLS] a, or a, is the first letter and the first vowel of the modern english alphabet and the iso basic latin alphabet. its name in english is a ( pronounced ), plural aes. it is similar in shape to the ancient greek letter alpha, from which it derives. the uppercase version consists of the two slanting sides of a triangle, crossed in the middle by a horizontal bar. the lowercase version can be [MASK] in two forms : the double - storey a and single - storey ɑ. the latter is commonly used in handwriting and fonts based on it, especially fonts intended to be read by children, [SEP]\n",
      "004 [CLS] alabama ( ) is a state in the southeastern region of the united states, bordered by tennessee to the north ; georgia to the east ; florida and the gulf of mexico to the south ; and mississippi to the west. alabama is the 30th largest by area and the 24th - most populous of the u. s. states. with a total of of inland waterways, alabama has among the most of any state. alabama is nicknamed the [MASK] [MASK] [MASK] [MASK] after the state bird. alabama is also known as the \" heart of dixie \" and the \" cotton state \". the state tree is the longleaf pine, and the state flower is [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, doc_ind in enumerate(doc_inds):\n",
    "    s = tkz.decode(docs_toks_in[i])\n",
    "    s = s.replace('\\n', '\\\\n')\n",
    "    print(f'{doc_ind:03d} {s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 the, andrel [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "001 past two [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "002 ( ( ( [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "003 used in [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "004 \" of \" named [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "for i, doc_ind in enumerate(doc_inds):\n",
    "    s = tkz.decode(docs_toks_out[i])\n",
    "    s = s.replace('\\n', '\\\\n')\n",
    "    print(f'{doc_ind:03d} {s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder embedding evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(txts: list[str]) -> torch.Tensor:\n",
    "    batch_toks = np.full((len(txts), inp_len), tkz.pad_token_id)\n",
    "    for i, txt in enumerate(txts):\n",
    "        toks: list[int] = tkz(txt)['input_ids']\n",
    "        n_toks = len(toks)\n",
    "        if n_toks > inp_len:\n",
    "            i_off = np.random.randint(n_toks - inp_len + 1)\n",
    "            toks = toks[i_off:i_off + inp_len]\n",
    "        batch_toks[i, :len(toks)] = toks\n",
    "    batch_toks_t = torch.from_numpy(batch_toks).to(device)\n",
    "    return batch_toks_t\n",
    "\n",
    "model.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 768])\n"
     ]
    }
   ],
   "source": [
    "txts = [\n",
    "    '\"Orana Australia Ltd\" Orana Australia Ltd is a not-for-profit organisation that provides a diverse range of training and support services to over 650 people with disabilities and their families in South Australia.\\n\\nHistory\\nThe Mentally Retarded Children’s Society of SA Inc. was established in 1950 by a group of parent',\n",
    "    'Australia',\n",
    "    'Orana Australia Ltd',\n",
    "    'Hello Kitty',\n",
    "]\n",
    "batch_toks = get_tokens(txts)\n",
    "embs = model.enc_bert(batch_toks, batch_toks != tkz.pad_token_id)\n",
    "# embs = embs.detach().cpu().numpy()\n",
    "embs = embs.detach().cpu()\n",
    "print(embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia [0.1348315] tensor(9.7678)\n",
      "Orana Australia Ltd [0.14778207] tensor(10.4330)\n",
      "Hello Kitty [0.08750954] tensor(10.7561)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(embs)):\n",
    "    cos_dist = F.cosine_similarity(embs[0:1], embs[i:i + 1])\n",
    "    norm_dist = torch.norm(embs[0] - embs[i])\n",
    "    print(txts[i], cos_dist.numpy(), norm_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = 'Context3. Like other American research universities, Northwestern was transformed by World War II. Franklyn B. Snyder led the university from 1939 to 1949, when nearly 50,000 military officers and personnel were trained on the Evanston and Chicago campuses. After the war, surging enrollments under the G.I. Bill drove drastic expansion of both campuses. In 1948 prominent anthropologist Melville J. '\n",
    "q = 'Question: Between 1939 and 1949, how many military officers and personnel were trained on the Evanston and Chicago campuses? Answer: [MASK] [MASK] [MASK] [MASK]'\n",
    "s = f'{ctx} {q}'\n",
    "toks = tkz(s).input_ids\n",
    "len(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 30522])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  101,  6123,  2509,  1012,  2066,  2060,  2137,  2470,  5534,  1010,\n",
       "         7855,  2001,  8590,  2011,  2088,  2162,  2462,  1012, 19597,  2078,\n",
       "         1038,  1012, 17840,  2419,  1996,  2118,  2013,  3912,  2000,  4085,\n",
       "         1010,  2043,  3053,  2753,  1010,  2199,  2510,  3738,  1998,  5073,\n",
       "         2020,  4738,  2006,  1996,  6473,  2669,  1998,  3190, 13696,  1012,\n",
       "         2044,  1996,  2162,  1010,  7505,  4726, 10316,  2015,  2104,  1996,\n",
       "         1043,  1012,  1045,  1012,  3021,  6303, 20851,  4935,  1997,  2119,\n",
       "        13696,  1012,  1999,  3882,  4069, 21571, 20154,  1046,  1012,  3160,\n",
       "         1024,  2090,  3912,  1998,  4085,  1010,  2129,  2116,  2510,  3738,\n",
       "         1998,  5073,  2020,  4738,  2006,  1996,  6473,  2669,  1998,  3190,\n",
       "        13696,  1029,  3437,  1024,  1000,  1024,  2137,  2015,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks_in = torch.tensor(toks).to(device).unsqueeze(0)\n",
    "logits_pred = model(toks_in, toks_in != tkz.pad_token_id)\n",
    "print(logits_pred.shape)\n",
    "probs_pred = torch.softmax(logits_pred[0], dim=-1)\n",
    "toks_out = torch.argmax(probs_pred, dim=-1)\n",
    "toks_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] context3. like other american research universities, northwestern was transformed by world war ii. franklyn b. snyder led the university from 1939 to 1949, when nearly 50, 000 military officers and personnel were trained on the evanston and chicago campuses. after the war, surging enrollments under the g. i. bill demanded ample expansion of both campuses. in 1948 prominent anthropologist melville j. question : between 1939 and 1949, how many military officers and personnel were trained on the evanston and chicago campuses? answer : \" : americans [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkz.decode(toks_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.1139e-12, 6.2658e-13, 1.4770e-12, 1.5230e-12, 1.4352e-12,\n",
       "        1.0220e-12, 1.1165e-12, 1.0452e-12, 1.1000e-12],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 128, 30522])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6792, 0.0907, 0.6140, 0.9981],\n",
       "         [0.2409, 0.8605, 0.0702, 0.2925],\n",
       "         [0.1105, 0.1167, 0.4092, 0.3401]],\n",
       "\n",
       "        [[0.3772, 0.2479, 0.2412, 0.0046],\n",
       "         [0.8316, 0.8350, 0.4125, 0.5759],\n",
       "         [0.0096, 0.6448, 0.9033, 0.0755]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(2, 3, 4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2586, 0.1435, 0.2422, 0.3557],\n",
      "         [0.2103, 0.3908, 0.1773, 0.2215],\n",
      "         [0.2168, 0.2182, 0.2923, 0.2728]],\n",
      "\n",
      "        [[0.2906, 0.2554, 0.2537, 0.2002],\n",
      "         [0.2911, 0.2921, 0.1914, 0.2254],\n",
      "         [0.1563, 0.2949, 0.3819, 0.1669]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.softmax(t, dim=2)\n",
    "print(t1)\n",
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [1],\n",
       "         [3]],\n",
       "\n",
       "        [[2],\n",
       "         [2],\n",
       "         [1]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.randint(0, 4, (2, 3, 1))\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1435],\n",
       "         [0.3908],\n",
       "         [0.2728]],\n",
       "\n",
       "        [[0.2537],\n",
       "         [0.1914],\n",
       "         [0.2949]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(t1, dim=2, index=t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
