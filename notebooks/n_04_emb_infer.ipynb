{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Optional\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pydantic_yaml import parse_yaml_file_as\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, AddedToken, PreTrainedTokenizer\n",
    "\n",
    "from mllm.data.wiki.dswiki import WikiDsLoader\n",
    "from mllm.exp.args import TOKENIZER_CFG_FNAME, ENCDEC_MODEL_CFG_FNAME, RANKER_MODEL_CFG_FNAME\n",
    "from mllm.model.mllm_encdec import MllmEncdecLevel\n",
    "from mllm.model.mllm_ranker import MllmRankerLevel\n",
    "from mllm.config.model import TokenizerCfg, MllmEncdecCfg, MllmRankerCfg\n",
    "from mllm.tokenization.chunk_tokenizer import calc_max_inp_size, gen_all_tokens, ChunkTokenizer, tokenizer_from_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(os.path.expandvars('$HOME')) / 'data'\n",
    "DS_DIR_PATH = DATA_PATH / 'wiki_20200501_en' / 'ch_100_fixed'\n",
    "\n",
    "# TRAIN_ENCDEC_PATH = DATA_PATH / 'train_mllm_encdec'\n",
    "# TRAIN_RANKER_PATH = DATA_PATH / 'train_mllm_ranker_qrels'\n",
    "# encdec_subdir = 'encdec-20240718_221554-wiki_20200501_en-ch_100_fixed'\n",
    "# ranker_subdir = 'ranker-20240903_215749-msmarco-fever'\n",
    "\n",
    "TRAIN_ENCDEC_PATH = DATA_PATH / 'train_mllm_encdec_0'\n",
    "TRAIN_RANKER_PATH = DATA_PATH / 'train_mllm_ranker_qrels_0'\n",
    "encdec_subdir = 'encdec-20241018_092135-wiki_20200501_en-ch_100_fixed'\n",
    "ranker_subdir = 'ranker-20241021_062053-msmarco-fever'\n",
    "\n",
    "encdec_train_path = TRAIN_ENCDEC_PATH / encdec_subdir\n",
    "ranker_train_path = TRAIN_RANKER_PATH / ranker_subdir\n",
    "encdec_snapshot_fpath = encdec_train_path / 'best.pth'\n",
    "ranker_snapshot_fpath = ranker_train_path / 'best.pth'\n",
    "encdec_tkz_cfg_fpath = encdec_train_path / TOKENIZER_CFG_FNAME\n",
    "ranker_tkz_cfg_fpath = ranker_train_path / TOKENIZER_CFG_FNAME\n",
    "encdec_model_cfg_fpath = encdec_train_path / ENCDEC_MODEL_CFG_FNAME\n",
    "ranker_model_cfg_fpath = ranker_train_path / RANKER_MODEL_CFG_FNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "docs_batch_size = 5\n",
    "max_chunks_per_doc = 3\n",
    "model_level = 0\n",
    "device = 'cpu'\n",
    "# device = 'cuda'\n",
    "\n",
    "device = torch.device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encdec_tkz_cfg = parse_yaml_file_as(TokenizerCfg, encdec_tkz_cfg_fpath)\n",
    "ranker_tkz_cfg = parse_yaml_file_as(TokenizerCfg, ranker_tkz_cfg_fpath)\n",
    "assert encdec_tkz_cfg == ranker_tkz_cfg\n",
    "tokenizer = tokenizer_from_config(encdec_tkz_cfg)\n",
    "tok_dict = encdec_tkz_cfg.custom_tokens\n",
    "pad_tok, qbeg_tok, qend_tok = tok_dict['pad'].ind, tok_dict['query_begin'].ind, tok_dict['query_end'].ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cache from /home/misha/data/wiki_20200501_en/ch_100_fixed/.mllm/ds.csv\n",
      "Loaded dataset size: 50989207\n"
     ]
    }
   ],
   "source": [
    "ds_loader = WikiDsLoader(\n",
    "    ds_path=DS_DIR_PATH, docs_batch_size=docs_batch_size, max_chunks_per_doc=max_chunks_per_doc,\n",
    "    pad_tok=pad_tok, qbeg_tok=qbeg_tok, qend_tok=qend_tok, device=device,\n",
    ")\n",
    "ds_loader.shuffle(train=True)\n",
    "ds_loader.shuffle(train=False)\n",
    "inp_len = ds_loader.emb_chunk_size if ds_loader.fixed_size else calc_max_inp_size(ds_loader.emb_chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokten_to_text(tokens: torch.Tensor) -> str:\n",
    "    tokens = tokens.flatten()\n",
    "    tokens = tokens[tokens != pad_tok]\n",
    "    tokens = list(tokens)\n",
    "    s = tokenizer.decode(tokens)\n",
    "    return s\n",
    "\n",
    "def distance(x: np.ndarray, y: np.ndarray, cosine: bool = False):\n",
    "    if not cosine:\n",
    "        return np.linalg.norm(x - y)\n",
    "    x_norm, y_norm = np.linalg.norm(x), np.linalg.norm(y)\n",
    "    return np.sum(x * y) / (x_norm * y_norm)\n",
    "\n",
    "def text_to_tokens(s: str, qbeg_tok: Optional[int] = None, qend_tok: Optional[int] = None) -> torch.Tensor:\n",
    "    tokens = tokenizer(s)['input_ids']\n",
    "    if qbeg_tok is not None:\n",
    "        assert qend_tok is not None\n",
    "        tokens = [qbeg_tok, *tokens, qend_tok]\n",
    "    n_tokens = len(tokens)\n",
    "    n_padded = n_tokens // inp_len + (n_tokens % inp_len > 0)\n",
    "    res = np.full((n_padded * inp_len, ), pad_tok, dtype=np.int32)\n",
    "    res[:n_tokens] = tokens\n",
    "    res = torch.from_numpy(res).to(device)\n",
    "    res = res.reshape(n_padded, inp_len)\n",
    "    return res\n",
    "\n",
    "def print_dist(target_embs: torch.Tensor, docs_embs: torch.Tensor, target_mask: torch.Tensor, cosine: bool = True):\n",
    "    for i, docs_emb in enumerate(docs_embs.detach().numpy()):\n",
    "        for target_emb in target_embs.detach().numpy():\n",
    "            dist = distance(target_emb, docs_emb, cosine)\n",
    "            print(f'{dist:0.6f} ', end='')\n",
    "        sfx = 'T' if target_mask[i] else 'F'\n",
    "        print(sfx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_encoder.src_word_emb.weight (50271, 256) -0.010897173 -3.2294865e-06 0.010897173\n",
      "vocab_encoder.layer_norm.weight (256,) -0.09595125 0.00065567554 0.09957045\n",
      "vocab_encoder.layer_norm.bias (256,) -0.09988814 0.0075067556 0.09993924\n",
      "vocab_decoder.word_prj.weight (50271, 256) -0.010897174 3.4891085e-07 0.010897171\n",
      "encoder.a_em () 0.04009992 0.04009992 0.04009992\n",
      "encoder.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.10824596 -0.00040247082 0.108248524\n",
      "encoder.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.10824942 0.00023723408 0.10825191\n",
      "encoder.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.108234644 0.00016568921 0.10825205\n",
      "encoder.layer_stack.0.slf_attn.fc.weight (256, 256) -0.108250834 -0.00017125583 0.1082469\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.098759234 0.0039213006 0.09965485\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09977838 0.00050682866 0.09982135\n",
      "encoder.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.068464346 -5.541658e-05 0.068465315\n",
      "encoder.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.09967045 0.0015592468 0.09987761\n",
      "encoder.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.06846501 5.350102e-05 0.06846529\n",
      "encoder.layer_stack.0.pos_ffn.w_2.bias (256,) -0.09973276 0.0026412127 0.097577535\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.0997826 0.0023268517 0.098943606\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.09930145 0.0017242811 0.0999163\n",
      "encoder.layer_stack.1.slf_attn.w_qs.weight (256, 256) -0.10825157 -0.00016441016 0.10824294\n",
      "encoder.layer_stack.1.slf_attn.w_ks.weight (256, 256) -0.10825305 -0.00027228403 0.108247794\n",
      "encoder.layer_stack.1.slf_attn.w_vs.weight (256, 256) -0.10824356 -2.1523236e-05 0.108250216\n",
      "encoder.layer_stack.1.slf_attn.fc.weight (256, 256) -0.108249806 0.00027677655 0.108250424\n",
      "encoder.layer_stack.1.slf_attn.layer_norm.weight (256,) -0.09985515 -0.0034155287 0.099598646\n",
      "encoder.layer_stack.1.slf_attn.layer_norm.bias (256,) -0.09877771 -0.0031265446 0.09987559\n",
      "encoder.layer_stack.1.pos_ffn.w_1.weight (1024, 256) -0.06846435 6.9049114e-05 0.068465285\n",
      "encoder.layer_stack.1.pos_ffn.w_1.bias (1024,) -0.09975125 0.0014894841 0.0998804\n",
      "encoder.layer_stack.1.pos_ffn.w_2.weight (256, 1024) -0.06846527 0.00010577513 0.06846529\n",
      "encoder.layer_stack.1.pos_ffn.w_2.bias (256,) -0.099999726 -0.0021182522 0.09776324\n",
      "encoder.layer_stack.1.pos_ffn.layer_norm.weight (256,) -0.099071026 0.0006723383 0.099889435\n",
      "encoder.layer_stack.1.pos_ffn.layer_norm.bias (256,) -0.09983803 0.0015150751 0.099428415\n",
      "encoder.layer_norm.weight (256,) -0.09787573 0.0008713245 0.099277094\n",
      "encoder.layer_norm.bias (256,) -0.09979381 -0.000514633 0.099367015\n",
      "decoder.A_emb2sec (100, 256, 256) -0.008113916 2.2289435e-06 0.008113914\n",
      "decoder.att_layers.0.slf_attn.w_qs.weight (256, 256) -0.10824641 -0.00024599265 0.10824973\n",
      "decoder.att_layers.0.slf_attn.w_ks.weight (256, 256) -0.10823979 0.00027200065 0.10825298\n",
      "decoder.att_layers.0.slf_attn.w_vs.weight (256, 256) -0.1082479 -0.0004418707 0.10825081\n",
      "decoder.att_layers.0.slf_attn.fc.weight (256, 256) -0.108248696 9.401424e-05 0.10825254\n",
      "decoder.att_layers.0.slf_attn.layer_norm.weight (256,) -0.09880506 0.004760201 0.09904261\n",
      "decoder.att_layers.0.slf_attn.layer_norm.bias (256,) -0.09952521 -0.0010836876 0.09959564\n",
      "decoder.att_layers.0.pos_ffn.w_1.weight (1024, 256) -0.06846526 -2.3305374e-06 0.06846447\n",
      "decoder.att_layers.0.pos_ffn.w_1.bias (1024,) -0.09989085 -0.001949843 0.099566355\n",
      "decoder.att_layers.0.pos_ffn.w_2.weight (256, 1024) -0.068465054 -2.2512622e-05 0.06846513\n",
      "decoder.att_layers.0.pos_ffn.w_2.bias (256,) -0.09955772 0.002972584 0.09950676\n",
      "decoder.att_layers.0.pos_ffn.layer_norm.weight (256,) -0.0994818 0.0020508617 0.09928824\n",
      "decoder.att_layers.0.pos_ffn.layer_norm.bias (256,) -0.09997292 0.0024100018 0.09974624\n",
      "decoder.att_layers.1.slf_attn.w_qs.weight (256, 256) -0.10824988 -1.2460776e-05 0.10824142\n",
      "decoder.att_layers.1.slf_attn.w_ks.weight (256, 256) -0.10825028 1.6176356e-05 0.10824969\n",
      "decoder.att_layers.1.slf_attn.w_vs.weight (256, 256) -0.10824411 0.00019446076 0.10824094\n",
      "decoder.att_layers.1.slf_attn.fc.weight (256, 256) -0.10825132 0.0004841385 0.108251676\n",
      "decoder.att_layers.1.slf_attn.layer_norm.weight (256,) -0.09966078 0.00039248704 0.09994725\n",
      "decoder.att_layers.1.slf_attn.layer_norm.bias (256,) -0.09941846 -4.551583e-05 0.097504936\n",
      "decoder.att_layers.1.pos_ffn.w_1.weight (1024, 256) -0.068465166 6.018493e-05 0.06846488\n",
      "decoder.att_layers.1.pos_ffn.w_1.bias (1024,) -0.09986526 0.00064931693 0.09975275\n",
      "decoder.att_layers.1.pos_ffn.w_2.weight (256, 1024) -0.06846409 3.9377396e-06 0.068465136\n",
      "decoder.att_layers.1.pos_ffn.w_2.bias (256,) -0.09995296 0.0031436814 0.09990599\n",
      "decoder.att_layers.1.pos_ffn.layer_norm.weight (256,) -0.09961914 0.00056469755 0.099871926\n",
      "decoder.att_layers.1.pos_ffn.layer_norm.bias (256,) -0.09932623 0.004500539 0.09933748\n"
     ]
    }
   ],
   "source": [
    "model_encdec_cfg = parse_yaml_file_as(MllmEncdecCfg, encdec_model_cfg_fpath)\n",
    "model_encdec = MllmEncdecLevel(model_encdec_cfg, model_level).to(device)\n",
    "checkpoint_encdec = torch.load(encdec_snapshot_fpath)\n",
    "model_encdec.load_state_dict(checkpoint_encdec['model'], strict=False)\n",
    "model_encdec.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15496,   612, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_tokens('Hello there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([18, 100]),\n",
       " torch.Size([2, 100]),\n",
       " tensor([ True,  True, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 10\n",
    "batch = ds_loader.get_batch(i, train=True)\n",
    "docs_chunks, target_chunks, target_mask = batch.gen_tensors()\n",
    "docs_chunks.shape, target_chunks.shape, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|query_begin|> Global CosmopolitansGlobal Cosmopolitans refers to \"a talented population of highly educated multilingual people that have lived, worked and studied for extensive periods in different cultures. While their international identities have diverse starting points and experiences, their views of the world and themselves are profoundly affected by both the realities of living in different cultures and their manner of coping with the challenges that emerge.\".\\n\\nThe term was developed by Linda Brimm, Professor of Organizational Behavior at INSEAD and further explored in her book Global Cosmopolitans: The Creative Edge of Difference.\\n\\nSee also \\n Cosmopolitanism\\n Global citizenship\\n World Citizen\\n\\nReferences\\n\\nCategory:Global citizenship\\nCategory:Globalization\\nCategory:Cosmopolitanism <|query_end|>'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_target = tokten_to_text(target_chunks)\n",
    "s_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|doc_begin|> <|doc_id_begin|> 3926504 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Global Cosmopolitans <|doc_title_end|> <|doc_body_begin|> Global Cosmopolitans refer\n",
      "<|doc_begin|> <|doc_id_begin|> 3926504 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>, Professor of Organizational Behavior at INSEAD and further explored in her book Global Cosmopolitans: \n",
      "<|doc_begin|> <|doc_id_begin|> 5225710 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Thoracic spinal nerve 3 <|doc_title_end|> <|doc_body_begin|> The thoracic spinal ner\n",
      "<|doc_begin|> <|doc_id_begin|> 3966441 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Fred Miller (American football, born 1973) <|doc_title_end|> <|doc_body_begin|> Fred\n",
      "<|doc_begin|> <|doc_id_begin|> 3966441 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>  Bowl XXXIV, having given up several sacks to Kearse in the regular season match up between the teams. \n",
      "<|doc_begin|> <|doc_id_begin|> 3966441 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>  final game with the Rams, as he signed with the Rams' Super Bowl opponent—the Titans—for the 2000 sea\n",
      "<|doc_begin|> <|doc_id_begin|> 3966441 <|doc_id_end|> <|doc_offset_begin|> 273 <|doc_offset_end|>  February 18, 2008 the Bears released him. He was re-signed by the Bears a week into the regular seaso\n",
      "<|doc_begin|> <|doc_id_begin|> 3966441 <|doc_id_end|> <|doc_offset_begin|> 364 <|doc_offset_end|>  the team finished their 2008 season with an 0-9 record. He also coached for the North Shore Titans yo\n",
      "<|doc_begin|> <|doc_id_begin|> 2292096 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Central Texas Electric Cooperative <|doc_title_end|> <|doc_body_begin|> Central Texa\n",
      "<|doc_begin|> <|doc_id_begin|> 2292096 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>  Country counties of Blanco, Gillespie, Kendall, Kerr, Kimble, Llano, Mason, McCulloch, Menard, Real, a\n",
      "<|doc_begin|> <|doc_id_begin|> 2292096 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|> Category:Gillespie County, Texas\\nCategory:Kendall County, Texas\\nCategory:Kerr County, Texas\\nCategory:K\n",
      "<|doc_begin|> <|doc_id_begin|> 3823083 <|doc_id_end|> <|doc_offset_begin|> 455 <|doc_offset_end|>  the 27th, the search vessels, British and American, gathered in a cove, later named Union Bay, at Bee\n",
      "<|doc_begin|> <|doc_id_begin|> 3823083 <|doc_id_end|> <|doc_offset_begin|> 546 <|doc_offset_end|>  party, from Rescue, followed traces of a similar journey by a party from one of Franklin's ships,  or\n",
      "<|doc_begin|> <|doc_id_begin|> 3823083 <|doc_id_end|> <|doc_offset_begin|> 637 <|doc_offset_end|>  two ships, Advance towing Rescue, turned east in hopes of returning to the United States that season.\n",
      "<|doc_begin|> <|doc_id_begin|> 3823083 <|doc_id_end|> <|doc_offset_begin|> 727 <|doc_offset_end|>  Beechey, and in December they drifted down Lancaster Sound. On 14 January 1851, they were carried int\n",
      "<|doc_begin|> <|doc_id_begin|> 3823083 <|doc_id_end|> <|doc_offset_begin|> 818 <|doc_offset_end|>  Disko Bay and into August attempted to renew their search. But the ice was heavier than the previous \n",
      "<|doc_begin|> <|doc_id_begin|> 5650217 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Basílica de Nuestra Señora de la Merced <|doc_title_end|> <|doc_body_begin|> Basílic\n",
      "<|doc_begin|> <|doc_id_begin|> 5650217 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>  of Mercy (Yarumal) <|doc_body_end|> <|doc_end|>\n"
     ]
    }
   ],
   "source": [
    "for toks in docs_chunks:\n",
    "    s = tokten_to_text(toks)\n",
    "    print(s[:200].replace('\\n', '\\\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embs = model_encdec.run_enc_emb(target_chunks)\n",
    "docs_embs = model_encdec.run_enc_emb(docs_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.210349 -0.023194 T\n",
      "-0.038159 0.102338 T\n",
      "0.246970 0.046396 F\n",
      "0.010592 -0.048031 F\n",
      "0.081924 -0.014397 F\n",
      "-0.038597 -0.080679 F\n",
      "0.043957 0.119103 F\n",
      "-0.153411 -0.039547 F\n",
      "-0.146363 -0.027516 F\n",
      "-0.047171 0.202399 F\n",
      "0.004024 0.006097 F\n",
      "0.037217 0.085870 F\n",
      "-0.057347 0.073293 F\n",
      "0.241241 -0.033839 F\n",
      "0.032375 0.033223 F\n",
      "0.041402 -0.134142 F\n",
      "-0.037370 0.080175 F\n",
      "-0.112170 0.257837 F\n"
     ]
    }
   ],
   "source": [
    "cosine = False\n",
    "cosine = True\n",
    "print_dist(target_embs, docs_embs, target_mask, cosine=cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_encoder.src_word_emb.weight (50271, 256) -0.010897174 -8.0230694e-07 0.010897171\n",
      "vocab_encoder.layer_norm.weight (256,) -0.09996011 0.002452353 0.0998353\n",
      "vocab_encoder.layer_norm.bias (256,) -0.09894606 -0.0014543043 0.098527074\n",
      "encoder.a_em () 0.09004872 0.09004872 0.09004872\n",
      "encoder.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.10825315 0.00013853457 0.108247146\n",
      "encoder.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.108252436 0.00011188915 0.108251445\n",
      "encoder.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.10824739 0.00022007307 0.10824797\n",
      "encoder.layer_stack.0.slf_attn.fc.weight (256, 256) -0.10824961 0.00014514917 0.10825284\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.09991062 0.006778831 0.09973943\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09960178 -0.0046932297 0.09947876\n",
      "encoder.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.0684649 5.3948297e-05 0.068465285\n",
      "encoder.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.09996933 0.00040751696 0.09993593\n",
      "encoder.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.068464465 1.6364935e-05 0.06846524\n",
      "encoder.layer_stack.0.pos_ffn.w_2.bias (256,) -0.09997957 0.0010143847 0.099746406\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.09979897 0.002435966 0.09972949\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.09907605 0.002673266 0.09825591\n",
      "encoder.layer_stack.1.slf_attn.w_qs.weight (256, 256) -0.108252324 -3.6885904e-05 0.1082516\n",
      "encoder.layer_stack.1.slf_attn.w_ks.weight (256, 256) -0.1082509 7.633884e-06 0.10824995\n",
      "encoder.layer_stack.1.slf_attn.w_vs.weight (256, 256) -0.10824806 -0.00052938645 0.10825046\n",
      "encoder.layer_stack.1.slf_attn.fc.weight (256, 256) -0.10824975 -0.00010067315 0.10825164\n",
      "encoder.layer_stack.1.slf_attn.layer_norm.weight (256,) -0.0995317 -0.0058074603 0.09613144\n",
      "encoder.layer_stack.1.slf_attn.layer_norm.bias (256,) -0.09894307 0.0011485105 0.09941665\n",
      "encoder.layer_stack.1.pos_ffn.w_1.weight (1024, 256) -0.06846482 -0.00017792097 0.06846518\n",
      "encoder.layer_stack.1.pos_ffn.w_1.bias (1024,) -0.09962793 0.0017607731 0.09997129\n",
      "encoder.layer_stack.1.pos_ffn.w_2.weight (256, 1024) -0.06846531 -0.00015082766 0.068463966\n",
      "encoder.layer_stack.1.pos_ffn.w_2.bias (256,) -0.09893649 0.0011128087 0.09919633\n",
      "encoder.layer_stack.1.pos_ffn.layer_norm.weight (256,) -0.09995148 0.001842032 0.09843429\n",
      "encoder.layer_stack.1.pos_ffn.layer_norm.bias (256,) -0.09993446 0.0009032566 0.09993308\n",
      "encoder.layer_norm.weight (256,) -0.09936333 0.0027532426 0.09806138\n",
      "encoder.layer_norm.bias (256,) -0.09824897 0.0034417557 0.09962133\n",
      "decoder.w.weight (256, 256) -0.10823787 4.3609147e-05 0.10825289\n",
      "decoder.layer_norm.weight (256,) -0.09981865 0.0037350743 0.09973234\n",
      "decoder.layer_norm.bias (256,) -0.09984919 -0.00825671 0.09888097\n"
     ]
    }
   ],
   "source": [
    "inp_len = ds_loader.emb_chunk_size if ds_loader.fixed_size else calc_max_inp_size(ds_loader.emb_chunk_size)\n",
    "model_ranker_cfg = parse_yaml_file_as(MllmRankerCfg, ranker_model_cfg_fpath)\n",
    "model_ranker = MllmRankerLevel(model_ranker_cfg, model_level).to(device)\n",
    "checkpoint_ranker = torch.load(ranker_snapshot_fpath)\n",
    "model_ranker.load_state_dict(checkpoint_ranker['model'])\n",
    "model_ranker.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25, 100]),\n",
       " torch.Size([3, 100]),\n",
       " tensor([ True,  True,  True, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 12\n",
    "batch = ds_loader.get_batch(i, train=True)\n",
    "docs_chunks, target_chunks, target_mask = batch.gen_tensors()\n",
    "docs_chunks.shape, target_chunks.shape, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|query_begin|> TritordeumTritordeum is a hybrid crop, obtained by crossing durum wheat with the wild barley Hordeum chilense. It has less gliadin (gluten) than wheat, but still performs well in breads, both in terms of dough rising and texture qualities, and in taste-testing, where it substantially outperformed gluten-free breads. It has ten times more lutein, more oleic acid, and more fiber than wheat, giving products made from it a yellower hue and a pleasant flavor profile.\\n\\nUnder development by the Spanish National Research Council since 1977, it was launched onto the market in April 2013 by the start-up Agrasys company created under the auspices of the University of Barcelona to commercialize the cereal. It is planted on about 1300 ha in Portugal, Spain, France, Italy and Turkey. It does better in hotter and drier growing conditions than wheat, using less water. Because of this water-saving feature, it won first prize for a Sustainable Ingredient in the 2018 Sustainable Food Awards organized by Ecovia Intelligence.\\n\\nReferences\\n\\nCategory:Cereals\\nCategory:Pooideae\\nCategory:Food plant hybrids <|query_end|>'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_target = tokten_to_text(target_chunks)\n",
    "s_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|doc_begin|> <|doc_id_begin|> 3382600 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Tritordeum <|doc_title_end|> <|doc_body_begin|> Tritordeum is a hybrid crop, obtaine\n",
      "<|doc_begin|> <|doc_id_begin|> 3382600 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|> leic acid, and more fiber than wheat, giving products made from it a yellower hue and a pleasant flavor\n",
      "<|doc_begin|> <|doc_id_begin|> 3382600 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>  It does better in hotter and drier growing conditions than wheat, using less water. Because of this w\n",
      "<|doc_begin|> <|doc_id_begin|> 5770866 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Birami <|doc_title_end|> <|doc_body_begin|> Birami is a panchayat village in Rajasth\n",
      "<|doc_begin|> <|doc_id_begin|> 5770866 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|> \\nGeography\\nThe village of Birami is 32 km by road southeast of the city of Jodhpur, located between the\n",
      "<|doc_begin|> <|doc_id_begin|> 5770866 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>  Tanawara, 30 km by road to the west.\\n\\nDemographics \\nIn the 2001 census, the village of Birami had 1,7\n",
      "<|doc_begin|> <|doc_id_begin|> 5770866 <|doc_id_end|> <|doc_offset_begin|> 273 <|doc_offset_end|> External links \\n \\n\\nCategory:Villages in Jodhpur district <|doc_body_end|> <|doc_end|>\n",
      "<|doc_begin|> <|doc_id_begin|> 2149109 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Ringer T-shirt <|doc_title_end|> <|doc_body_begin|> A ringer T-shirt is a T-shirt in\n",
      "<|doc_begin|> <|doc_id_begin|> 2149109 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|> and-roll subculture. \\n\\nRingers grew in popularity even more as the 1970s began and remained popular thr\n",
      "<|doc_begin|> <|doc_id_begin|> 2149109 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|> :1950s fashion\\nCategory:1970s fashion\\nCategory:2000s fashion\\nCategory:T-shirts <|doc_body_end|> <|doc_\n",
      "<|doc_begin|> <|doc_id_begin|> 4176085 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Étienne Lécroart <|doc_title_end|> <|doc_body_begin|> Étienne Lécroart (born 1960) i\n",
      "<|doc_begin|> <|doc_id_begin|> 4176085 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>  participates in several musical bands.\\n\\nAwards\\n 1999 : Lauréat du Trophée Presse-Citron, best French p\n",
      "<|doc_begin|> <|doc_id_begin|> 4176085 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>  et Tic (Hors Gabarit - 1995)\\nLa vie exemplaire de Saint Sinus (Cornélius - 1995)\\nPoil au Cupidon (Glé\n",
      "<|doc_begin|> <|doc_id_begin|> 4176085 <|doc_id_end|> <|doc_offset_begin|> 273 <|doc_offset_end|> uil - 1998)\\nRatatouille (Le Seuil - 1999)\\nCercle vicieux (L'Association - 2000) — the first half of th\n",
      "<|doc_begin|> <|doc_id_begin|> 4176085 <|doc_id_end|> <|doc_offset_begin|> 364 <|doc_offset_end|> énat - 2002)\\nSuperlipopette (Glénat - 2003)\\nLe Cycle (L'Association - 2003)\\nOupus 2 (L'Association - 2\n",
      "<|doc_begin|> <|doc_id_begin|> 1258156 <|doc_id_end|> <|doc_offset_begin|> 546 <|doc_offset_end|>  (Liquidambar styraciflua), and Pohutukawa tree (Metrosideros excelsa).  Leading up to the study one o\n",
      "<|doc_begin|> <|doc_id_begin|> 1258156 <|doc_id_end|> <|doc_offset_begin|> 637 <|doc_offset_end|>  author on the publication.\\n\\nLead author Deborah Springer said, \"Just as people who travel to South Am\n",
      "<|doc_begin|> <|doc_id_begin|> 1258156 <|doc_id_end|> <|doc_offset_begin|> 727 <|doc_offset_end|>  Papua New Guinea and Northern Australia. Cases have also been reported in other regions, indicating i\n",
      "<|doc_begin|> <|doc_id_begin|> 1258156 <|doc_id_end|> <|doc_offset_begin|> 818 <|doc_offset_end|>  extremely rapidly within white blood cells.\\n\\nIn the United States, C. gattii serotype B, subtype VGII\n",
      "<|doc_begin|> <|doc_id_begin|> 1258156 <|doc_id_end|> <|doc_offset_begin|> 908 <|doc_offset_end|>  from Washington, and one each from Idaho and California. Slightly more than half of these case were i\n",
      "<|doc_begin|> <|doc_id_begin|> 3563517 <|doc_id_end|> <|doc_offset_begin|> 637 <|doc_offset_end|> \\n\\nTours\\nThe band has toured with Despised Icon, Parkway Drive, Impending Doom, Danzig, Dimmu Borgir, V\n",
      "<|doc_begin|> <|doc_id_begin|> 3563517 <|doc_id_end|> <|doc_offset_begin|> 727 <|doc_offset_end|>, The Warriors, and Terror.\\nThey were featured on The Atticus Metal Tour with Emmure, All Shall Perish,\n",
      "<|doc_begin|> <|doc_id_begin|> 3563517 <|doc_id_end|> <|doc_offset_begin|> 818 <|doc_offset_end|> imation of the Nation tour.\\nOn January 20, 2010, it was announced that Winds of Plague would be playin\n",
      "<|doc_begin|> <|doc_id_begin|> 3563517 <|doc_id_end|> <|doc_offset_begin|> 908 <|doc_offset_end|>  the Burial and headliners As I Lay Dying. They played several songs from their then upcoming new albu\n",
      "<|doc_begin|> <|doc_id_begin|> 3563517 <|doc_id_end|> <|doc_offset_begin|> 998 <|doc_offset_end|>  Perish during Caliban's \"Get Infected\" Tour 2012 with support of We Butter The Bread With Butter, Eye\n"
     ]
    }
   ],
   "source": [
    "for toks in docs_chunks:\n",
    "    s = tokten_to_text(toks)\n",
    "    print(s[:200].replace('\\n', '\\\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embs = model_ranker.run_enc_emb(target_chunks)\n",
    "docs_embs = model_ranker.run_enc_emb(docs_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.851829 0.768325 0.717993 T\n",
      "0.710824 0.864115 0.791755 T\n",
      "0.751394 0.784505 0.901214 T\n",
      "0.601972 0.765515 0.664485 F\n",
      "0.506602 0.649703 0.602839 F\n",
      "0.574463 0.730713 0.660725 F\n",
      "0.385619 0.497914 0.482378 F\n",
      "0.592718 0.615067 0.464479 F\n",
      "0.415337 0.564398 0.493007 F\n",
      "0.590724 0.656549 0.577092 F\n",
      "0.516982 0.525026 0.389098 F\n",
      "0.691276 0.658016 0.652711 F\n",
      "0.466441 0.544165 0.483684 F\n",
      "0.521704 0.572658 0.464098 F\n",
      "0.583230 0.539854 0.519573 F\n",
      "0.464891 0.593930 0.624339 F\n",
      "0.446136 0.458801 0.597052 F\n",
      "0.523654 0.535276 0.620607 F\n",
      "0.393278 0.383729 0.532924 F\n",
      "0.473186 0.469718 0.547572 F\n",
      "0.421835 0.542141 0.402089 F\n",
      "0.443597 0.613942 0.547976 F\n",
      "0.482703 0.653324 0.531004 F\n",
      "0.495839 0.644287 0.531666 F\n",
      "0.453870 0.573715 0.463971 F\n"
     ]
    }
   ],
   "source": [
    "cosine = False\n",
    "cosine = True\n",
    "print_dist(target_embs, docs_embs, target_mask, cosine=cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n",
      "0.281365 T\n",
      "0.378958 T\n",
      "0.301095 T\n",
      "0.381230 F\n",
      "0.428091 F\n",
      "0.460663 F\n",
      "0.501278 F\n",
      "0.475151 F\n",
      "0.510579 F\n",
      "0.420649 F\n",
      "0.636904 F\n",
      "0.370556 F\n",
      "0.539754 F\n",
      "0.610080 F\n",
      "0.480029 F\n",
      "0.353345 F\n",
      "0.226356 F\n",
      "0.358335 F\n",
      "0.394531 F\n",
      "0.479233 F\n",
      "0.359256 F\n",
      "0.500798 F\n",
      "0.417691 F\n",
      "0.460942 F\n",
      "0.405442 F\n"
     ]
    }
   ],
   "source": [
    "txt = 'El Charco del Cura Reservoir'\n",
    "# txt = 'Théodore Eugène César Ruyssen'\n",
    "# txt = 'Théodore Ruyssen'\n",
    "txt = 'orders in certain situations, and both required trade-offs'\n",
    "txt = 'Tritordeum hybrid crop'\n",
    "txt = 'etienne Lecroart ddd'\n",
    "txt = 'Ну приветики ну погоди'\n",
    "\n",
    "# txt = 'The graph sandwich problem for property Π is defined as follows:'\n",
    "cosine = True\n",
    "txt_tokens = text_to_tokens(txt, qbeg_tok=qbeg_tok, qend_tok=qend_tok)\n",
    "# txt_tokens = txt_tokens.repeat(3, 1)\n",
    "print(txt_tokens.shape)\n",
    "txt_embs = model_ranker.run_enc_emb(txt_tokens)\n",
    "print_dist(txt_embs, docs_embs, target_mask, cosine=cosine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.001259',\n",
       " '0.003427',\n",
       " '0.001251',\n",
       " '0.003243',\n",
       " '0.007756',\n",
       " '0.014418',\n",
       " '0.033158',\n",
       " '0.038146',\n",
       " '0.018492',\n",
       " '0.008177',\n",
       " '0.388008',\n",
       " '0.003869',\n",
       " '0.064414',\n",
       " '0.174521',\n",
       " '0.030543',\n",
       " '0.006355',\n",
       " '0.000525',\n",
       " '0.003603',\n",
       " '0.003947',\n",
       " '0.080367',\n",
       " '0.003919',\n",
       " '0.048465',\n",
       " '0.010669',\n",
       " '0.039282',\n",
       " '0.012188']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rank = model_ranker.run_qs_infer(docs_chunks, txt_tokens)\n",
    "rank_str = [f'{r:.06f}' for r in rank.flatten()]\n",
    "rank_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 100])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_chunks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8737, 0.8907, 0.7371, 0.5201, 0.4101, 0.8237, 0.7342, 0.8183, 0.8694,\n",
       "         0.6263, 0.6608, 0.6087, 0.2565, 0.3135, 0.3250]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank = model_ranker(txt_tokens, docs_chunks)\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6538e-21, 3.7294e-11, 2.4412e-19, 3.4020e-23, 7.5630e-20, 3.8663e-25,\n",
       "         1.3986e-18, 7.2614e-22, 1.0000e+00, 1.0000e+00, 1.0000e+00, 4.7166e-09,\n",
       "         1.9997e-11, 1.3684e-08]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank = model_ranker(target_chunks, docs_chunks)\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device1 = torch.device('cpu')\n",
    "device2 = torch.device('cuda')\n",
    "device3 = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(device1 == device2)\n",
    "print(device2 == device3)\n",
    "print(device1 == device3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cpu', str)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device1.type, type(device1.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.5,  0.5],\n",
       "       [ 1.7,  1.7,  1.7],\n",
       "       [-1. ,  7. , 33. ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "toks = [\n",
    "    np.ones(3) * 0.5,\n",
    "    np.ones(3) * 1.7,\n",
    "    np.array([-1, 7, 33])\n",
    "]\n",
    "np.stack(toks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.5,  0.5],\n",
       "       [ 1.7,  1.7,  1.7],\n",
       "       [-1. ,  7. , 33. ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(np.array(toks), np.stack(toks, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
