{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, AddedToken, PreTrainedTokenizer\n",
    "\n",
    "from mllm.data.dsfixed import DsLoader\n",
    "from mllm.model.mllm_encdec import MllmEncdec\n",
    "from mllm.model.mllm_ranker import MllmRanker\n",
    "from mllm.model.config import create_mllm_encdec_cfg, create_mllm_ranker_cfg\n",
    "from mllm.tokenization.chunk_tokenizer import calc_max_inp_size, gen_all_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(os.path.expandvars('$HOME')) / 'data'\n",
    "TRAIN_ENCDEC_PATH = DATA_PATH / 'train_mllm_encdec'\n",
    "TRAIN_RANKER_PATH = DATA_PATH / 'train_mllm_ranker'\n",
    "DS_DIR_PATH = DATA_PATH / 'wiki_20200501_en' / 'ch_100_fixed'\n",
    "\n",
    "encdec_subdir = 'encdec-20240718_221554-wiki_20200501_en-ch_100_fixed'\n",
    "ranker_subdir = 'ranker-20240722_225232-wiki_20200501_en-ch_100_fixed'\n",
    "encdec_train_path = TRAIN_ENCDEC_PATH / encdec_subdir\n",
    "ranker_train_path = TRAIN_RANKER_PATH / ranker_subdir\n",
    "encdec_snapshot_path = encdec_train_path / 'best.pth'\n",
    "ranker_snapshot_path = ranker_train_path / 'best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "docs_batch_size = 5\n",
    "max_chunks_per_doc = 3\n",
    "device = 'cpu'\n",
    "# device = 'cuda'\n",
    "\n",
    "device = torch.device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cache from /home/misha/data/wiki_20200501_en/ch_100_fixed/.mllm/ds.csv\n",
      "Loaded dataset size: 50989207\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', model_max_length=100000)\n",
    "tok_dict = gen_all_tokens(tokenizer)\n",
    "pad_tok, qbeg_tok, qend_tok = tok_dict['pad'].ind, tok_dict['query_begin'].ind, tok_dict['query_end'].ind\n",
    "ds_loader = DsLoader(\n",
    "    ds_dir_path=DS_DIR_PATH, docs_batch_size=docs_batch_size, max_chunks_per_doc=max_chunks_per_doc,\n",
    "    pad_tok=pad_tok, qbeg_tok=qbeg_tok, qend_tok=qend_tok, device=device,\n",
    ")\n",
    "ds_loader.shuffle(train=True)\n",
    "ds_loader.shuffle(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_encoder.src_word_emb.weight (50270, 256) -0.010897282 3.297105e-06 0.010897281\n",
      "vocab_encoder.layer_norm.weight (256,) -0.09975823 -0.0025114394 0.09987755\n",
      "vocab_encoder.layer_norm.bias (256,) -0.09988537 -0.007381388 0.099962406\n",
      "encoder.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.10825245 -0.00040648147 0.10825277\n",
      "encoder.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.10824651 -0.0002117044 0.108251125\n",
      "encoder.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.10824577 -0.00016405826 0.108251214\n",
      "encoder.layer_stack.0.slf_attn.fc.weight (256, 256) -0.10825165 0.00010650148 0.1082506\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.09909498 0.0031277358 0.098389916\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09852611 0.00021516567 0.09963594\n",
      "encoder.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.068464994 -1.0003656e-05 0.06846488\n",
      "encoder.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.099880375 0.0015438542 0.099917404\n",
      "encoder.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.0684647 -3.695621e-05 0.068464674\n",
      "encoder.layer_stack.0.pos_ffn.w_2.bias (256,) -0.09957941 0.0020497527 0.09737736\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.09947195 0.0015110604 0.09898522\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.09970957 0.0039037121 0.0998918\n",
      "encoder.w_em.weight (1, 100) -0.23817135 -0.005802563 0.23852295\n",
      "decoder.A_emb2sec (100, 256, 256) -0.008113916 -1.1313335e-06 0.008113918\n",
      "decoder.att_layers.0.slf_attn.w_qs.weight (256, 256) -0.108250216 -0.00017885957 0.108246684\n",
      "decoder.att_layers.0.slf_attn.w_ks.weight (256, 256) -0.10825314 -3.8648002e-05 0.108246796\n",
      "decoder.att_layers.0.slf_attn.w_vs.weight (256, 256) -0.108251445 6.820202e-05 0.10825147\n",
      "decoder.att_layers.0.slf_attn.fc.weight (256, 256) -0.10824423 -0.00024411344 0.108248234\n",
      "decoder.att_layers.0.slf_attn.layer_norm.weight (256,) -0.09985622 -0.0037287087 0.09931284\n",
      "decoder.att_layers.0.slf_attn.layer_norm.bias (256,) -0.09960695 0.00013314513 0.09981384\n",
      "decoder.att_layers.0.pos_ffn.w_1.weight (1024, 256) -0.06846515 -5.2930976e-05 0.068465136\n",
      "decoder.att_layers.0.pos_ffn.w_1.bias (1024,) -0.09997386 0.0031843865 0.09992688\n",
      "decoder.att_layers.0.pos_ffn.w_2.weight (256, 1024) -0.06846499 6.779283e-05 0.06846521\n",
      "decoder.att_layers.0.pos_ffn.w_2.bias (256,) -0.09885056 0.002904273 0.099525444\n",
      "decoder.att_layers.0.pos_ffn.layer_norm.weight (256,) -0.099305116 0.0031109913 0.099496484\n",
      "decoder.att_layers.0.pos_ffn.layer_norm.bias (256,) -0.0996341 0.0010085823 0.09950968\n",
      "vocab_decoder.word_prj.weight (50270, 256) -0.010897282 4.0946805e-08 0.010897281\n"
     ]
    }
   ],
   "source": [
    "inp_len = ds_loader.emb_chunk_size if ds_loader.fixed_size else calc_max_inp_size(ds_loader.emb_chunk_size)\n",
    "model_encdec_cfg = create_mllm_encdec_cfg(\n",
    "    n_vocab=len(tokenizer), d_word_wec=256, inp_len=inp_len,\n",
    "    enc_n_layers=1, dec_n_layers=1,\n",
    "    n_heads=8, d_model=256, d_inner=1024,\n",
    "    pad_idx=pad_tok, dropout_rate=0.1, enc_with_emb_mat=True,\n",
    ")\n",
    "model_encdec = MllmEncdec(model_encdec_cfg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_encdec = torch.load(encdec_snapshot_path)\n",
    "model_encdec.load_state_dict(checkpoint_encdec['model'])\n",
    "model_encdec.eval()\n",
    "del checkpoint_encdec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokten_to_text(tokens: torch.Tensor) -> str:\n",
    "    tokens = tokens.flatten()\n",
    "    tokens = tokens[tokens != pad_tok]\n",
    "    tokens = list(tokens)\n",
    "    s = tokenizer.decode(tokens)\n",
    "    return s\n",
    "\n",
    "def distance(x: np.ndarray, y: np.ndarray, cosine: bool = False):\n",
    "    if not cosine:\n",
    "        return np.linalg.norm(x - y)\n",
    "    x_norm, y_norm = np.linalg.norm(x), np.linalg.norm(y)\n",
    "    return np.sum(x * y) / (x_norm * y_norm)\n",
    "\n",
    "def text_to_tokens(s: str) -> torch.Tensor:\n",
    "    tokens = tokenizer(s)['input_ids']\n",
    "    n_tokens = len(tokens)\n",
    "    n_padded = n_tokens // inp_len + (n_tokens % inp_len > 0)\n",
    "    res = np.full((n_padded * inp_len, ), pad_tok, dtype=np.int32)\n",
    "    res[:n_tokens] = tokens\n",
    "    res = torch.from_numpy(res).to(device)\n",
    "    res = res.reshape(n_padded, inp_len)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15496,   612, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_tokens('Hello there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9, 100]),\n",
       " torch.Size([2, 100]),\n",
       " tensor([ True,  True, False, False, False, False, False, False, False]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 10\n",
    "batch = ds_loader.get_batch(i, train=True)\n",
    "docs_chunks, target_chunks, target_mask = batch.gen_tensors()\n",
    "docs_chunks.shape, target_chunks.shape, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|query_begin|> 1987 in Hong KongEvents in the year 1987 in British Hong Kong.\\n\\nIncumbents\\n Monarch – Elizabeth II\\n Governor – Sir David Akers-Jones (until 9 April), Sir David Wilson (starting 9 April)\\n\\nEvents\\n\\nJanuary\\n\\nFebruary\\n\\nMarch\\n\\nApril\\n\\nMay\\n\\nJune\\n\\nJuly\\n\\nAugust\\n\\nSeptember\\n\\nOctober\\n\\nNovember\\n\\nDecember\\n\\nReferences\\n\\nCategory:1987 in Hong Kong\\nCategory:Years of the 20th century in Hong Kong\\nHong Kong\\nHong Kong\\nCategory:1987 in British Overseas Territories <|query_end|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_target = tokten_to_text(target_chunks)\n",
    "s_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|doc_begin|> <|doc_id_begin|> 1991677 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> 1987 in Hong Kong <|doc_title_end|> <|doc_body_begin|> Events in the year 1987 in Br\n",
      "<|doc_begin|> <|doc_id_begin|> 1991677 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|> \\n\\nCategory:1987 in Hong Kong\\nCategory:Years of the 20th century in Hong Kong\\nHong Kong\\nHong Kong\\nCatego\n",
      "<|doc_begin|> <|doc_id_begin|> 1219385 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Charles Boles House <|doc_title_end|> <|doc_body_begin|> The Charles Boles House, lo\n",
      "<|doc_begin|> <|doc_id_begin|> 1219385 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>  designed more than 80 residences, churches, and commercial buildings in Kalispell.\\n\\nReferences\\n\\nCatego\n",
      "<|doc_begin|> <|doc_id_begin|> 3140247 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Stebnik <|doc_title_end|> <|doc_body_begin|> Stebnik may refer to:\\n\\nStebník,  villag\n",
      "<|doc_begin|> <|doc_id_begin|> 400572 <|doc_id_end|> <|doc_offset_begin|> 1192 <|doc_offset_end|>  reassigned to the U-Flotille, Hochseeflotte (1st Submarine Flotilla, High Seas Fleet); but remained i\n",
      "<|doc_begin|> <|doc_id_begin|> 400572 <|doc_id_end|> <|doc_offset_begin|> 1283 <|doc_offset_end|>, the United States Navy expressed an interest in acquiring several former German submarines to serve a\n",
      "<|doc_begin|> <|doc_id_begin|> 400572 <|doc_id_end|> <|doc_offset_begin|> 1374 <|doc_offset_end|>  on 3 April, in company with the submarine tender, and UB-88, UB-148, and. This unlikely American task\n",
      "<|doc_begin|> <|doc_id_begin|> 3539765 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Coleophora narbonensis <|doc_title_end|> <|doc_body_begin|> Coleophora narbonensis i\n"
     ]
    }
   ],
   "source": [
    "for toks in docs_chunks:\n",
    "    s = tokten_to_text(toks)\n",
    "    print(s[:200].replace('\\n', '\\\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embs = model_encdec.run_enc_emb(target_chunks)\n",
    "docs_embs = model_encdec.run_enc_emb(docs_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806555 0.197400 T\n",
      "0.357577 0.731144 T\n",
      "0.306005 0.101844 F\n",
      "0.201823 0.351509 F\n",
      "0.121644 0.385238 F\n",
      "0.244798 0.153832 F\n",
      "0.309911 0.141900 F\n",
      "0.223536 0.158956 F\n",
      "0.134927 0.299271 F\n"
     ]
    }
   ],
   "source": [
    "for i, docs_emb in enumerate(docs_embs.detach().numpy()):\n",
    "    for target_emb in target_embs.detach().numpy():\n",
    "        dist = distance(target_emb, docs_emb, True)\n",
    "        print(f'{dist:0.6f} ', end='')\n",
    "    sfx = 'T' if target_mask[i] else 'F'\n",
    "    print(sfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_encoder.src_word_emb.weight (50270, 256) -0.010897282 -1.7279738e-06 0.010897279\n",
      "vocab_encoder.layer_norm.weight (256,) -0.098713435 -0.00068409566 0.09903578\n",
      "vocab_encoder.layer_norm.bias (256,) -0.099790215 -0.0012416485 0.09938752\n",
      "encoders.0.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.10825065 0.0003846487 0.108250104\n",
      "encoders.0.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.10825002 -4.3549368e-05 0.10824833\n",
      "encoders.0.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.108249344 -0.00020081543 0.108252294\n",
      "encoders.0.layer_stack.0.slf_attn.fc.weight (256, 256) -0.108252816 -4.5218905e-05 0.1082463\n",
      "encoders.0.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.09881319 0.00011143202 0.09914114\n",
      "encoders.0.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09990596 0.0003516497 0.09964675\n",
      "encoders.0.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.06846519 -3.5115994e-05 0.0684649\n",
      "encoders.0.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.09875158 -0.0010093136 0.09944109\n",
      "encoders.0.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.06846321 -0.00016303948 0.06846521\n",
      "encoders.0.layer_stack.0.pos_ffn.w_2.bias (256,) -0.099798135 -0.0030074841 0.0998803\n",
      "encoders.0.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.09906606 -0.004313059 0.09969669\n",
      "encoders.0.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.09995061 0.0027134586 0.099307574\n",
      "encoders.0.w_em.weight (1, 100) -0.2425918 -0.026042558 0.23191011\n",
      "decoders.0.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.10825267 -8.538761e-05 0.108252876\n",
      "decoders.0.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.108246475 -0.00044028 0.10825201\n",
      "decoders.0.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.10824921 8.584757e-06 0.10824827\n",
      "decoders.0.layer_stack.0.slf_attn.fc.weight (256, 256) -0.1082508 0.00012864501 0.1082509\n",
      "decoders.0.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.09976374 -0.0019519995 0.09983406\n",
      "decoders.0.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09974124 -0.0001687 0.09677023\n",
      "decoders.0.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.06846415 -3.0036603e-05 0.06846508\n",
      "decoders.0.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.099653125 -0.0018196905 0.09916041\n",
      "decoders.0.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.0684653 8.2340346e-05 0.06846488\n",
      "decoders.0.layer_stack.0.pos_ffn.w_2.bias (256,) -0.09730931 0.0024772068 0.09999806\n",
      "decoders.0.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.09809416 0.0003457354 0.09976999\n",
      "decoders.0.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.09968877 -0.006485308 0.097221665\n",
      "decoders.0.layer_norm.weight (256,) -0.09862586 -0.0024930458 0.09883981\n",
      "decoders.0.layer_norm.bias (256,) -0.099685825 -0.00021420047 0.09953176\n",
      "decoders.0.rank_prj.weight (1, 256) -0.15116313 -0.0033344214 0.15144873\n"
     ]
    }
   ],
   "source": [
    "inp_len = ds_loader.emb_chunk_size if ds_loader.fixed_size else calc_max_inp_size(ds_loader.emb_chunk_size)\n",
    "model_ranker_cfg = create_mllm_ranker_cfg(\n",
    "    n_vocab=len(tokenizer), inp_len=inp_len, d_word_wec=256,\n",
    "    n_levels=1, enc_n_layers=1, dec_n_layers=1,\n",
    "    n_heads=8, d_k=32, d_v=32, d_model=256, d_inner=1024,\n",
    "    pad_idx=pad_tok, dropout_rate=0.1, enc_with_emb_mat=True,\n",
    ")\n",
    "\n",
    "model_ranker = MllmRanker(model_ranker_cfg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_ranker = torch.load(ranker_snapshot_path)\n",
    "model_ranker.load_state_dict(checkpoint_ranker['model'])\n",
    "model_ranker.eval()\n",
    "del checkpoint_ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embs = model_ranker.run_enc_emb(target_chunks)\n",
    "docs_embs = model_ranker.run_enc_emb(docs_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.480150 0.234837 T\n",
      "0.233881 0.560774 T\n",
      "0.056342 -0.095653 F\n",
      "0.016592 0.186500 F\n",
      "-0.069437 0.286522 F\n",
      "-0.053729 -0.298380 F\n",
      "-0.054713 -0.335745 F\n",
      "-0.122766 -0.349096 F\n",
      "-0.281377 0.023078 F\n"
     ]
    }
   ],
   "source": [
    "def print_dist(target_embs: torch.Tensor, docs_embs: torch.Tensor, cosine: bool = True):\n",
    "    for i, docs_emb in enumerate(docs_embs.detach().numpy()):\n",
    "        for target_emb in target_embs.detach().numpy():\n",
    "            dist = distance(target_emb, docs_emb, cosine)\n",
    "            print(f'{dist:0.6f} ', end='')\n",
    "        sfx = 'T' if target_mask[i] else 'F'\n",
    "        print(sfx)\n",
    "\n",
    "cosine = False\n",
    "cosine = True\n",
    "print_dist(target_embs, docs_embs, cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.049998 T\n",
      "0.411652 T\n",
      "-0.035347 F\n",
      "0.134199 F\n",
      "0.358041 F\n",
      "-0.158853 F\n",
      "-0.138117 F\n",
      "-0.066518 F\n",
      "0.190424 F\n"
     ]
    }
   ],
   "source": [
    "txt = 'Hong Kong 1987'\n",
    "# txt = 'Events in the year 1987'\n",
    "# txt = 'Events in the year 1987 Hong Kong'\n",
    "cosine = True\n",
    "txt_tokens = text_to_tokens(txt)\n",
    "txt_embs = model_ranker.run_enc_emb(txt_tokens)\n",
    "print_dist(txt_embs, docs_embs, cosine=cosine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.0222e-05],\n",
       "         [5.7394e-05],\n",
       "         [2.1583e-05],\n",
       "         [1.7689e-05],\n",
       "         [7.0402e-01],\n",
       "         [3.1666e-06],\n",
       "         [3.6296e-06],\n",
       "         [4.0126e-06],\n",
       "         [6.7034e-05]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank = model_ranker(txt_tokens, docs_chunks)\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
