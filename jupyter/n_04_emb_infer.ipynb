{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Optional\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, AddedToken, PreTrainedTokenizer\n",
    "\n",
    "from mllm.data.wiki.dswiki import WikiDsLoader\n",
    "from mllm.model.mllm_encdec import MllmEncdec\n",
    "from mllm.model.mllm_ranker import MllmRanker\n",
    "from mllm.exp.cfg import create_mllm_encdec_cfg, create_mllm_ranker_cfg\n",
    "from mllm.tokenization.chunk_tokenizer import calc_max_inp_size, gen_all_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(os.path.expandvars('$HOME')) / 'data'\n",
    "TRAIN_ENCDEC_PATH = DATA_PATH / 'train_mllm_encdec'\n",
    "# TRAIN_RANKER_PATH = DATA_PATH / 'train_mllm_ranker'\n",
    "TRAIN_RANKER_PATH = DATA_PATH / 'train_mllm_ranker_qs'\n",
    "DS_DIR_PATH = DATA_PATH / 'wiki_20200501_en' / 'ch_100_fixed'\n",
    "\n",
    "encdec_subdir = 'encdec-20240718_221554-wiki_20200501_en-ch_100_fixed'\n",
    "# ranker_subdir = 'ranker-20240722_225232-wiki_20200501_en-ch_100_fixed'\n",
    "# ranker_subdir = 'ranker-20240724_230827-wiki_20200501_en-ch_100_fixed'\n",
    "# ranker_subdir = 'ranker-20240726_232850-wiki_20200501_en-ch_100_fixed'\n",
    "# ranker_subdir = 'ranker-20240722_225232-wiki_20200501_en-ch_100_fixed'\n",
    "ranker_subdir = 'ranker-20240730_213328-wiki_20200501_en-ch_100_fixed'\n",
    "# ranker_subdir = 'ranker-20240806_221913-msmarco'\n",
    "ranker_subdir = 'ranker-20240815_180317-msmarco'\n",
    "encdec_train_path = TRAIN_ENCDEC_PATH / encdec_subdir\n",
    "ranker_train_path = TRAIN_RANKER_PATH / ranker_subdir\n",
    "encdec_snapshot_path = encdec_train_path / 'best.pth'\n",
    "ranker_snapshot_path = ranker_train_path / 'best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "docs_batch_size = 5\n",
    "max_chunks_per_doc = 3\n",
    "device = 'cpu'\n",
    "# device = 'cuda'\n",
    "\n",
    "device = torch.device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cache from /home/misha/data/wiki_20200501_en/ch_100_fixed/.mllm/ds.csv\n",
      "Loaded dataset size: 50989207\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', model_max_length=100000)\n",
    "tok_dict = gen_all_tokens(tokenizer)\n",
    "pad_tok, qbeg_tok, qend_tok = tok_dict['pad'].ind, tok_dict['query_begin'].ind, tok_dict['query_end'].ind\n",
    "ds_loader = WikiDsLoader(\n",
    "    ds_path=DS_DIR_PATH, docs_batch_size=docs_batch_size, max_chunks_per_doc=max_chunks_per_doc,\n",
    "    pad_tok=pad_tok, qbeg_tok=qbeg_tok, qend_tok=qend_tok, device=device,\n",
    ")\n",
    "ds_loader.shuffle(train=True)\n",
    "ds_loader.shuffle(train=False)\n",
    "inp_len = ds_loader.emb_chunk_size if ds_loader.fixed_size else calc_max_inp_size(ds_loader.emb_chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokten_to_text(tokens: torch.Tensor) -> str:\n",
    "    tokens = tokens.flatten()\n",
    "    tokens = tokens[tokens != pad_tok]\n",
    "    tokens = list(tokens)\n",
    "    s = tokenizer.decode(tokens)\n",
    "    return s\n",
    "\n",
    "def distance(x: np.ndarray, y: np.ndarray, cosine: bool = False):\n",
    "    if not cosine:\n",
    "        return np.linalg.norm(x - y)\n",
    "    x_norm, y_norm = np.linalg.norm(x), np.linalg.norm(y)\n",
    "    return np.sum(x * y) / (x_norm * y_norm)\n",
    "\n",
    "def text_to_tokens(s: str, qbeg_tok: Optional[int] = None, qend_tok: Optional[int] = None) -> torch.Tensor:\n",
    "    tokens = tokenizer(s)['input_ids']\n",
    "    if qbeg_tok is not None:\n",
    "        assert qend_tok is not None\n",
    "        tokens = [qbeg_tok, *tokens, qend_tok]\n",
    "    n_tokens = len(tokens)\n",
    "    n_padded = n_tokens // inp_len + (n_tokens % inp_len > 0)\n",
    "    res = np.full((n_padded * inp_len, ), pad_tok, dtype=np.int32)\n",
    "    res[:n_tokens] = tokens\n",
    "    res = torch.from_numpy(res).to(device)\n",
    "    res = res.reshape(n_padded, inp_len)\n",
    "    return res\n",
    "\n",
    "def print_dist(target_embs: torch.Tensor, docs_embs: torch.Tensor, target_mask: torch.Tensor, cosine: bool = True):\n",
    "    for i, docs_emb in enumerate(docs_embs.detach().numpy()):\n",
    "        for target_emb in target_embs.detach().numpy():\n",
    "            dist = distance(target_emb, docs_emb, cosine)\n",
    "            print(f'{dist:0.6f} ', end='')\n",
    "        sfx = 'T' if target_mask[i] else 'F'\n",
    "        print(sfx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_encoder.src_word_emb.weight (50270, 256) -0.010897282 -2.312324e-06 0.010897279\n",
      "vocab_encoder.layer_norm.weight (256,) -0.09936088 0.0025157412 0.09930128\n",
      "vocab_encoder.layer_norm.bias (256,) -0.099341646 -0.0038342725 0.09996939\n",
      "encoder.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.10824653 0.00019045657 0.10824982\n",
      "encoder.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.108253084 0.00037009074 0.1082508\n",
      "encoder.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.10824686 -0.00019107715 0.10825228\n",
      "encoder.layer_stack.0.slf_attn.fc.weight (256, 256) -0.10825217 1.51973e-05 0.10824655\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.099464975 -0.0007957523 0.09971613\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09999391 0.00015633227 0.09630709\n",
      "encoder.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.068465225 -6.698758e-05 0.06846504\n",
      "encoder.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.09969604 0.0031469457 0.0999882\n",
      "encoder.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.06846495 4.238102e-05 0.06846522\n",
      "encoder.layer_stack.0.pos_ffn.w_2.bias (256,) -0.09999285 0.0019646124 0.098518886\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.09919732 0.00074220845 0.09953789\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.09986724 -0.0012141446 0.09849154\n",
      "encoder.w_em.weight (1, 100) -0.21545205 0.02072555 0.24363595\n",
      "encoder.layer_norm.weight (256,) -0.09951141 -0.0031941265 0.09921314\n",
      "encoder.layer_norm.bias (256,) -0.09973545 0.00015650457 0.09997436\n",
      "decoder.A_emb2sec (100, 256, 256) -0.008113909 -1.5867378e-06 0.008113918\n",
      "decoder.att_layers.0.slf_attn.w_qs.weight (256, 256) -0.108246565 -0.00011288003 0.10825215\n",
      "decoder.att_layers.0.slf_attn.w_ks.weight (256, 256) -0.10825294 -0.000418403 0.108251795\n",
      "decoder.att_layers.0.slf_attn.w_vs.weight (256, 256) -0.10824436 -0.00017862066 0.10825185\n",
      "decoder.att_layers.0.slf_attn.fc.weight (256, 256) -0.108240955 -0.0002729901 0.10825285\n",
      "decoder.att_layers.0.slf_attn.layer_norm.weight (256,) -0.09970149 0.0025805861 0.09982114\n",
      "decoder.att_layers.0.slf_attn.layer_norm.bias (256,) -0.09895283 0.00036759063 0.099720694\n",
      "decoder.att_layers.0.pos_ffn.w_1.weight (1024, 256) -0.068464726 7.241628e-05 0.06846469\n",
      "decoder.att_layers.0.pos_ffn.w_1.bias (1024,) -0.099916 -0.00064845657 0.09963099\n",
      "decoder.att_layers.0.pos_ffn.w_2.weight (256, 1024) -0.06846455 2.9912833e-05 0.0684646\n",
      "decoder.att_layers.0.pos_ffn.w_2.bias (256,) -0.09985816 -0.004434074 0.09823539\n",
      "decoder.att_layers.0.pos_ffn.layer_norm.weight (256,) -0.09856343 0.0027209078 0.09963143\n",
      "decoder.att_layers.0.pos_ffn.layer_norm.bias (256,) -0.09985411 -0.0030520782 0.09973283\n",
      "vocab_decoder.word_prj.weight (50270, 256) -0.010897282 7.3196424e-07 0.010897279\n"
     ]
    }
   ],
   "source": [
    "model_encdec_cfg = create_mllm_encdec_cfg(\n",
    "    n_vocab=len(tokenizer), d_word_wec=256, inp_len=inp_len,\n",
    "    enc_n_layers=1, dec_n_layers=1,\n",
    "    n_heads=8, d_model=256, d_inner=1024,\n",
    "    pad_idx=pad_tok, dropout_rate=0.1, enc_with_emb_mat=True,\n",
    ")\n",
    "model_encdec = MllmEncdec(model_encdec_cfg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_encdec = torch.load(encdec_snapshot_path)\n",
    "model_encdec.load_state_dict(checkpoint_encdec['model'], strict=False)\n",
    "model_encdec.eval()\n",
    "del checkpoint_encdec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15496,   612, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_tokens('Hello there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 100]),\n",
       " torch.Size([1, 100]),\n",
       " tensor([False, False, False,  True, False, False, False, False, False, False,\n",
       "         False, False, False]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 10\n",
    "batch = ds_loader.get_batch(i, train=True)\n",
    "docs_chunks, target_chunks, target_mask = batch.gen_tensors()\n",
    "docs_chunks.shape, target_chunks.shape, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|query_begin|> MakangaraweMakangarawe is an administrative ward in the Temeke district of the Dar es Salaam Region of Tanzania. According to the 2002 census, the ward has a total population of 42,332.\\n\\nReferences\\n\\nCategory:Temeke District\\nCategory:Wards of Tanzania\\nCategory:Populated places in Dar es Salaam Region <|query_end|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_target = tokten_to_text(target_chunks)\n",
    "s_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|doc_begin|> <|doc_id_begin|> 1720712 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Simon Sjödin <|doc_title_end|> <|doc_body_begin|> Simon Sjödin (born 4 October 1986)\n",
      "<|doc_begin|> <|doc_id_begin|> 1720712 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>  the 200 meter butterfly.\\n\\nIn 2007, he changed swim club from Södertörns SS to SK Neptun.\\n\\nIn 2008, Sim\n",
      "<|doc_begin|> <|doc_id_begin|> 1720712 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>  butterfly and 200 m individual medley. Simon reached the semi-final in 200 m butterfly with a time of\n",
      "<|doc_begin|> <|doc_id_begin|> 3821945 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Makangarawe <|doc_title_end|> <|doc_body_begin|> Makangarawe is an administrative wa\n",
      "<|doc_begin|> <|doc_id_begin|> 4317597 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> California State Route 28 <|doc_title_end|> <|doc_body_begin|> State Route 28 (SR 28\n",
      "<|doc_begin|> <|doc_id_begin|> 4317597 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>  Beach and continues to its terminus at Nevada State Route 28 at the Nevada state line.  Route 28 is on\n",
      "<|doc_begin|> <|doc_id_begin|> 4317597 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>  Highway Administration. SR 28 is eligible for the State Scenic Highway System, but it is not official\n",
      "<|doc_begin|> <|doc_id_begin|> 4021682 <|doc_id_end|> <|doc_offset_begin|> 818 <|doc_offset_end|>  and Plenipotentiary\\nAppointed: July 2, 1991\\nPresented credentials: September 8, 1991\\nTerminated missi\n",
      "<|doc_begin|> <|doc_id_begin|> 4021682 <|doc_id_end|> <|doc_offset_begin|> 908 <|doc_offset_end|>  Hume – Career FSO\\nTitle: Ambassador Extraordinary and Plenipotentiary\\nAppointed: November 10, 1997\\nPr\n",
      "<|doc_begin|> <|doc_id_begin|> 4021682 <|doc_id_end|> <|doc_offset_begin|> 998 <|doc_offset_end|>  Left post May 13, 2003\\nRichard W. Erdman – Career FSO\\nTitle: Ambassador Extraordinary and Plenipotent\n",
      "<|doc_begin|> <|doc_id_begin|> 1893130 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>  It follows the typical rhyme scheme of the form, ABAB CDCD EFEF GG, and is written in iambic pentamete\n",
      "<|doc_begin|> <|doc_id_begin|> 1893130 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>   ×    / \\nNo matter then although my foot did stand (44.5)\\n/ = ictus, a metrically strong syllabic pos\n",
      "<|doc_begin|> <|doc_id_begin|> 1893130 <|doc_id_end|> <|doc_offset_begin|> 273 <|doc_offset_end|> :\\n\\n ×   ×  /    /   ×    /   ×   /    ×     / \\nIf the dull substance of my flesh were thought, (44.1)\\n\n"
     ]
    }
   ],
   "source": [
    "for toks in docs_chunks:\n",
    "    s = tokten_to_text(toks)\n",
    "    print(s[:200].replace('\\n', '\\\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embs = model_encdec.run_enc_emb(target_chunks)\n",
    "docs_embs = model_encdec.run_enc_emb(docs_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.635558 F\n",
      "0.663355 F\n",
      "0.620507 F\n",
      "0.770461 T\n",
      "0.674028 F\n",
      "0.673462 F\n",
      "0.689574 F\n",
      "0.629326 F\n",
      "0.631240 F\n",
      "0.648918 F\n",
      "0.635148 F\n",
      "0.602765 F\n",
      "0.631762 F\n"
     ]
    }
   ],
   "source": [
    "cosine = False\n",
    "cosine = True\n",
    "print_dist(target_embs, docs_embs, target_mask, cosine=cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_encoder.src_word_emb.weight (50270, 256) -0.010897281 -6.14697e-07 0.010897281\n",
      "vocab_encoder.layer_norm.weight (256,) -0.097218536 0.0008541888 0.09964408\n",
      "vocab_encoder.layer_norm.bias (256,) -0.09858362 0.0009617999 0.09993384\n",
      "encoders.0.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.1082485 -0.00027505943 0.10824111\n",
      "encoders.0.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.108251765 -0.00022737503 0.108252436\n",
      "encoders.0.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.10824233 0.00028861655 0.10825155\n",
      "encoders.0.layer_stack.0.slf_attn.fc.weight (256, 256) -0.108250685 -0.0003508202 0.10824441\n",
      "encoders.0.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.09998933 -0.0011108869 0.09964647\n",
      "encoders.0.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09951138 0.0024151234 0.099080265\n",
      "encoders.0.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.06846395 -5.2154814e-05 0.068464555\n",
      "encoders.0.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.099506676 -0.0013511884 0.09999602\n",
      "encoders.0.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.06846523 -5.0642662e-05 0.06846465\n",
      "encoders.0.layer_stack.0.pos_ffn.w_2.bias (256,) -0.09986305 0.0046720393 0.09972656\n",
      "encoders.0.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.099907234 -0.0035591945 0.09815641\n",
      "encoders.0.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.099465564 0.0023338227 0.09987935\n",
      "encoders.0.w_em.weight (1, 100) -0.24340685 -0.016641965 0.24204637\n",
      "encoders.0.layer_norm.weight (256,) -0.099000156 -0.0021646372 0.09917823\n",
      "encoders.0.layer_norm.bias (256,) -0.098677956 -0.0026408383 0.09953753\n",
      "decoders.0.w.weight (256, 256) -0.10824321 0.00061353284 0.10825259\n",
      "decoders.0.layer_norm.weight (256,) -0.09976367 0.0043850066 0.09931033\n",
      "decoders.0.layer_norm.bias (256,) -0.09982508 -0.0047540097 0.09954349\n"
     ]
    }
   ],
   "source": [
    "inp_len = ds_loader.emb_chunk_size if ds_loader.fixed_size else calc_max_inp_size(ds_loader.emb_chunk_size)\n",
    "model_ranker_cfg = create_mllm_ranker_cfg(\n",
    "    n_vocab=len(tokenizer), inp_len=inp_len, d_word_wec=256,\n",
    "    n_levels=1, enc_n_layers=1, dec_n_layers=1,\n",
    "    n_heads=8, d_k=32, d_v=32, d_model=256, d_inner=1024,\n",
    "    pad_idx=pad_tok, dropout_rate=0.1, enc_with_emb_mat=True,\n",
    ")\n",
    "\n",
    "model_ranker = MllmRanker(model_ranker_cfg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_ranker = torch.load(ranker_snapshot_path)\n",
    "model_ranker.load_state_dict(checkpoint_ranker['model'])\n",
    "model_ranker.eval()\n",
    "del checkpoint_ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 100]),\n",
       " torch.Size([1, 100]),\n",
       " tensor([False, False, False,  True, False, False, False, False, False, False,\n",
       "         False, False, False]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 10\n",
    "batch = ds_loader.get_batch(i, train=True)\n",
    "docs_chunks, target_chunks, target_mask = batch.gen_tensors()\n",
    "docs_chunks.shape, target_chunks.shape, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|query_begin|> MakangaraweMakangarawe is an administrative ward in the Temeke district of the Dar es Salaam Region of Tanzania. According to the 2002 census, the ward has a total population of 42,332.\\n\\nReferences\\n\\nCategory:Temeke District\\nCategory:Wards of Tanzania\\nCategory:Populated places in Dar es Salaam Region <|query_end|>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_target = tokten_to_text(target_chunks)\n",
    "s_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|doc_begin|> <|doc_id_begin|> 1720712 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Simon Sjödin <|doc_title_end|> <|doc_body_begin|> Simon Sjödin (born 4 October 1986)\n",
      "<|doc_begin|> <|doc_id_begin|> 1720712 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>  the 200 meter butterfly.\\n\\nIn 2007, he changed swim club from Södertörns SS to SK Neptun.\\n\\nIn 2008, Sim\n",
      "<|doc_begin|> <|doc_id_begin|> 1720712 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>  butterfly and 200 m individual medley. Simon reached the semi-final in 200 m butterfly with a time of\n",
      "<|doc_begin|> <|doc_id_begin|> 3821945 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Makangarawe <|doc_title_end|> <|doc_body_begin|> Makangarawe is an administrative wa\n",
      "<|doc_begin|> <|doc_id_begin|> 4317597 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> California State Route 28 <|doc_title_end|> <|doc_body_begin|> State Route 28 (SR 28\n",
      "<|doc_begin|> <|doc_id_begin|> 4317597 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>  Beach and continues to its terminus at Nevada State Route 28 at the Nevada state line.  Route 28 is on\n",
      "<|doc_begin|> <|doc_id_begin|> 4317597 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>  Highway Administration. SR 28 is eligible for the State Scenic Highway System, but it is not official\n",
      "<|doc_begin|> <|doc_id_begin|> 4021682 <|doc_id_end|> <|doc_offset_begin|> 998 <|doc_offset_end|>  Left post May 13, 2003\\nRichard W. Erdman – Career FSO\\nTitle: Ambassador Extraordinary and Plenipotent\n",
      "<|doc_begin|> <|doc_id_begin|> 4021682 <|doc_id_end|> <|doc_offset_begin|> 1089 <|doc_offset_end|>, 2006\\nTerminated mission: Left post June 26, 2008\\nDavid D. Pearce – Career FSO\\nTitle: Ambassador Extr\n",
      "<|doc_begin|> <|doc_id_begin|> 4021682 <|doc_id_end|> <|doc_offset_begin|> 1179 <|doc_offset_end|> Title: Ambassador Extraordinary and Plenipotentiary\\nAppointed: \\nPresented credentials: August 10, 201\n",
      "<|doc_begin|> <|doc_id_begin|> 1893130 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>  It follows the typical rhyme scheme of the form, ABAB CDCD EFEF GG, and is written in iambic pentamete\n",
      "<|doc_begin|> <|doc_id_begin|> 1893130 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>   ×    / \\nNo matter then although my foot did stand (44.5)\\n/ = ictus, a metrically strong syllabic pos\n",
      "<|doc_begin|> <|doc_id_begin|> 1893130 <|doc_id_end|> <|doc_offset_begin|> 273 <|doc_offset_end|> :\\n\\n ×   ×  /    /   ×    /   ×   /    ×     / \\nIf the dull substance of my flesh were thought, (44.1)\\n\n"
     ]
    }
   ],
   "source": [
    "for toks in docs_chunks:\n",
    "    s = tokten_to_text(toks)\n",
    "    print(s[:200].replace('\\n', '\\\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embs = model_ranker.run_enc_emb(target_chunks)\n",
    "docs_embs = model_ranker.run_enc_emb(docs_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.616676 F\n",
      "0.549515 F\n",
      "0.533021 F\n",
      "0.962482 T\n",
      "0.689291 F\n",
      "0.588012 F\n",
      "0.586342 F\n",
      "0.421391 F\n",
      "0.427308 F\n",
      "0.470581 F\n",
      "0.349605 F\n",
      "0.233505 F\n",
      "0.138538 F\n"
     ]
    }
   ],
   "source": [
    "cosine = False\n",
    "cosine = True\n",
    "print_dist(target_embs, docs_embs, target_mask, cosine=cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n",
      "0.652693 F\n",
      "0.655255 F\n",
      "0.652386 F\n",
      "0.845256 T\n",
      "0.639311 F\n",
      "0.544554 F\n",
      "0.556776 F\n",
      "0.431144 F\n",
      "0.456301 F\n",
      "0.458844 F\n",
      "0.432921 F\n",
      "0.280780 F\n",
      "0.343342 F\n"
     ]
    }
   ],
   "source": [
    "txt = 'Hong Kong 1987'\n",
    "txt = 'Climate Classification system, Mays Landing has a humid subtropical climate, abbreviated \"Cfa\"'\n",
    "# txt = 'Climate Classification system'\n",
    "txt = 'War and Peace'\n",
    "txt = 'Bandar Express, Ichhamati Express and Benapole Express'\n",
    "txt = 'Rick Anderson'\n",
    "txt = 'Makangarawe Temeke ward'\n",
    "cosine = True\n",
    "txt_tokens = text_to_tokens(txt, qbeg_tok=qbeg_tok, qend_tok=qend_tok)\n",
    "# txt_tokens = txt_tokens.repeat(3, 1)\n",
    "print(txt_tokens.shape)\n",
    "txt_embs = model_ranker.run_enc_emb(txt_tokens)\n",
    "print_dist(txt_embs, docs_embs, target_mask, cosine=cosine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9422, 0.8513, 0.7140, 0.9978, 0.8926, 0.6926, 0.7297, 0.1455, 0.2862,\n",
       "         0.2185, 0.1052, 0.0027, 0.0214]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rank = model_ranker.run_qs_infer(docs_chunks, txt_tokens)\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8737, 0.8907, 0.7371, 0.5201, 0.4101, 0.8237, 0.7342, 0.8183, 0.8694,\n",
       "         0.6263, 0.6608, 0.6087, 0.2565, 0.3135, 0.3250]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank = model_ranker(txt_tokens, docs_chunks)\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6538e-21, 3.7294e-11, 2.4412e-19, 3.4020e-23, 7.5630e-20, 3.8663e-25,\n",
       "         1.3986e-18, 7.2614e-22, 1.0000e+00, 1.0000e+00, 1.0000e+00, 4.7166e-09,\n",
       "         1.9997e-11, 1.3684e-08]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank = model_ranker(target_chunks, docs_chunks)\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
