{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Optional\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, AddedToken, PreTrainedTokenizer\n",
    "\n",
    "from mllm.data.dswiki import WikiDsLoader\n",
    "from mllm.model.mllm_encdec import MllmEncdec\n",
    "from mllm.model.mllm_ranker import MllmRanker\n",
    "from mllm.model.config import create_mllm_encdec_cfg, create_mllm_ranker_cfg\n",
    "from mllm.tokenization.chunk_tokenizer import calc_max_inp_size, gen_all_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(os.path.expandvars('$HOME')) / 'data'\n",
    "TRAIN_ENCDEC_PATH = DATA_PATH / 'train_mllm_encdec'\n",
    "TRAIN_RANKER_PATH = DATA_PATH / 'train_mllm_ranker'\n",
    "TRAIN_RANKER_PATH = DATA_PATH / 'train_mllm_ranker_qs'\n",
    "DS_DIR_PATH = DATA_PATH / 'wiki_20200501_en' / 'ch_100_fixed'\n",
    "\n",
    "encdec_subdir = 'encdec-20240718_221554-wiki_20200501_en-ch_100_fixed'\n",
    "# ranker_subdir = 'ranker-20240722_225232-wiki_20200501_en-ch_100_fixed'\n",
    "# ranker_subdir = 'ranker-20240724_230827-wiki_20200501_en-ch_100_fixed'\n",
    "# ranker_subdir = 'ranker-20240726_232850-wiki_20200501_en-ch_100_fixed'\n",
    "# ranker_subdir = 'ranker-20240722_225232-wiki_20200501_en-ch_100_fixed'\n",
    "ranker_subdir = 'ranker-20240730_213328-wiki_20200501_en-ch_100_fixed'\n",
    "ranker_subdir = 'ranker-20240806_221913-msmarco'\n",
    "encdec_train_path = TRAIN_ENCDEC_PATH / encdec_subdir\n",
    "ranker_train_path = TRAIN_RANKER_PATH / ranker_subdir\n",
    "encdec_snapshot_path = encdec_train_path / 'best.pth'\n",
    "ranker_snapshot_path = ranker_train_path / 'best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "docs_batch_size = 5\n",
    "max_chunks_per_doc = 3\n",
    "device = 'cpu'\n",
    "# device = 'cuda'\n",
    "\n",
    "device = torch.device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cache from /home/misha/data/wiki_20200501_en/ch_100_fixed/.mllm/ds.csv\n",
      "Loaded dataset size: 50989207\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', model_max_length=100000)\n",
    "tok_dict = gen_all_tokens(tokenizer)\n",
    "pad_tok, qbeg_tok, qend_tok = tok_dict['pad'].ind, tok_dict['query_begin'].ind, tok_dict['query_end'].ind\n",
    "ds_loader = WikiDsLoader(\n",
    "    ds_path=DS_DIR_PATH, docs_batch_size=docs_batch_size, max_chunks_per_doc=max_chunks_per_doc,\n",
    "    pad_tok=pad_tok, qbeg_tok=qbeg_tok, qend_tok=qend_tok, device=device,\n",
    ")\n",
    "ds_loader.shuffle(train=True)\n",
    "ds_loader.shuffle(train=False)\n",
    "inp_len = ds_loader.emb_chunk_size if ds_loader.fixed_size else calc_max_inp_size(ds_loader.emb_chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokten_to_text(tokens: torch.Tensor) -> str:\n",
    "    tokens = tokens.flatten()\n",
    "    tokens = tokens[tokens != pad_tok]\n",
    "    tokens = list(tokens)\n",
    "    s = tokenizer.decode(tokens)\n",
    "    return s\n",
    "\n",
    "def distance(x: np.ndarray, y: np.ndarray, cosine: bool = False):\n",
    "    if not cosine:\n",
    "        return np.linalg.norm(x - y)\n",
    "    x_norm, y_norm = np.linalg.norm(x), np.linalg.norm(y)\n",
    "    return np.sum(x * y) / (x_norm * y_norm)\n",
    "\n",
    "def text_to_tokens(s: str, qbeg_tok: Optional[int] = None, qend_tok: Optional[int] = None) -> torch.Tensor:\n",
    "    tokens = tokenizer(s)['input_ids']\n",
    "    if qbeg_tok is not None:\n",
    "        assert qend_tok is not None\n",
    "        tokens = [qbeg_tok, *tokens, qend_tok]\n",
    "    n_tokens = len(tokens)\n",
    "    n_padded = n_tokens // inp_len + (n_tokens % inp_len > 0)\n",
    "    res = np.full((n_padded * inp_len, ), pad_tok, dtype=np.int32)\n",
    "    res[:n_tokens] = tokens\n",
    "    res = torch.from_numpy(res).to(device)\n",
    "    res = res.reshape(n_padded, inp_len)\n",
    "    return res\n",
    "\n",
    "def print_dist(target_embs: torch.Tensor, docs_embs: torch.Tensor, target_mask: torch.Tensor, cosine: bool = True):\n",
    "    for i, docs_emb in enumerate(docs_embs.detach().numpy()):\n",
    "        for target_emb in target_embs.detach().numpy():\n",
    "            dist = distance(target_emb, docs_emb, cosine)\n",
    "            print(f'{dist:0.6f} ', end='')\n",
    "        sfx = 'T' if target_mask[i] else 'F'\n",
    "        print(sfx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_encoder.src_word_emb.weight (50270, 256) -0.010897281 -5.371368e-07 0.010897278\n",
      "vocab_encoder.layer_norm.weight (256,) -0.09767083 0.0018497845 0.098900214\n",
      "vocab_encoder.layer_norm.bias (256,) -0.099482946 -0.007212233 0.0995147\n",
      "encoder.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.1082476 0.00014912081 0.108250156\n",
      "encoder.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.10825111 -0.0003242747 0.10823374\n",
      "encoder.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.108251184 -0.0002698056 0.10825296\n",
      "encoder.layer_stack.0.slf_attn.fc.weight (256, 256) -0.10824782 -8.96823e-05 0.10825316\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.09930195 -0.0023568799 0.099972546\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09888889 -0.0020617023 0.0983296\n",
      "encoder.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.06846405 -2.1045988e-05 0.06846517\n",
      "encoder.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.09992778 0.0017288679 0.09995016\n",
      "encoder.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.068465225 3.5902303e-05 0.0684652\n",
      "encoder.layer_stack.0.pos_ffn.w_2.bias (256,) -0.09937046 -0.004402348 0.09993773\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.09926772 -0.0026635586 0.0998775\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.09978353 -0.0036565568 0.09933783\n",
      "encoder.w_em.weight (1, 100) -0.24314097 0.010542053 0.23665036\n",
      "encoder.layer_norm.weight (256,) -0.09943658 0.0011948277 0.09953103\n",
      "encoder.layer_norm.bias (256,) -0.09935689 -0.0043721735 0.09872978\n",
      "decoder.A_emb2sec (100, 256, 256) -0.008113918 -1.0405342e-06 0.008113918\n",
      "decoder.att_layers.0.slf_attn.w_qs.weight (256, 256) -0.108251676 -0.00023806517 0.108252816\n",
      "decoder.att_layers.0.slf_attn.w_ks.weight (256, 256) -0.108252 7.627507e-05 0.108250886\n",
      "decoder.att_layers.0.slf_attn.w_vs.weight (256, 256) -0.108249806 -5.388938e-06 0.10825138\n",
      "decoder.att_layers.0.slf_attn.fc.weight (256, 256) -0.10824644 0.0003488814 0.1082524\n",
      "decoder.att_layers.0.slf_attn.layer_norm.weight (256,) -0.09985375 0.0034754316 0.0982054\n",
      "decoder.att_layers.0.slf_attn.layer_norm.bias (256,) -0.098041676 0.00280235 0.09979397\n",
      "decoder.att_layers.0.pos_ffn.w_1.weight (1024, 256) -0.06846478 1.9548268e-05 0.068464816\n",
      "decoder.att_layers.0.pos_ffn.w_1.bias (1024,) -0.099586844 -0.00018490301 0.09997865\n",
      "decoder.att_layers.0.pos_ffn.w_2.weight (256, 1024) -0.068464704 -7.0996684e-05 0.06846422\n",
      "decoder.att_layers.0.pos_ffn.w_2.bias (256,) -0.099832945 -0.0024875747 0.09955104\n",
      "decoder.att_layers.0.pos_ffn.layer_norm.weight (256,) -0.099645294 0.002044194 0.09966619\n",
      "decoder.att_layers.0.pos_ffn.layer_norm.bias (256,) -0.0990983 -0.0011273737 0.09956443\n",
      "vocab_decoder.word_prj.weight (50270, 256) -0.010897281 -4.0425457e-06 0.010897279\n"
     ]
    }
   ],
   "source": [
    "model_encdec_cfg = create_mllm_encdec_cfg(\n",
    "    n_vocab=len(tokenizer), d_word_wec=256, inp_len=inp_len,\n",
    "    enc_n_layers=1, dec_n_layers=1,\n",
    "    n_heads=8, d_model=256, d_inner=1024,\n",
    "    pad_idx=pad_tok, dropout_rate=0.1, enc_with_emb_mat=True,\n",
    ")\n",
    "model_encdec = MllmEncdec(model_encdec_cfg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_encdec = torch.load(encdec_snapshot_path)\n",
    "model_encdec.load_state_dict(checkpoint_encdec['model'], strict=False)\n",
    "model_encdec.eval()\n",
    "del checkpoint_encdec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15496,   612, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267,\n",
       "         50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267, 50267]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_tokens('Hello there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 100]),\n",
       " torch.Size([3, 100]),\n",
       " tensor([False, False, False, False, False, False, False, False, False,  True,\n",
       "          True,  True, False, False, False]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 10\n",
    "batch = ds_loader.get_batch(i, train=True)\n",
    "docs_chunks, target_chunks, target_mask = batch.gen_tensors()\n",
    "docs_chunks.shape, target_chunks.shape, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|query_begin|> Doune HillclimbDoune Hillclimb, Carse of Cambus, near Doune in the district of Stirling, Scotland, is the home of the only round of the British Hill Climb Championship currently to be held in Scotland, (Bo'ness, Fintray and the Rest And Be Thankful have featured in the past). The course is 1,476 yards (1,350 m) in length (although when it was first constructed in 1968 it was around 33yd / 30 m longer) and meetings have been staged by the Lothian Car Club since 1968.\\n\\nPrior to 1968, Lothian Car Club ran rounds of the British Hill Climb Championship at the Bo'ness Hillclimb from 1948 until 1967, when a house estate was built over part of the Bo'ness track. In 1967 the hillclimb track at Doune was designed by Ray Fielding and built with the first event taking place in April 1968. \\n\\nThe current outright record holder is Scott Moran, who set a time of 34.76 seconds in June 2014 in his Gould GR61X. Video of a 35.05 second run by Jos Goodyear in his GWR Raptor can be seen here: \\n\\nDoune Hillclimb holds one meeting of the Scottish Hillclimb <|query_end|>\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_target = tokten_to_text(target_chunks)\n",
    "s_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|doc_begin|> <|doc_id_begin|> 3290073 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Echo Party <|doc_title_end|> <|doc_body_begin|> Echo Party is a mixtape by American \n",
      "<|doc_begin|> <|doc_id_begin|> 3290073 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>. It was recorded using tape echo, guitar, and kazoo, among other instruments.\\n\\nRelease\\nEcho Party was r\n",
      "<|doc_begin|> <|doc_id_begin|> 3290073 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>  an average score of 71, based on 10 reviews, indicating \"generally favorable reviews\".\\n\\nRick Anderson\n",
      "<|doc_begin|> <|doc_id_begin|> 2574801 <|doc_id_end|> <|doc_offset_begin|> 91 <|doc_offset_end|>  passed the exam as banker in 1969. From 1972 to 1974 she continued her education in evening classes an\n",
      "<|doc_begin|> <|doc_id_begin|> 2574801 <|doc_id_end|> <|doc_offset_begin|> 182 <|doc_offset_end|>  she received her PhD in 1987. APS She was then promoted to Docent. From 1992 to 97 she was Director o\n",
      "<|doc_begin|> <|doc_id_begin|> 2574801 <|doc_id_end|> <|doc_offset_begin|> 273 <|doc_offset_end|> person of the Sport Faculty. In 2011 she went into retirement. She became additionally External Resear\n",
      "<|doc_begin|> <|doc_id_begin|> 736502 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> 1986 NCAA Division I Men's Ice Hockey Tournament <|doc_title_end|> <|doc_body_begin|>\n",
      "<|doc_begin|> <|doc_id_begin|> 736502 <|doc_id_end|> <|doc_offset_begin|> 92 <|doc_offset_end|>  Providence, Rhode Island.\\n\\nQualifying teams\\nThe NCAA permitted 8 teams to qualify for the tournament an\n",
      "<|doc_begin|> <|doc_id_begin|> 736502 <|doc_id_end|> <|doc_offset_begin|> 184 <|doc_offset_end|>  The two odd-number ranked teams from one region were placed into a bracket with the two even-number ra\n",
      "<|doc_begin|> <|doc_id_begin|> 3299071 <|doc_id_end|> <|doc_offset_begin|> 0 <|doc_offset_end|> <|doc_title_begin|> Doune Hillclimb <|doc_title_end|> <|doc_body_begin|> Doune Hillclimb, Carse of Cambu\n",
      "<|doc_begin|> <|doc_id_begin|> 3299071 <|doc_id_end|> <|doc_offset_begin|> 90 <|doc_offset_end|> although when it was first constructed in 1968 it was around 33yd / 30 m longer) and meetings have been\n",
      "<|doc_begin|> <|doc_id_begin|> 3299071 <|doc_id_end|> <|doc_offset_begin|> 180 <|doc_offset_end|> ne was designed by Ray Fielding and built with the first event taking place in April 1968. \\n\\nThe curre\n",
      "<|doc_begin|> <|doc_id_begin|> 2004165 <|doc_id_end|> <|doc_offset_begin|> 276 <|doc_offset_end|>  and lowest in January, at around 4.4 °C.\\n\\nHistory\\nThe village of Ōbu was established within Chita Dis\n",
      "<|doc_begin|> <|doc_id_begin|> 2004165 <|doc_id_end|> <|doc_offset_begin|> 368 <|doc_offset_end|>  November 1, 1915. Ōbu became a city on September 1, 1970.\\n\\nGovernment\\nŌbu has a mayor-council form of\n",
      "<|doc_begin|> <|doc_id_begin|> 2004165 <|doc_id_end|> <|doc_offset_begin|> 460 <|doc_offset_end|> Economy\\nŌbu has a mixed economic base due to its proximity to the Nagoya metropolis and numerous trans\n"
     ]
    }
   ],
   "source": [
    "for toks in docs_chunks:\n",
    "    s = tokten_to_text(toks)\n",
    "    print(s[:200].replace('\\n', '\\\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embs = model_encdec.run_enc_emb(target_chunks)\n",
    "docs_embs = model_encdec.run_enc_emb(docs_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.660717 0.578585 0.606018 F\n",
      "0.647171 0.610953 0.636123 F\n",
      "0.630963 0.629010 0.721536 F\n",
      "0.627274 0.657909 0.664354 F\n",
      "0.728437 0.709906 0.706816 F\n",
      "0.636042 0.686461 0.691334 F\n",
      "0.673499 0.701636 0.627083 F\n",
      "0.685973 0.685685 0.619705 F\n",
      "0.608890 0.641603 0.542379 F\n",
      "0.712978 0.683268 0.619175 T\n",
      "0.673893 0.675812 0.678965 T\n",
      "0.637482 0.633176 0.700422 T\n",
      "0.652289 0.665160 0.629680 F\n",
      "0.603251 0.673676 0.642585 F\n",
      "0.643143 0.639594 0.577397 F\n"
     ]
    }
   ],
   "source": [
    "cosine = False\n",
    "cosine = True\n",
    "print_dist(target_embs, docs_embs, target_mask, cosine=cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_encoder.src_word_emb.weight (50270, 256) -0.010897279 -1.6627607e-06 0.010897281\n",
      "vocab_encoder.layer_norm.weight (256,) -0.0986701 0.0019635216 0.09986668\n",
      "vocab_encoder.layer_norm.bias (256,) -0.09950633 0.001656612 0.09917348\n",
      "encoders.0.layer_stack.0.slf_attn.w_qs.weight (256, 256) -0.108237505 -0.00041527097 0.10825055\n",
      "encoders.0.layer_stack.0.slf_attn.w_ks.weight (256, 256) -0.10825316 -0.0002554225 0.10825235\n",
      "encoders.0.layer_stack.0.slf_attn.w_vs.weight (256, 256) -0.108251885 -0.00017302623 0.10824738\n",
      "encoders.0.layer_stack.0.slf_attn.fc.weight (256, 256) -0.10825226 -0.00021478075 0.10824859\n",
      "encoders.0.layer_stack.0.slf_attn.layer_norm.weight (256,) -0.09978091 -0.0037236954 0.09753255\n",
      "encoders.0.layer_stack.0.slf_attn.layer_norm.bias (256,) -0.09951987 0.001332393 0.0995569\n",
      "encoders.0.layer_stack.0.pos_ffn.w_1.weight (1024, 256) -0.06846455 2.6755595e-05 0.068465225\n",
      "encoders.0.layer_stack.0.pos_ffn.w_1.bias (1024,) -0.09987401 -0.0004405306 0.09974468\n",
      "encoders.0.layer_stack.0.pos_ffn.w_2.weight (256, 1024) -0.068464406 -4.6609275e-05 0.06846531\n",
      "encoders.0.layer_stack.0.pos_ffn.w_2.bias (256,) -0.099904396 -0.0032130568 0.09877118\n",
      "encoders.0.layer_stack.0.pos_ffn.layer_norm.weight (256,) -0.0995986 -0.0020073662 0.09875145\n",
      "encoders.0.layer_stack.0.pos_ffn.layer_norm.bias (256,) -0.09971885 -0.0018645611 0.09990263\n",
      "encoders.0.w_em.weight (1, 100) -0.2425105 0.018025523 0.2425941\n",
      "encoders.0.layer_norm.weight (256,) -0.099648915 -0.001607616 0.09952392\n",
      "encoders.0.layer_norm.bias (256,) -0.09838917 0.004012759 0.0991562\n",
      "decoders.0.w.weight (256, 256) -0.10824556 -1.1161268e-05 0.10825049\n",
      "decoders.0.layer_norm.weight (256,) -0.09935206 -0.0029714513 0.09945176\n",
      "decoders.0.layer_norm.bias (256,) -0.09993815 -0.006660583 0.09998995\n"
     ]
    }
   ],
   "source": [
    "inp_len = ds_loader.emb_chunk_size if ds_loader.fixed_size else calc_max_inp_size(ds_loader.emb_chunk_size)\n",
    "model_ranker_cfg = create_mllm_ranker_cfg(\n",
    "    n_vocab=len(tokenizer), inp_len=inp_len, d_word_wec=256,\n",
    "    n_levels=1, enc_n_layers=1, dec_n_layers=1,\n",
    "    n_heads=8, d_k=32, d_v=32, d_model=256, d_inner=1024,\n",
    "    pad_idx=pad_tok, dropout_rate=0.1, enc_with_emb_mat=True,\n",
    ")\n",
    "\n",
    "model_ranker = MllmRanker(model_ranker_cfg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_ranker = torch.load(ranker_snapshot_path)\n",
    "model_ranker.load_state_dict(checkpoint_ranker['model'])\n",
    "model_ranker.eval()\n",
    "del checkpoint_ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 100]),\n",
       " torch.Size([3, 100]),\n",
       " tensor([ True,  True,  True, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 10\n",
    "batch = ds_loader.get_batch(i, train=True)\n",
    "docs_chunks, target_chunks, target_mask = batch.gen_tensors()\n",
    "docs_chunks.shape, target_chunks.shape, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|query_begin|>  an average score of 71, based on 10 reviews, indicating \"generally favorable reviews\".\\n\\nRick Anderson of AllMusic gave the album 4 stars out of 5, saying, \"you\\'ll hear vintage hip-hop basslines, 808 beats, and exuberant \\'80s-style rapping interspersed with weirdness like chopped-up Latin rhythms and shout-outs to New York boroughs and zodiac signs.\" Adam Kennedy of BBC called it \"a production album over mere mixtape, one for the breakdancers, as well as appreciators of both forward-thinking and back-in-the-day craft.\" Nate Patrin of Pitchfork gave the album a 6.8 out of 10, saying, \"if you\\'ve ever wanted to hear classic cuts from the dawn of hip hop turned into hallucinogenic setpieces that knock and clang like glitched-up King Tubby, Echo Party should justify whatever the hell it is Edan\\'s been doing with his time over the past four years.\"\\n\\nDave Segal of The Stranger included it on the \"2009\\'s Top Overlooked Releases\" list.\\n\\nTrack listing\\n\\nPersonnel\\nCredits adapted from liner notes.\\n\\n Edan – recording, mixing, concept, art direction, design\\n Mark Donahue – mastering\\n Trevor \"Karma\" <|query_end|>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_target = tokten_to_text(target_chunks)\n",
    "s_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m toks \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdocs_chunks\u001b[49m:\n\u001b[1;32m      2\u001b[0m     s \u001b[38;5;241m=\u001b[39m tokten_to_text(toks)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(s[:\u001b[38;5;241m200\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "for toks in docs_chunks:\n",
    "    s = tokten_to_text(toks)\n",
    "    print(s[:200].replace('\\n', '\\\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embs = model_ranker.run_enc_emb(target_chunks)\n",
    "docs_embs = model_ranker.run_enc_emb(docs_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.955599 0.844631 0.682803 T\n",
      "0.860412 0.956955 0.735367 T\n",
      "0.736258 0.745605 0.942338 T\n",
      "0.198733 0.191638 0.264690 F\n",
      "0.250881 0.231401 0.250573 F\n",
      "0.020881 -0.025045 0.148455 F\n",
      "0.258112 0.155107 0.196491 F\n",
      "0.128318 0.113541 0.201714 F\n",
      "0.377852 0.314580 0.287486 F\n",
      "0.316507 0.256909 0.265288 F\n",
      "0.250763 0.175146 0.239105 F\n",
      "0.415557 0.330336 0.426571 F\n",
      "-0.099260 -0.177954 -0.001805 F\n",
      "0.030993 -0.092427 0.130320 F\n",
      "0.148946 0.036831 0.214487 F\n"
     ]
    }
   ],
   "source": [
    "cosine = False\n",
    "cosine = True\n",
    "print_dist(target_embs, docs_embs, target_mask, cosine=cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_to_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRick Anderson\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m cosine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m txt_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtext_to_tokens\u001b[49m(txt, qbeg_tok\u001b[38;5;241m=\u001b[39mqbeg_tok, qend_tok\u001b[38;5;241m=\u001b[39mqend_tok)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# txt_tokens = txt_tokens.repeat(3, 1)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(txt_tokens\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_to_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "txt = 'Hong Kong 1987'\n",
    "txt = 'Climate Classification system, Mays Landing has a humid subtropical climate, abbreviated \"Cfa\"'\n",
    "# txt = 'Climate Classification system'\n",
    "txt = 'War and Peace'\n",
    "txt = 'Bandar Express, Ichhamati Express and Benapole Express'\n",
    "txt = 'Rick Anderson'\n",
    "cosine = True\n",
    "txt_tokens = text_to_tokens(txt, qbeg_tok=qbeg_tok, qend_tok=qend_tok)\n",
    "# txt_tokens = txt_tokens.repeat(3, 1)\n",
    "print(txt_tokens.shape)\n",
    "txt_embs = model_ranker.run_enc_emb(txt_tokens)\n",
    "print_dist(txt_embs, docs_embs, target_mask, cosine=cosine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0610, 0.0575, 0.1869, 0.6856, 0.8816, 0.9152, 0.7861, 0.3635, 0.6011,\n",
       "         0.0871, 0.3947, 0.4667, 0.5001, 0.2087]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rank = model_ranker.run_qs_infer(docs_chunks, txt_tokens)\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8737, 0.8907, 0.7371, 0.5201, 0.4101, 0.8237, 0.7342, 0.8183, 0.8694,\n",
       "         0.6263, 0.6608, 0.6087, 0.2565, 0.3135, 0.3250]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank = model_ranker(txt_tokens, docs_chunks)\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6538e-21, 3.7294e-11, 2.4412e-19, 3.4020e-23, 7.5630e-20, 3.8663e-25,\n",
       "         1.3986e-18, 7.2614e-22, 1.0000e+00, 1.0000e+00, 1.0000e+00, 4.7166e-09,\n",
       "         1.9997e-11, 1.3684e-08]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank = model_ranker(target_chunks, docs_chunks)\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
